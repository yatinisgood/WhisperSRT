WEBVTT

00:00.000 --> 00:29.980
Thank you for watching.

00:29.980 --> 00:31.320
I am a visionary.

00:36.980 --> 00:41.040
Illuminating galaxies to witness the birth of stars.

00:47.300 --> 00:51.660
And sharpening our understanding of extreme weather events.

00:56.800 --> 00:58.960
I am a helper.

00:59.980 --> 01:03.620
Guiding the blind through a crowded world.

01:08.160 --> 01:10.700
I was thinking about running to the store.

01:11.060 --> 01:13.740
And giving voice to those who cannot speak.

01:15.300 --> 01:17.100
Do not make me laugh.

01:17.600 --> 01:21.160
I am a transformer.

01:23.240 --> 01:26.560
Harnessing gravity to store renewable power.

01:29.980 --> 01:38.580
And paving the way towards unlimited clean energy for us all.

01:42.200 --> 01:44.440
I am a trainer.

01:45.980 --> 01:48.420
Teaching robots to assist.

01:51.760 --> 01:54.180
To watch out for danger.

01:58.160 --> 01:59.160
And help.

01:59.160 --> 01:59.960
And help.

01:59.960 --> 02:01.740
Help save lives.

02:04.800 --> 02:07.120
I am a healer.

02:09.140 --> 02:11.960
Providing a new generation of cures.

02:13.340 --> 02:16.100
And new levels of patient care.

02:16.240 --> 02:18.240
I am a doctor that I am allergic to penicillin.

02:18.640 --> 02:20.180
Is it still okay to take the medications?

02:20.760 --> 02:21.260
Definitely.

02:21.600 --> 02:23.840
These antibiotics don't contain penicillin.

02:23.960 --> 02:25.760
So it's perfectly safe for you to take them.

02:27.220 --> 02:29.380
I am a navigator.

02:30.640 --> 02:31.600
Define.

02:32.100 --> 02:33.260
Be yourself.

02:33.360 --> 02:34.020
Be yourself.

02:34.220 --> 02:35.700
It's your counselors crown.

02:36.920 --> 02:37.400
Be good.

02:37.520 --> 02:38.080
Be open.

02:38.140 --> 02:38.660
Be open.

02:38.660 --> 02:39.080
Be yourself.

02:39.080 --> 02:39.660
Be yourself.

02:39.720 --> 02:40.160
Be yourself.

02:40.160 --> 02:40.660
Be yourself.

02:40.700 --> 02:41.120
Be yourself.

02:41.140 --> 02:41.620
Be yourself.

02:41.620 --> 02:42.120
Be yourself.

02:42.200 --> 02:42.760
Be yourself.

02:42.980 --> 02:43.480
Be yourself.

02:43.680 --> 02:44.020
Be yourself.

02:44.040 --> 02:44.480
Be yourself.

02:45.020 --> 02:46.460
And understand every decision.

02:46.460 --> 02:47.980
To take on challenges.

02:48.840 --> 02:49.500
To help.

02:49.660 --> 02:51.300
To help you get me out of this pit.

02:51.300 --> 02:53.740
I am a data shore not only I do not know theese stuck-up.

02:53.940 --> 02:54.960
But get a more efficient bite.

02:55.400 --> 02:56.160
I am filled.

02:56.180 --> 02:57.060
With seituluks.

02:57.860 --> 02:58.840
With sauna.

02:58.840 --> 02:59.580
With ho iets.

02:59.960 --> 03:19.960
en muchos idiomas y escribí la música i am ai brought to life by nvidia deep
learning

03:19.960 --> 03:31.960
and brilliant minds everywhere

03:33.960 --> 03:45.960
please welcome to the stage nvidia founder and ceo jensen wong

03:49.960 --> 03:55.240
welcome to gtc

04:00.440 --> 04:03.800
i hope you realize this is not a concert

04:06.520 --> 04:10.360
you have arrived at a developers conference

04:12.280 --> 04:19.800
there will be a lot of science described algorithms computer architecture
mathematics

04:19.960 --> 04:35.960
i sensed a very heavy weight in the room all of a sudden almost like you were in
the wrong place

04:35.960 --> 04:44.440
no no conference in the world is there a greater assembly of researchers from
such diverse fields

04:44.440 --> 04:49.480
of science from climate tech to research and innovation

04:49.960 --> 04:55.400
radio science is trying to figure out how to use ai to robotically control mimos

04:55.400 --> 05:03.560
for next generation 6g radios robotic self-driving cars even artificial
intelligence

05:05.960 --> 05:09.160
even artificial intelligence everybody's

05:11.400 --> 05:19.480
first i noticed a sense of relief there all of all of a sudden also this
conference is represented

05:20.120 --> 05:25.640
by some amazing companies this list this is not the attendees

05:27.640 --> 05:32.040
these are the presenters and what's amazing is this

05:33.560 --> 05:41.720
if you take away all of my friends close friends michael dell is sitting right
there in the i.t

05:41.720 --> 05:42.200
industry

05:47.880 --> 05:49.880
all of the friends i grew up with

05:50.200 --> 05:57.960
in the industry if you take away that list this is what's amazing these are the
presenters

05:58.680 --> 06:04.600
of the non-it industries using accelerated computing to solve problems that
normal computers

06:04.600 --> 06:16.040
can't it's represented in life sciences health care genomics transportation of
course retail

06:16.600 --> 06:19.480
logistics manufacturing industrial

06:20.120 --> 06:24.760
tomahawk or pharmaceutical technology technology modern data negotiation chapter

06:24.760 --> 06:27.560
please turn your焼 to a妳

06:27.560 --> 06:33.400
you're here to present to talk about your research 100 trillion dollars of the
world's industries

06:33.400 --> 06:37.080
represented in this room today this is absolutely amazing

06:43.960 --> 06:49.160
there is absolutely something happening there is something going on

06:49.160 --> 06:49.560
university

06:49.960 --> 06:57.200
the industry is being transformed not just ours because the computer industry

06:57.200 --> 07:02.960
the computer is the single most important instrument of society today

07:02.960 --> 07:08.580
fundamental transformations and computing affects every industry but how

07:08.580 --> 07:13.180
did we start how did we get here I made a little cartoon for you literally I

07:13.180 --> 07:22.160
drew this in one page this is NVIDIA's journey started in 1993 this might be

07:22.160 --> 07:28.600
the rest of the talk 1993 this is our journey we were founded in 1993 there are

07:28.600 --> 07:32.860
several important events that happen along the way I'll just highlight a few

07:32.860 --> 07:38.860
in 2006 CUDA which has turned out to have been a revolutionary computing

07:38.860 --> 07:42.760
model we thought it was revolutionary then it was going to be an overnight

07:42.760 --> 07:43.180
success

07:43.180 --> 07:45.420
in almost 20 years later it happenedione

07:45.420 --> 08:05.080
we saw it coming to decades later in 2012 alex net AI and CUDA made first

08:05.080 --> 08:11.920
contact in 2016 recognizing the importance of this computing model we

08:11.920 --> 08:12.760
invented a brand-new

08:12.760 --> 08:21.760
brand new type of computer. We called it DGX-1. 170 teraflops in this
supercomputer. Eight

08:21.760 --> 08:28.040
GPUs connected together for the very first time. I hand delivered the very first
DGX-1

08:28.040 --> 08:44.900
to a startup located in San Francisco called OpenAI. DGX-1 was the world's first
AI supercomputer.

08:44.900 --> 08:55.880
Remember 170 teraflops. 2017, the transformer arrived. 2022, ChatGPT captured
the world's

08:55.880 --> 08:57.980
imaginations.

08:57.980 --> 09:05.700
People realized the importance and the capabilities of artificial intelligence.
In 2023, generative

09:05.700 --> 09:15.520
AI emerged and a new industry begins. Why? Why is it a new industry? Because the
software

09:15.520 --> 09:22.680
never existed before. We are now producing software. Using computers to write
software.

09:22.680 --> 09:27.460
Producing software that never existed before. It is a brand new category. It
took share

09:27.460 --> 09:27.960
from nothing.

09:27.960 --> 09:36.260
It's a brand new category. And the way you produce the software is unlike
anything we've

09:36.260 --> 09:48.000
ever done before. In data centers, generating tokens, producing floating point
numbers at

09:48.000 --> 09:55.500
very large scale. As if in the beginning of this last industrial revolution,
when people

09:55.500 --> 09:57.440
realized that you would set up factories, you would build factories, you would
build

09:57.440 --> 10:00.540
most simple software to produce data and get a large number of monoliths
toлоNH粒s.

10:00.540 --> 10:04.980
Applied energy to it. And this invisible, valuable thing of electricity

10:04.980 --> 10:10.920
came out, . A CGegitatior . And a hundred years later, 200 years later

10:10.920 --> 10:19.500
, we are now creating new types of electrons. Tokens, using infrastructu , we
call! Factories,

10:19.500 --> 10:25.240
AI! Factories, to generate this new incredibly valuable thing. Called Artificial
intelligence.

10:25.240 --> 10:26.240
A

10:26.240 --> 10:27.300
nir Finish ends. Shaftesbury.com. . A new industry has posterized real life이랑
Mars. Makingct Yes.

10:27.300 --> 10:33.260
emerged well we're going to talk about many things about this new industry

10:33.260 --> 10:37.620
we're going to talk about how we're going to do computing next we're going to
talk

10:37.620 --> 10:42.420
about the type of software that you build because of this new industry the

10:42.420 --> 10:47.100
new software how you would think about this new software what about

10:47.100 --> 10:53.220
applications in this new industry and then maybe what's next and how can we

10:53.220 --> 10:59.480
start preparing today for what is about to come next well but before I start I

10:59.480 --> 11:06.720
want to show you the soul of Nvidia the soul of our company at the intersection

11:06.720 --> 11:17.400
of computer graphics physics and artificial intelligence all intersecting

11:17.400 --> 11:23.200
inside a computer in omniverse in a virtual

11:23.200 --> 11:28.240
world simulation. Everything we're going to show you today, literally everything

11:28.240 --> 11:34.420
we're going to show you today, is a simulation, not animation. It's only

11:34.420 --> 11:39.940
beautiful because it's physics. The world is beautiful. It's only amazing
because

11:39.940 --> 11:44.040
it's being animated with robotics. It's being animated with artificial

11:44.040 --> 11:49.120
intelligence. What you're about to see all day is completely generated,

11:49.120 --> 11:54.640
completely simulated, and omniverse. And all of it, what you're about to enjoy,
is

11:54.640 --> 11:59.760
the world's first concert where everything is homemade.

12:05.040 --> 12:11.740
Everything is homemade. You're about to watch some home videos. So sit back and

12:11.740 --> 12:14.600
enjoy yourself.

12:19.120 --> 12:35.060
Mmm...

12:49.120 --> 13:19.100
Let's do this.

13:19.120 --> 13:49.100
Let's do this.

13:49.120 --> 14:19.100
Let's do this.

14:19.120 --> 14:49.100
Let's do this.

14:49.120 --> 14:59.480
God, I love NVIDIA.

15:04.780 --> 15:08.460
Accelerated computing has reached the tipping point.

15:09.580 --> 15:11.840
General purpose computing has run out of steam.

15:12.740 --> 15:17.040
We need another way of doing computing so that we can continue to scale,

15:17.040 --> 15:19.040
so that we can continue to drive down the cost,

15:19.120 --> 15:25.820
so that we can continue to consume more and more computing while being
sustainable.

15:26.660 --> 15:31.800
Accelerated computing is a dramatic speed-up over general purpose computing.

15:32.520 --> 15:39.620
And in every single industry we engage, and I'll show you many, the impact is
dramatic.

15:40.140 --> 15:43.080
But in no industry is it more important than our own.

15:43.080 --> 15:48.840
The industry of using simulation tools to create products.

15:49.120 --> 15:54.980
In this industry, it is not about driving down the cost of computing.

15:54.980 --> 15:57.800
It's about driving up the scale of computing.

15:57.800 --> 16:04.560
We would like to be able to simulate the entire product that we do completely in
full fidelity,

16:04.560 --> 16:10.140
completely digitally, and essentially what we call digital twins.

16:10.140 --> 16:16.740
We would like to design it, build it, simulate it, operate it completely
digitally.

16:16.740 --> 16:18.660
In order to do that, we need to be able to simulate it.

16:18.660 --> 16:23.300
In order to do that, we need to accelerate an entire industry.

16:23.300 --> 16:28.760
And today, I would like to announce that we have some partners who are joining
us in this journey

16:28.760 --> 16:36.920
to accelerate their entire ecosystem so that we can bring the world into
accelerated computing.

16:36.920 --> 16:39.020
But there's a bonus.

16:39.020 --> 16:45.100
When you become accelerated, your infrastructure is CUDA GPUs.

16:45.100 --> 16:48.200
And when that happens, it's exactly the same infrastructure.

16:48.660 --> 16:51.360
For generative AI.

16:51.360 --> 16:56.340
And so I'm just delighted to announce several very important partnerships.

16:56.340 --> 16:58.780
They're some of the most important companies in the world.

16:58.780 --> 17:02.900
ANSYS does engineering simulation for what the world makes.

17:02.900 --> 17:07.080
We're partnering with them to CUDA accelerate the ANSYS ecosystem,

17:07.080 --> 17:10.880
to connect ANSYS to the Omniverse digital twin.

17:10.880 --> 17:12.300
Incredible.

17:12.300 --> 17:17.240
The thing that's really great is that the install base of NVIDIA GPU accelerated
systems are all over the world.

17:17.240 --> 17:18.400
In every cloud.

17:18.660 --> 17:19.940
In every system.

17:19.940 --> 17:21.480
All over enterprises.

17:21.480 --> 17:26.860
And so the applications they accelerate will have a giant install base to go
serve.

17:26.860 --> 17:29.200
End users will have amazing applications.

17:29.200 --> 17:34.720
And of course, system makers and CSPs will have great customer demand.

17:34.720 --> 17:36.740
Synopsys.

17:36.740 --> 17:42.040
Synopsys is NVIDIA's literally first software partner.

17:42.040 --> 17:44.320
They were there in the very first day of our company.

17:44.320 --> 17:48.040
Synopsys revolutionized the chip industry with high level design.

17:48.660 --> 17:52.080
We are going to CUDA accelerate Synopsys.

17:52.080 --> 17:54.660
We're accelerating computational lithography.

17:54.660 --> 17:59.660
One of the most important applications that nobody has ever known about.

17:59.660 --> 18:03.540
In order to make chips, we have to push lithography to a limit.

18:03.540 --> 18:05.780
NVIDIA has created a library.

18:05.780 --> 18:10.380
A domain-specific library that accelerates computational lithography.

18:10.380 --> 18:11.940
Incredibly.

18:11.940 --> 18:17.940
Once we can accelerate it and in software define all of TSMC, who is announcing
today that they're going to go into,

18:17.940 --> 18:18.520
Click.

18:18.660 --> 18:24.320
to production with NVIDIA CooLitho. Once the software defined and accelerated,
the next step

18:24.320 --> 18:29.500
is to apply generative AI to the future of semiconductor manufacturing, pushing
geometry

18:29.500 --> 18:39.380
even further. Cadence builds the world's essential EDA and SDA tools. We also
use Cadence. Between

18:39.380 --> 18:44.900
these three companies, Ansys, Synopsys, and Cadence, we basically build NVIDIA.
Together,

18:44.900 --> 18:51.140
we are CooLitho accelerating Cadence. They're also building a supercomputer out
of NVIDIA GPUs

18:51.140 --> 18:58.240
so that their customers could do fluid dynamic simulation at a hundred, a
thousand times scale.

18:59.040 --> 19:06.140
Basically, a wind tunnel in real time. Cadence Millennium, a supercomputer with
NVIDIA GPUs

19:06.140 --> 19:12.820
inside. A software company building supercomputers. I love seeing that. Building
Cadence co-pilots

19:12.820 --> 19:14.540
together. Imagine a day.

19:14.900 --> 19:23.780
When Cadence could, Synopsys, Ansys, tool providers would offer you AI co-pilots
so that we have

19:23.780 --> 19:30.300
thousands and thousands of co-pilot assistants helping us design chips, design
systems. And we're

19:30.300 --> 19:35.500
also going to connect Cadence Digital Twin Platform to Omniverse. As you can see
the trend here,

19:35.980 --> 19:43.380
we're accelerating the world's CAE, EDA, and SDA so that we could create our
future in digital

19:43.380 --> 19:44.880
twins. And we're going to connect the two together. We're going to create a
future in digital twins.

19:44.880 --> 19:49.680
We're going to connect them all to Omniverse, the fundamental operating system
for future digital twins.

19:51.560 --> 19:57.720
One of the industries that benefited tremendously from scale, and you all know
this one very well,

19:57.720 --> 20:05.320
large language models. Basically, after the transformer was invented, we were
able to scale

20:05.320 --> 20:11.000
large language models at incredible rates, effectively doubling every six
months. Now,

20:11.000 --> 20:13.960
how is it possible that by doubling every six months,

20:14.880 --> 20:20.320
that we have grown the industry, we have grown the computational requirements so
far?

20:20.320 --> 20:23.960
And the reason for that is quite simply this. If you double the size of the
model,

20:23.960 --> 20:28.240
you double the size of your brain, you need twice as much information to go fill
it.

20:28.240 --> 20:36.320
And so every time you double your parameter count, you also have to
appropriately increase

20:36.320 --> 20:44.200
your training token count. The combination of those two numbers becomes the
computation scale you have to support.

20:44.880 --> 20:51.680
The latest, the state of the art openAI model is approximately 1.8 trillion
parameters.

20:51.680 --> 20:58.560
1.8 trillion parameters required several trillion tokens to go train.

20:58.560 --> 21:04.260
So a few trillion parameters on the order of a few trillion tokens on the order
of,

21:04.260 --> 21:14.800
when you multiply the two of them together, approximately 30, 40, 50 billion
quadrillion floating point operations.

21:14.880 --> 21:18.640
per second now we just have to do some co math right now just hang hang with me

21:19.200 --> 21:27.520
so you have 30 billion quadrillion a quadrillion is like a peta and so if you
had a petaflop

21:27.520 --> 21:35.200
gpu you would need 30 billion seconds to go compute to go train that model 30
billion

21:35.200 --> 21:41.840
seconds is approximately 1 000 years well 1 000 years it's worth it

21:44.880 --> 21:49.760
you like to do it sooner but it's worth it

21:52.800 --> 21:56.160
which is usually my answer when most people tell me hey how long how long is it
going to

21:56.160 --> 22:03.440
take to do something so we got 20 years it's worth it but can we do it next week

22:06.720 --> 22:14.400
and so 1 000 years 1 000 years so what we need what we need are bigger gpus

22:15.520 --> 22:22.160
we need much much bigger gpus we recognized this early on and we realized that
the answer

22:22.160 --> 22:26.960
is to put a whole bunch of gpus together and of course innovate a whole bunch of
things along

22:26.960 --> 22:32.640
the way like inventing tensor cores advancing mv links so that we could create
essentially

22:32.640 --> 22:39.040
virtually giant gpus and connecting them all together with amazing networks from
a company

22:39.040 --> 22:44.800
called melanox infiniband so that we could create these giant systems and so
dgx1 was our

22:44.800 --> 22:50.160
first version but it wasn't the last we built we built supercomputers all the
way all along the way

22:51.360 --> 23:01.040
in 2021 we had celine 4 500 gpus or so and then in 2023 we built one of the
largest ai

23:01.040 --> 23:08.960
supercomputers in the world it's just come online eos and as we're building
these things we're

23:08.960 --> 23:13.280
trying to help the world build these things and in order to help the world build
these things we got

23:13.280 --> 23:14.720
to build them first we're trying to help the world build these things we got to
build them first we're

23:14.720 --> 23:16.000
building these things we're trying to help the world build these things we got
to build these things we got to build the chips the systems

23:16.640 --> 23:23.440
the networking all of the software necessary to do this you should see these
systems imagine writing

23:23.440 --> 23:28.240
a piece of software that runs across the entire system distributing the
computation across

23:29.040 --> 23:33.520
thousands of gpus but inside are thousands of smaller gpus

23:35.280 --> 23:40.160
millions of gpus to distribute work across all of that and to balance the
workload

23:40.160 --> 23:44.560
so that you can get the most energy efficiency the best computation time

23:44.720 --> 23:50.000
keep your costs down and so those those fundamental innovations

23:51.360 --> 24:00.560
is what got us here and here we are as we see the miracle of chat gpt emerge in
front of us

24:00.560 --> 24:08.320
we also realize we have a long ways to go we need even larger models we're going
to train it with

24:08.320 --> 24:13.040
multi-modality data not just text on the internet but we're going to we're going
to train it on

24:13.040 --> 24:20.640
texts and images and graphs and charts and just as we learn watching tv and so
there's going to

24:20.640 --> 24:26.720
be a whole bunch of watching video so that these models can be grounded in
physics understands that

24:26.720 --> 24:33.600
an arm doesn't go through a wall and so these models would have common sense by
watching a lot

24:33.600 --> 24:39.680
of the world's video combined with a lot of the world's languages they'll use
things like synthetic

24:39.680 --> 24:42.800
data generation just as you and i do when we try to learn

24:43.040 --> 24:46.800
we might use our imagination to simulate how it's going to

24:46.800 --> 24:52.080
end up just as i did when i was preparing for this keynote i was simulating it
all along the way

24:55.120 --> 24:58.480
i hope it's going to turn out as well as i had into my head

25:06.000 --> 25:11.520
as i was simulating how this keynote was going to turn out somebody did say that
another performer

25:13.840 --> 25:16.240
did her performance completely on a treadmill

25:17.600 --> 25:23.520
so that she could be in shape to deliver it with full energy i i didn't do that

25:25.920 --> 25:29.280
if i get a little winded about 10 minutes into this you know what happened

25:31.440 --> 25:36.880
and so so where were we we're sitting here using synthetic data generation we're
going

25:36.880 --> 25:40.560
to use reinforcement learning we're going to practice it in our mind we're going
to have

25:40.560 --> 25:42.800
ai working with ai training each other

25:43.040 --> 25:48.560
just like student teacher debaters all of that is going to increase the size of
our model it's

25:48.560 --> 25:53.040
going to increase the amount of con the amount of data that we have and we're
going to have to build

25:53.040 --> 26:03.040
even bigger gpus hopper is fantastic but we need bigger gpus and so ladies and
gentlemen

26:05.200 --> 26:11.440
i would like to introduce you to a very very big gpu

26:13.040 --> 26:16.400
i wouldn't want to overstate that in particular but i know that we are an ego of
a всех

26:16.400 --> 26:18.560
the nets you are the ones to네 Herren

26:18.560 --> 26:26.400
i'll break you down

26:29.200 --> 26:34.880
named after david blackwell mathematician

26:36.000 --> 26:40.720
game theorists probability we thought it was a perfect

26:41.280 --> 26:41.840
perfect name

26:41.840 --> 26:42.880
Blackwell ladies and gentlemen enjoy this

26:42.880 --> 27:12.860
I don't know.

27:12.880 --> 27:42.860
I don't know.

27:42.880 --> 28:12.860
I don't know.

28:12.880 --> 28:42.860
I don't know.

28:42.880 --> 29:12.860
I don't know.

29:12.880 --> 29:19.160
Blackwell is not a chip.

29:19.160 --> 29:20.820
Blackwell is the name of a platform.

29:21.960 --> 29:25.700
People think we make GPUs, and we do,

29:26.580 --> 29:29.400
but GPUs don't look the way they used to.

29:30.960 --> 29:36.260
Here's the, if you will, the heart of the Blackwell system,

29:37.000 --> 29:39.800
and this inside the company is not called Blackwell.

29:39.880 --> 29:40.440
It's just a number.

29:41.400 --> 29:42.860
And Blackwell is a chip.

29:42.860 --> 29:47.680
This, this is Blackwell sitting next to, oh,

29:47.860 --> 29:50.380
this is the most advanced GPU in the world in production today.

29:54.560 --> 29:55.720
This is Hopper.

29:56.780 --> 29:57.900
This is Hopper.

29:58.180 --> 29:59.420
Hopper changed the world.

30:01.260 --> 30:02.640
This is Blackwell.

30:02.640 --> 30:02.680
This is Blackwell.

30:12.100 --> 30:12.840
It's okay.

30:12.860 --> 30:13.240
Hopper.

30:19.120 --> 30:20.380
You're, you're very good.

30:22.000 --> 30:23.280
Good, good boy.

30:25.020 --> 30:25.660
Oh, good girl.

30:30.220 --> 30:35.640
208 billion transistors, and so, so you could see, you, I can see,

30:36.100 --> 30:38.360
there's a small line between two dyes.

30:38.360 --> 30:41.640
This is the first time two dyes have abutted like this together

30:41.640 --> 30:42.480
in such a way.

30:42.480 --> 30:42.500
This is the first time two dyes have abutted like this together in such a way.

30:42.500 --> 30:42.560
This is the first time two dyes have abutted like this together in such a way.

30:42.560 --> 30:46.180
It's such a way that the two chip, the two dyes think it's one chip.

30:47.180 --> 30:51.000
There's 10 terabytes of data between it, 10 terabytes per second,

30:51.700 --> 30:56.580
so that these two, these two sides of the Blackwell chip have no clue which side
they're on.

30:57.540 --> 31:03.000
There's no memory locality issues, no cache issues, it's just one giant chip,

31:03.820 --> 31:09.360
and so, when we were told that Blackwell's ambitions were beyond the limits of
physics,

31:10.000 --> 31:11.600
the engineer said, so what?

31:11.600 --> 31:18.040
and so this is what what happened and so this is the Blackwell chip and it goes

31:18.040 --> 31:25.480
into two types of systems the first one is form-fit function compatible to

31:25.480 --> 31:29.540
hopper and so you slide on hopper and you push in Blackwell that's the reason

31:29.540 --> 31:33.980
why one of the challenges of ramping is going to be so efficient there are

31:33.980 --> 31:38.120
installations of hoppers all over the world and they could be they could be

31:38.120 --> 31:42.860
you know the same infrastructure same design the power the electricity the

31:42.860 --> 31:49.580
thermals the software identical push it right back and so this is a hopper

31:49.580 --> 31:55.940
version for the current HGX configuration and this is what the other

31:55.940 --> 32:01.880
the second hopper looks like this now this is a prototype board and Janine

32:01.880 --> 32:08.120
could I just borrow ladies and gentlemen Janine Paul

32:08.120 --> 32:17.140
and so this this is the this is a fully functioning board and I'll just be

32:17.140 --> 32:29.580
careful here this right here is I don't know 10 billion dollars the second one's

32:29.580 --> 32:37.540
five it gets cheaper after that so any customers in the audience

32:37.560 --> 32:37.940
yeah

32:37.940 --> 32:38.100
yeah

32:38.100 --> 32:38.940
It's okay.

32:42.920 --> 32:44.820
All right, but this one's quite expensive.

32:44.820 --> 32:46.360
This is the bring-up board,

32:46.360 --> 32:49.460
and the way it's gonna go to production

32:49.460 --> 32:51.400
is like this one here, okay?

32:51.400 --> 32:53.520
And so you're gonna take this.

32:53.520 --> 32:58.520
It has two Blackwell chips and four Blackwell dies

32:59.040 --> 33:02.200
connected to a Grace CPU.

33:02.200 --> 33:05.660
The Grace CPU has a super-fast chip-to-chip link.

33:05.660 --> 33:09.640
What's amazing is this computer is the first of its kind

33:09.640 --> 33:12.120
where this much computation, first of all,

33:12.120 --> 33:15.540
fits into this small of a place.

33:15.540 --> 33:18.220
Second, it's memory-coherent.

33:18.220 --> 33:20.920
They feel like they're just one big happy family

33:20.920 --> 33:24.120
working on one application together,

33:24.120 --> 33:26.320
and so everything is coherent within it.

33:28.000 --> 33:30.820
Just the amount of, you know, you saw the numbers.

33:30.820 --> 33:34.140
There's a lot of terabytes this and terabytes that,

33:34.140 --> 33:35.440
but this is a miracle.

33:35.440 --> 33:38.140
This is a, this, let's see.

33:38.140 --> 33:40.000
What are some of the things on here?

33:40.000 --> 33:45.000
There's a NVLink on top, PCI Express on the bottom,

33:47.560 --> 33:52.560
on your, which one is mine, and your left?

33:53.520 --> 33:55.780
One of them, it doesn't matter.

33:55.780 --> 33:59.980
One of them is a CPU chip-to-chip link.

33:59.980 --> 34:02.000
It's my left or your, depending on which side.

34:02.000 --> 34:05.060
I was just, I was trying to sort that out, and I just kind of.

34:06.160 --> 34:07.000
Doesn't matter.

34:12.340 --> 34:14.000
Hopefully it comes plugged in, so.

34:19.140 --> 34:22.560
Okay, so this is the Grace Blackwell system.

34:32.040 --> 34:33.040
But there's more.

34:35.440 --> 34:40.140
So turns out, it turns out, all of the specs is fantastic,

34:40.140 --> 34:41.820
but we need a whole lot of new features.

34:43.360 --> 34:47.440
In order to push the limits beyond, if you will, the limits of physics,

34:48.500 --> 34:52.020
we would like to always get a lot more X-factors.

34:52.020 --> 34:55.720
And so one of the things that we did was we invented another transformer engine.

34:55.720 --> 34:58.600
Another transformer engine, the second generation.

34:58.600 --> 35:04.240
It has the ability to dynamically and automatically rescale and recast,

35:04.240 --> 35:05.420
and recast.

35:05.420 --> 35:13.540
numerical formats to a lower precision whenever it can remember artificial
intelligence is about

35:13.540 --> 35:21.220
probability and so you kind of have you know 1.7 approximately 1.7 times
approximately 1.4 to be

35:21.220 --> 35:27.300
approximately something else does that make sense and so so the ability for the
mathematics to

35:27.300 --> 35:34.200
retain the precision and the range necessary in that particular stage of the
pipeline super

35:34.200 --> 35:39.960
important and so this is it's not just about the fact that we designed a smaller
ALU it's not quite

35:39.960 --> 35:46.920
the world's not quite that simple you've got to figure out when you can use that
across a computation

35:46.920 --> 35:55.260
that is thousands of GPUs it's running for weeks and weeks on weeks and you want
to make sure that

35:55.260 --> 36:01.740
the the the train job is going to converge and so this new transformer engine we
have a fifth

36:01.740 --> 36:02.820
generation MV link

36:04.200 --> 36:11.280
it's now twice as fast as hopper but very importantly it has computation in the
network and the reason

36:11.280 --> 36:16.200
for that is because when you have so many different GPUs working together we
have to share our

36:16.200 --> 36:21.380
information with each other we have to synchronize and update each other and
every so often we have

36:21.380 --> 36:27.240
to reduce the partial products and then rebroadcast out the partial products
that some of the partial

36:27.240 --> 36:32.280
products back to everybody else and so there's a lot of what is called all
reduce and all to all and

36:32.280 --> 36:33.420
all gather

36:34.200 --> 36:39.000
in this area of synchronization and collectives so that we can have GPUs working
with each other

36:39.000 --> 36:46.140
having extraordinarily fast links and being able to do mathematics right in the
network allows us to

36:46.140 --> 36:52.980
essentially amplify even further so even though it's 1.8 terabytes per second
it's effectively higher

36:52.980 --> 37:01.920
than that and so it's many times that of hopper the likelihood of a
supercomputer running for weeks

37:01.920 --> 37:04.140
on end is approximately zero

37:04.200 --> 37:08.880
and the reason for that is because there's so many components working at the
same time

37:09.780 --> 37:15.480
the statistic the probability of them working continuously is very low and so we
need to make

37:15.480 --> 37:22.680
sure that whenever there is a well we checkpoint and restart as often as we can
but if we have the

37:22.680 --> 37:32.220
ability to detect a weak chip or a weak note early we can retire it and maybe
swap in another processor

37:32.220 --> 37:34.020
that ability to keep the

37:34.200 --> 37:38.880
authorization of the super computer high especially when you just spent 2
billion dollars building it is super

37:38.880 --> 37:40.880
important and so we put in a race engine a reliability engine that does a
hundred percent self test in

37:40.880 --> 37:50.680
system test of every single gate every single bit of memory on the Blackwell
chip and all the memory

37:50.680 --> 37:57.240
doesn't connected to it it's almost as if I'm helping a'm

37:57.240 --> 37:59.480
helping a rotor test a real-time information chip and all using a physical
computer so just starting

37:59.480 --> 38:00.580
to understand where this K wheat is in my work language the power chip is great
and the

38:00.580 --> 38:02.580
capabilities it's taking you are amazing and they're really really exciting I'm
really interested in

38:02.580 --> 38:03.780
you do learn but you have to use a computer to work so in all that stuff it's
even using a simple radio

38:03.780 --> 38:11.100
as if we shipped with every single chip its own advanced tester that we test our

38:11.100 --> 38:14.880
chips with. This is the first time we're doing this, super excited about it.

38:14.880 --> 38:17.800
Secure AI.

38:22.780 --> 38:32.340
Only this conference do they clap for RAS. The secure AI, obviously you've

38:32.340 --> 38:36.960
just spent hundreds of millions of dollars creating a very important AI and

38:36.960 --> 38:42.840
the code, the intelligence of that AI is encoded in the parameters. You want to

38:42.840 --> 38:45.600
make sure that on the one hand you don't lose it, on the other hand it doesn't
get

38:45.600 --> 38:53.280
contaminated. And so we now have the ability to encrypt data, of course at

38:53.280 --> 39:01.060
rest, but also in transit and while it's being computed. It's all encrypted and
so

39:01.060 --> 39:02.340
we now have the ability to

39:02.340 --> 39:07.820
encrypt and transmission and when we're computing it is in a trusted

39:07.820 --> 39:14.400
environment, trusted engine environment. The last thing is decompression. Moving

39:14.400 --> 39:19.140
data in and out of these nodes when the compute is so fast becomes really

39:19.140 --> 39:25.020
essential. And so we've put in a high line speed compression engine and

39:25.020 --> 39:29.740
effectively moves data 20 times faster in and out of these computers. These

39:29.740 --> 39:31.920
computers are so powerful and

39:32.340 --> 39:36.800
And there's such a large investment, the last thing we want to do is have them
be idle.

39:36.800 --> 39:47.100
And so all of these capabilities are intended to keep Blackwell fed and as busy
as possible.

39:47.100 --> 39:57.780
Overall, compared to Hopper, it is two and a half times the FP8 performance for
training

39:57.780 --> 40:00.020
per chip.

40:00.020 --> 40:05.200
It also has this new format called FP6, so that even though the computation
speed is

40:05.200 --> 40:11.620
the same, the bandwidth that's amplified because of the memory, the amount of
parameters you

40:11.620 --> 40:14.400
can store in the memory is now amplified.

40:14.400 --> 40:17.320
FP4 effectively doubles the throughput.

40:17.320 --> 40:20.840
This is vitally important for inference.

40:20.840 --> 40:27.040
One of the things that is becoming very clear is that whenever you use a
computer with AI

40:27.040 --> 40:29.080
on the other side.

40:29.080 --> 40:30.000
When you're chatting with...

40:30.000 --> 40:38.880
When you get the chat bot, when you're asking it to review or make an image,
remember in

40:38.880 --> 40:43.120
the back is a GPU generating tokens.

40:43.120 --> 40:49.420
Some people call it inference, but it's more appropriately generation.

40:49.420 --> 40:53.360
The way that computing has done in the past was retrieval.

40:53.360 --> 40:58.160
You would grab your phone, you would touch something, some signals go off,
basically

40:58.160 --> 41:00.000
an email goes off to some storage.

41:00.000 --> 41:02.520
You're talking to a computer somewhere.

41:02.520 --> 41:04.060
There's prerecorded content.

41:04.060 --> 41:07.520
Somebody wrote a story or somebody made an image or somebody recorded a video.

41:07.520 --> 41:13.880
That prerecorded content is then streamed back to the phone and recomposed in a
way

41:13.880 --> 41:18.780
based on a recommender system to present the information to you.

41:18.780 --> 41:25.280
You know that in the future, the vast majority of that content will not be
retrieved.

41:25.280 --> 41:28.920
And the reason for that is because that was prerecorded by somebody who doesn't
understand

41:28.920 --> 41:29.840
the context.

41:29.840 --> 41:35.540
retrieve so much content. If you can be working with an AI that understands the

41:35.540 --> 41:39.400
context, who you are, for what reason you're fetching this information, and

41:39.400 --> 41:45.720
produces the information for you just the way you like it, the amount of energy

41:45.720 --> 41:50.000
we save, the amount of networking bandwidth we save, the amount of waste of

41:50.000 --> 41:56.180
time we save, will be tremendous. The future is generative, which is the reason

41:56.180 --> 42:00.200
why we call it generative AI, which is the reason why this is a brand new

42:00.200 --> 42:06.080
industry. The way we compute is fundamentally different. We created a

42:06.080 --> 42:12.200
processor for the generative AI era, and one of the most important parts of it
is

42:12.200 --> 42:21.400
content token generation. We call it, this format is FP4. Well, that's a lot of

42:21.400 --> 42:26.160
computation. 5x, the token

42:26.160 --> 42:37.200
generation, 5x, the inference capability of Hopper, seems like enough. But why

42:37.200 --> 42:43.680
stop there? The answer is it's not enough, and I'm going to show you why. I'm
going

42:43.680 --> 42:48.000
to show you why. And so we would like to have a bigger GPU, even bigger than
this

42:48.000 --> 42:55.400
one. And so we decided to scale it, and notice, but first let me just tell you

42:55.400 --> 42:56.160
how we've scaled.

42:56.160 --> 43:02.320
Over the course of the last eight years, we've increased computation by 1,000

43:02.320 --> 43:06.800
times. Eight years, 1,000 times. Remember back in the good old days of Moore's
Law,

43:06.800 --> 43:13.200
it was 2x, well, 5x every, what, 10x every five years. That's easiest,

43:13.200 --> 43:18.840
easiest math. 10x every five years, a hundred times every ten years. One hundred

43:18.840 --> 43:25.080
times every ten years, at the, in the middle, in the heydays of the PC

43:25.080 --> 43:25.680
Revolution.

43:26.160 --> 43:33.400
One hundred times every ten years. In the last eight years, we've gone 1,000
times.

43:33.400 --> 43:38.000
We have two more years to go. And so that puts it in perspective.

43:42.000 --> 43:46.600
The rate at which we're advancing computing is insane, and it's still not

43:46.600 --> 43:54.040
fast enough, so we built another chip. This chip is just an incredible chip. We

43:54.040 --> 43:56.040
call it the MV-Link switch.

43:56.960 --> 44:02.040
Its 50 billion transistors, it's almost the size of Hopper all by itself. This

44:02.040 --> 44:08.120
switch ship has four MV-Link's in it, each 1.8 terabytes per second,

44:11.080 --> 44:13.400
and it has computation, and as I mentioned,

44:14.600 --> 44:18.280
what is this chip for? If we were to build such

44:20.280 --> 44:23.540
a chip, we can have every single GPU talk

44:23.540 --> 44:25.960
to every other GPU at full speed.

44:26.160 --> 44:29.160
at the same time. That's insane.

44:37.160 --> 44:40.160
It doesn't even make sense.

44:40.160 --> 44:43.160
But if you could do that, if you can find a way to do that

44:43.160 --> 44:46.160
and build a system to do that,

44:46.160 --> 44:49.160
that's cost-effective. That's cost-effective.

44:49.160 --> 44:52.160
How incredible would it be

44:52.160 --> 44:55.160
that we could have all these GPUs connect over

44:55.160 --> 44:58.160
a coherent link

44:58.160 --> 45:01.160
so that they effectively are one giant GPU?

45:01.160 --> 45:04.160
Well, one of the great inventions

45:04.160 --> 45:07.160
in order to make it cost-effective is that this chip

45:07.160 --> 45:10.160
has to drive copper directly.

45:10.160 --> 45:13.160
The Surtees of this chip is just a phenomenal invention

45:13.160 --> 45:16.160
so that we could do direct drive to copper.

45:16.160 --> 45:19.160
And as a result, you can build a system

45:19.160 --> 45:22.160
that looks like this.

45:22.160 --> 45:25.160
Now this system,

45:25.160 --> 45:28.160
this system,

45:28.160 --> 45:31.160
is kind of insane.

45:31.160 --> 45:34.160
This is one DGX.

45:34.160 --> 45:37.160
This is what a DGX looks like now.

45:37.160 --> 45:40.160
Remember, just six years ago,

45:40.160 --> 45:43.160
it was pretty heavy,

45:43.160 --> 45:46.160
but I was able to lift it.

45:46.160 --> 45:49.160
I delivered

45:49.160 --> 45:52.160
the DGX to a company

45:52.160 --> 45:55.160
that was the first DGX1 to open AI.

45:55.160 --> 45:57.160
And the researchers there,

45:57.160 --> 45:59.160
the pictures are on the internet,

45:59.160 --> 46:02.160
and we all autographed it.

46:02.160 --> 46:05.160
And if you come to my office,

46:05.160 --> 46:08.160
it's autographed there. It's really beautiful.

46:08.160 --> 46:10.160
But you could lift it.

46:10.160 --> 46:11.160
This DGX,

46:11.160 --> 46:12.160
this DGX,

46:12.160 --> 46:14.160
that DGX, by the way,

46:14.160 --> 46:18.160
was 170 teraflops.

46:18.160 --> 46:21.160
If you're not familiar with the numbering system,

46:21.160 --> 46:24.160
it's 0.17 teraflops.

46:24.160 --> 46:26.160
So this is 720.

46:26.160 --> 46:28.160
The first one I delivered to open AI

46:28.160 --> 46:30.160
was 0.17.

46:30.160 --> 46:32.160
You could round it up to 0.2.

46:32.160 --> 46:34.160
It won't make any difference.

46:34.160 --> 46:36.160
But back then, it was like, wow,

46:36.160 --> 46:38.160
you know, 30 more teraflops.

46:38.160 --> 46:42.160
And so this is now 720 teraflops,

46:42.160 --> 46:44.160
almost an exaflop for training,

46:44.160 --> 46:47.160
and the world's first one exaflop machine

46:47.160 --> 46:49.160
in one rack.

46:51.160 --> 46:58.160
Just so you know,

46:58.160 --> 47:01.160
there are only a couple, two, three exaflops machines

47:01.160 --> 47:03.160
on the planet as we speak.

47:03.160 --> 47:07.160
And so this is an exaflop AI system

47:07.160 --> 47:09.160
in one single rack.

47:09.160 --> 47:14.160
Well, let's take a look at the back of it.

47:14.160 --> 47:17.160
So this is what makes it possible.

47:17.160 --> 47:18.160
That's the back,

47:18.160 --> 47:20.160
that's the back,

47:20.160 --> 47:23.160
that's the back of the VXMV link spine.

47:23.160 --> 47:27.160
130 terabytes per second

47:27.160 --> 47:30.160
goes through the back of that chassis.

47:30.160 --> 47:32.160
That is more than the aggregate bandwidth

47:32.160 --> 47:34.160
of the internet.

47:43.160 --> 47:45.160
So we could basically send everything

47:45.160 --> 47:47.160
to everybody within a second.

47:47.160 --> 47:49.160
And so we have 5,000 cables,

47:49.160 --> 47:52.160
5,000 NVLink cables,

47:52.160 --> 47:54.160
in total, two miles.

47:54.160 --> 47:56.160
Now this is the amazing thing.

47:56.160 --> 47:58.160
If we had to use optics,

47:58.160 --> 48:01.160
we would have had to use transceivers and retimers,

48:01.160 --> 48:03.160
and those transceivers and retimers alone

48:03.160 --> 48:08.160
would have cost 20,000 watts,

48:08.160 --> 48:11.160
two kilowatts of just transceivers alone,

48:11.160 --> 48:14.160
just to drive the NVLink spine.

48:14.160 --> 48:15.160
As a result,

48:15.160 --> 48:18.160
we did it completely for free over NVLink switch,

48:18.160 --> 48:21.160
and we were able to save the 20 kilowatts for computation.

48:21.160 --> 48:24.160
This entire rack is 120 kilowatts,

48:24.160 --> 48:27.160
so that 20 kilowatts makes a huge difference.

48:27.160 --> 48:29.160
It's liquid cooled.

48:29.160 --> 48:32.160
What goes in is 25 degrees C about room temperature.

48:32.160 --> 48:37.160
What comes out is 45 degrees C about your jacuzzi.

48:37.160 --> 48:39.160
So room temperature goes in,

48:39.160 --> 48:40.160
jacuzzi comes out,

48:40.160 --> 48:42.160
two liters per second.

48:48.160 --> 48:52.160
We could sell a peripheral.

48:57.160 --> 49:00.160
600,000 parts.

49:00.160 --> 49:01.160
Somebody used to say,

49:01.160 --> 49:03.160
you know, you guys make GPUs,

49:03.160 --> 49:04.160
and we do,

49:04.160 --> 49:07.160
but this is what a GPU looks like to me.

49:07.160 --> 49:10.160
When somebody says GPU, I see this.

49:10.160 --> 49:13.160
Two years ago, when I saw a GPU, it was the HGX.

49:13.160 --> 49:16.160
It was 70 pounds, 35,000 parts.

49:16.160 --> 49:17.160
Our GPUs now

49:17.160 --> 49:20.160
are 600,000 parts

49:20.160 --> 49:23.160
and 3,000 pounds.

49:23.160 --> 49:25.160
3,000 pounds.

49:25.160 --> 49:26.160
3,000 pounds,

49:26.160 --> 49:28.160
that's kind of like the weight of a,

49:28.160 --> 49:33.160
you know, carbon fiber Ferrari.

49:33.160 --> 49:36.160
I don't know if that's a useful metric, but...

49:38.160 --> 49:39.160
Everybody's going,

49:39.160 --> 49:40.160
I feel it.

49:40.160 --> 49:41.160
I feel it. I get it.

49:41.160 --> 49:42.160
I get that.

49:42.160 --> 49:45.160
Now that you mention that, I feel it.

49:45.160 --> 49:47.160
I don't know what's 3,000 pounds.

49:47.160 --> 49:50.160
Okay, so 3,000 pounds,

49:50.160 --> 49:51.160
a ton and a half.

49:51.160 --> 49:54.160
So it's not quite an elephant.

49:54.160 --> 49:56.160
So this is what a DGX looks like.

49:56.160 --> 49:58.160
Now let's see what it looks like in operation.

49:58.160 --> 49:59.160
Okay, let's imagine,

49:59.160 --> 50:02.160
how do we put this to work and what does that mean?

50:02.160 --> 50:05.160
Well, if you were to train a GPT model,

50:05.160 --> 50:08.160
1.8 trillion parameter model,

50:08.160 --> 50:11.160
it took apparently about, you know,

50:11.160 --> 50:15.160
three to five months or so with 25,000 amperes.

50:15.160 --> 50:16.160
If we were to do it with Hopper,

50:16.160 --> 50:19.160
it would probably take something like 8,000 GPUs

50:19.160 --> 50:21.160
and it would consume 15 megawatts.

50:21.160 --> 50:23.160
8,000 GPUs and 15 megawatts.

50:23.160 --> 50:25.160
It would take 90 days, about three months.

50:25.160 --> 50:29.160
And that would allow you to train something that is,

50:29.160 --> 50:32.160
you know, this groundbreaking AI model.

50:32.160 --> 50:37.160
And this is obviously not as expensive as anybody would think,

50:37.160 --> 50:39.160
but it's 8,000 GPUs.

50:39.160 --> 50:40.160
It's still a lot of money.

50:40.160 --> 50:43.160
And so 8,000 GPUs, 15 megawatts.

50:43.160 --> 50:45.160
If you were to use Blackwell to do this,

50:45.160 --> 50:50.160
it would only take 2,000 GPUs.

50:50.160 --> 50:53.160
2,000 GPUs, same 90 days.

50:53.160 --> 50:55.160
But this is the amazing part.

50:55.160 --> 50:57.160
Only four megawatts of power.

50:57.160 --> 51:00.160
So from 15, yeah, that's right.

51:05.160 --> 51:07.160
And that's our goal.

51:07.160 --> 51:11.160
Our goal is to continuously drive down the cost and the energy.

51:11.160 --> 51:12.160
They're directly proportional to each other.

51:12.160 --> 51:14.160
Cost and energy associated with the computer.

51:14.160 --> 51:17.160
Associated with the computing so that we can continue to expand

51:17.160 --> 51:20.160
and scale up the computation that we have to do

51:20.160 --> 51:22.160
to train the next generation models.

51:22.160 --> 51:25.160
Well, this is training.

51:25.160 --> 51:30.160
Inference or generation is vitally important going forward.

51:30.160 --> 51:32.160
You know, probably some half of the time

51:32.160 --> 51:34.160
that NVIDIA GPUs are in the cloud these days,

51:34.160 --> 51:36.160
it's being used for token generation.

51:36.160 --> 51:38.160
You know, they're either doing co-pilot this

51:38.160 --> 51:40.160
or chat, you know, chat GPT that

51:40.160 --> 51:42.160
or all these different models that are being used

51:42.160 --> 51:43.160
when you're interacting with it.

51:43.160 --> 51:46.160
Or generating images or generating videos.

51:46.160 --> 51:49.160
Generating proteins, generating chemicals.

51:49.160 --> 51:52.160
There's a bunch of generation going on.

51:52.160 --> 51:56.160
All of that is in the category of computing we call inference.

51:56.160 --> 52:00.160
But inference is extremely hard for large language models

52:00.160 --> 52:03.160
because these large language models have several properties.

52:03.160 --> 52:05.160
One, they're very large.

52:05.160 --> 52:07.160
And so it doesn't fit on one GPU.

52:07.160 --> 52:11.160
This is, imagine, imagine Excel doesn't fit on one GPU.

52:11.160 --> 52:12.160
You know?

52:12.160 --> 52:13.160
And, you know,

52:13.160 --> 52:15.160
imagine some application you're running on a daily basis

52:15.160 --> 52:17.160
doesn't fit on one computer.

52:17.160 --> 52:20.160
Like a video game doesn't fit on one computer.

52:20.160 --> 52:22.160
And most, in fact, do.

52:22.160 --> 52:25.160
And many times in the past, in hyperscale computing,

52:25.160 --> 52:28.160
many applications for many people fit on the same computer.

52:28.160 --> 52:32.160
And now, all of a sudden, this one inference application

52:32.160 --> 52:34.160
where you're interacting with this chat bot,

52:34.160 --> 52:38.160
that chat bot requires a super computer in the back to run it.

52:38.160 --> 52:40.160
And that's the future.

52:40.160 --> 52:42.160
The future is generative with these chat bots.

52:42.160 --> 52:45.160
And these chat bots are trillions of tokens,

52:45.160 --> 52:47.160
trillions of parameters.

52:47.160 --> 52:51.160
And they have to generate tokens at interactive rates.

52:51.160 --> 52:53.160
Now, what does that mean?

52:53.160 --> 52:58.160
Well, three tokens is about a word.

52:58.160 --> 53:05.160
You know, the, you know, space, the final frontier,

53:05.160 --> 53:07.160
these are the adventures.

53:07.160 --> 53:10.160
That's like 80 tokens.

53:10.160 --> 53:11.160
Okay?

53:11.160 --> 53:13.160
I don't know if that's useful to you.

53:13.160 --> 53:19.160
And so, you know, the art of communications

53:19.160 --> 53:22.160
is selecting good analogies.

53:22.160 --> 53:25.160
Yeah, this is not going well.

53:28.160 --> 53:30.160
I don't know what he's talking about.

53:30.160 --> 53:32.160
Never seen Star Trek.

53:32.160 --> 53:34.160
And so, here we are.

53:34.160 --> 53:36.160
We're trying to generate these tokens.

53:36.160 --> 53:38.160
When you're interacting with it, you're hoping that the tokens

53:38.160 --> 53:40.160
come back to you as quickly as possible

53:40.160 --> 53:42.160
and as quickly as you can read it.

53:42.160 --> 53:45.160
And so, the ability for generation tokens is really important.

53:45.160 --> 53:49.160
You have to paralyze the work of this model across many, many GPUs

53:49.160 --> 53:51.160
so that you could achieve several things.

53:51.160 --> 53:54.160
One, on the one hand, you would like throughput

53:54.160 --> 53:57.160
because that throughput reduces the cost,

53:57.160 --> 54:01.160
the overall cost per token of generating.

54:01.160 --> 54:06.160
So, your throughput dictates the cost of delivering the service.

54:06.160 --> 54:09.160
On the other hand, you have another interactive rate,

54:09.160 --> 54:11.160
which is another tokens per second,

54:11.160 --> 54:13.160
where it's about per user.

54:13.160 --> 54:15.160
And that has everything to do with quality of service.

54:15.160 --> 54:20.160
And so, these two things compete against each other.

54:20.160 --> 54:22.160
And we have to find a way to distribute work

54:22.160 --> 54:24.160
across all of these different GPUs

54:24.160 --> 54:27.160
and paralyze it in a way that allows us to achieve both.

54:27.160 --> 54:32.160
And it turns out the search space is enormous.

54:32.160 --> 54:35.160
You know, I told you there's going to be math involved.

54:35.160 --> 54:38.160
And everybody's going, oh, dear.

54:39.160 --> 54:43.160
I heard some gasps just now when I put up that slide.

54:43.160 --> 54:47.160
So, this right here, the y-axis is tokens per second,

54:47.160 --> 54:49.160
data center throughput.

54:49.160 --> 54:53.160
The x-axis is tokens per second, interactivity of the person.

54:53.160 --> 54:55.160
And notice, the upper right is the best.

54:55.160 --> 54:58.160
You want interactivity to be very high,

54:58.160 --> 55:00.160
number of tokens per second per user.

55:00.160 --> 55:03.160
You want the tokens per second per data center to be very high.

55:03.160 --> 55:05.160
The upper right is terrific.

55:05.160 --> 55:07.160
However, it's very hard to do that.

55:07.160 --> 55:09.160
And in order for us to search

55:09.160 --> 55:12.160
for the best answer across every single one

55:12.160 --> 55:15.160
of those intersections, x, y coordinates,

55:15.160 --> 55:18.160
okay, so just look at every single x, y coordinate,

55:18.160 --> 55:21.160
all those blue dots came from some repartitioning

55:21.160 --> 55:23.160
of the software.

55:23.160 --> 55:26.160
Some optimizing solution has to go and figure out

55:26.160 --> 55:31.160
whether to use tensor parallel, expert parallel,

55:31.160 --> 55:34.160
pipeline parallel, or data parallel,

55:34.160 --> 55:37.160
and distribute this enormous model

55:37.160 --> 55:39.160
across all these different GPUs.

55:39.160 --> 55:42.160
And sustain the performance that you need.

55:42.160 --> 55:44.160
This exploration space would be impossible

55:44.160 --> 55:47.160
if not for the programmability of NVIDIA's GPUs.

55:47.160 --> 55:49.160
And so we could, because of CUDA,

55:49.160 --> 55:51.160
because we have such a rich ecosystem,

55:51.160 --> 55:53.160
we could explore this universe

55:53.160 --> 55:56.160
and find that green roofline.

55:56.160 --> 55:58.160
It turns out that green roofline,

55:58.160 --> 56:02.160
notice, you got TP2EPADP4,

56:02.160 --> 56:05.160
it means two tensor parallel,

56:05.160 --> 56:08.160
tensor parallel across two GPUs,

56:08.160 --> 56:10.160
expert parallel across eight,

56:10.160 --> 56:12.160
data parallel across four.

56:12.160 --> 56:13.160
Notice on the other end,

56:13.160 --> 56:15.160
you got tensor parallel across four,

56:15.160 --> 56:17.160
and expert parallel across 16.

56:17.160 --> 56:20.160
The configuration, the distribution of that software,

56:20.160 --> 56:23.160
it's a different, different runtime

56:23.160 --> 56:26.160
that would produce these different results.

56:26.160 --> 56:28.160
And you have to go discover that roofline.

56:28.160 --> 56:30.160
Well, that's just one model.

56:30.160 --> 56:33.160
And this is just one configuration of a computer.

56:33.160 --> 56:36.160
Imagine all of the models being created around the world

56:36.160 --> 56:38.160
and all the different,

56:38.160 --> 56:41.160
configurations of systems that are going to be available.

56:44.160 --> 56:47.160
So now that you understand the basics,

56:47.160 --> 56:54.160
let's take a look at inference of Blackwell compared to Hopper.

56:54.160 --> 56:57.160
And this is the extraordinary thing.

56:57.160 --> 57:00.160
In one generation, because we created a system

57:00.160 --> 57:05.160
that's designed for trillion parameter generative AI,

57:05.160 --> 57:08.160
the inference capability of Blackwell is off the charts.

57:08.160 --> 57:13.160
And in fact, it is some 30 times Hopper.

57:13.160 --> 57:14.160
Yeah.

57:19.160 --> 57:21.160
For large language models,

57:21.160 --> 57:25.160
for large language models like ChatGPT and others like it,

57:25.160 --> 57:27.160
the blue line is Hopper.

57:27.160 --> 57:31.160
I gave you, imagine we didn't change the architecture of Hopper,

57:31.160 --> 57:33.160
we just made it a bigger chip.

57:33.160 --> 57:36.160
We just used the latest, you know, greatest,

57:36.160 --> 57:41.160
10 terabytes per second.

57:41.160 --> 57:43.160
We connected the two chips together.

57:43.160 --> 57:45.160
We got this giant 208 billion parameter chip.

57:45.160 --> 57:48.160
How would we have performed if nothing else changed?

57:48.160 --> 57:51.160
And it turns out, quite wonderfully,

57:51.160 --> 57:54.160
quite wonderfully, and that's the purple line,

57:54.160 --> 57:56.160
but not as great as it could be.

57:56.160 --> 57:59.160
And that's where the FP4 tensor core,

57:59.160 --> 58:01.160
the new transformer engine,

58:01.160 --> 58:04.160
and very importantly, the NVLink switch.

58:04.160 --> 58:05.160
And the reason for that,

58:05.160 --> 58:08.160
is because all these GPUs have to share the results,

58:08.160 --> 58:09.160
partial products.

58:09.160 --> 58:12.160
Whenever they do all to all, all gather,

58:12.160 --> 58:14.160
whenever they communicate with each other,

58:14.160 --> 58:20.160
that NVLink switch is communicating almost 10 times faster

58:20.160 --> 58:24.160
than what we could do in the past using the fastest networks.

58:24.160 --> 58:29.160
Okay, so Blackwell is going to be just an amazing system

58:29.160 --> 58:31.160
for generative AI.

58:31.160 --> 58:33.160
And in the future,

58:33.160 --> 58:34.160
in the future,

58:34.160 --> 58:35.160
in the future,

58:35.160 --> 58:38.160
data centers are going to be thought of,

58:38.160 --> 58:39.160
as I mentioned earlier,

58:39.160 --> 58:41.160
as an AI factory.

58:41.160 --> 58:46.160
An AI factory's goal in life is to generate revenues,

58:46.160 --> 58:49.160
generate, in this case,

58:49.160 --> 58:53.160
intelligence in this facility,

58:53.160 --> 58:56.160
not generating electricity as in AC generators,

58:56.160 --> 58:59.160
but of the last Industrial Revolution

58:59.160 --> 59:00.160
and this Industrial Revolution,

59:00.160 --> 59:02.160
the generation of intelligence.

59:02.160 --> 59:04.160
And so this ability is super,

59:04.160 --> 59:06.160
super important.

59:06.160 --> 59:09.160
The excitement of Blackwell is really off the charts.

59:09.160 --> 59:10.160
You know, when we first,

59:10.160 --> 59:13.160
when we first,

59:13.160 --> 59:15.160
you know, this is a year and a half ago,

59:15.160 --> 59:16.160
two years ago,

59:16.160 --> 59:17.160
I guess two years ago,

59:17.160 --> 59:20.160
when we first started to go to market with Hopper,

59:20.160 --> 59:24.160
you know, we had the benefit of two CSPs

59:24.160 --> 59:26.160
joined us in a lunch.

59:26.160 --> 59:28.160
And we were, you know, delighted.

59:28.160 --> 59:32.160
And so we had two customers.

59:32.160 --> 59:34.160
We have more now.

59:47.160 --> 59:50.160
Unbelievable excitement for Blackwell.

59:50.160 --> 59:51.160
Unbelievable excitement.

59:51.160 --> 59:53.160
And there's a whole bunch of different configurations.

59:53.160 --> 59:56.160
Of course, I showed you the configurations

59:56.160 --> 59:58.160
that slide into the Hopper form factor,

59:58.160 --> 01:00:00.160
so that's easy to upgrade.

01:00:00.160 --> 01:00:01.160
I showed you examples

01:00:01.160 --> 01:00:02.160
that are liquid-cooled,

01:00:02.160 --> 01:00:04.160
that are the extreme versions of it.

01:00:04.160 --> 01:00:08.160
One entire rack that's connected by MVLink-72.

01:00:08.160 --> 01:00:10.160
We're going to,

01:00:10.160 --> 01:00:13.160
Blackwell is going to be ramping

01:00:13.160 --> 01:00:16.160
to the world's AI companies,

01:00:16.160 --> 01:00:18.160
of which there are so many now,

01:00:18.160 --> 01:00:20.160
doing amazing work in different modalities.

01:00:20.160 --> 01:00:24.160
The CSPs, every CSP is geared up.

01:00:24.160 --> 01:00:27.160
All the OEMs and ODMs,

01:00:27.160 --> 01:00:29.160
regional clouds,

01:00:29.160 --> 01:00:30.160
sovereign AI,

01:00:30.160 --> 01:00:33.160
and telcos all over the world

01:00:33.160 --> 01:00:35.160
are signing up to launch with Blackwell.

01:00:43.160 --> 01:00:47.160
Blackwell would be the most successful product launch

01:00:47.160 --> 01:00:49.160
in our history.

01:00:49.160 --> 01:00:51.160
And so I can't wait to see that.

01:00:51.160 --> 01:00:53.160
I want to thank some partners

01:00:53.160 --> 01:00:55.160
that are joining us in this.

01:00:55.160 --> 01:00:57.160
AWS is gearing up for Blackwell.

01:00:57.160 --> 01:00:59.160
They're going to build the first

01:00:59.160 --> 01:01:01.160
GPU with secure AI.

01:01:01.160 --> 01:01:05.160
They're building out a 222-exaflops system.

01:01:05.160 --> 01:01:07.160
You know, just now when we animated,

01:01:07.160 --> 01:01:09.160
just now the digital twin,

01:01:09.160 --> 01:01:12.160
if you saw all of those clusters coming down.

01:01:12.160 --> 01:01:15.160
By the way, that is not just art.

01:01:15.160 --> 01:01:18.160
That is a digital twin of what we're building.

01:01:18.160 --> 01:01:20.160
That's how big it's going to be.

01:01:20.160 --> 01:01:21.160
Besides infrastructure,

01:01:21.160 --> 01:01:23.160
we're doing a lot of things together with AWS.

01:01:23.160 --> 01:01:26.160
We're CUDA-accelerating SageMaker AI.

01:01:26.160 --> 01:01:28.160
We're CUDA-accelerating Bedrock AI.

01:01:28.160 --> 01:01:31.160
Amazon Robotics is working with us

01:01:31.160 --> 01:01:34.160
using NVIDIA Omniverse and Isaac Sim.

01:01:34.160 --> 01:01:38.160
AWS Health has NVIDIA Health integrated into it.

01:01:38.160 --> 01:01:43.160
So AWS has really leaned into accelerated computing.

01:01:43.160 --> 01:01:45.160
Google is gearing up for Blackwell.

01:01:45.160 --> 01:01:49.160
GCP already has A100s, H100s, T4s, L4s,

01:01:49.160 --> 01:01:52.160
a whole fleet of NVIDIA CUDA GPUs.

01:01:52.160 --> 01:01:55.160
And they recently announced the Gemma model

01:01:55.160 --> 01:01:57.160
that runs across all of it.

01:01:57.160 --> 01:02:00.160
We're working to optimize and accelerate

01:02:00.160 --> 01:02:02.160
every aspect of GCP.

01:02:02.160 --> 01:02:04.160
We're accelerating Dataproc for data processing,

01:02:04.160 --> 01:02:06.160
their data processing engine,

01:02:06.160 --> 01:02:09.160
JAX, XLA, Vertex AI,

01:02:09.160 --> 01:02:11.160
and MuJoCo for robotics.

01:02:11.160 --> 01:02:13.160
So we're working with Google and GCP

01:02:13.160 --> 01:02:15.160
across a whole bunch of initiatives.

01:02:15.160 --> 01:02:17.160
Oracle is gearing up for Blackwell.

01:02:17.160 --> 01:02:19.160
Oracle is a great partner of ours

01:02:19.160 --> 01:02:21.160
for NVIDIA DGX Cloud.

01:02:21.160 --> 01:02:23.160
And we're also working together to accelerate

01:02:23.160 --> 01:02:25.160
something that's really important to a lot of companies,

01:02:25.160 --> 01:02:27.160
Oracle Database.

01:02:27.160 --> 01:02:30.160
Microsoft is accelerating,

01:02:30.160 --> 01:02:32.160
and Microsoft is gearing up for Blackwell.

01:02:32.160 --> 01:02:35.160
Microsoft NVIDIA has a wide-ranging partnership.

01:02:35.160 --> 01:02:36.160
We're accelerating CUDA,

01:02:36.160 --> 01:02:38.160
accelerating all kinds of services.

01:02:38.160 --> 01:02:40.160
When you chat, obviously,

01:02:40.160 --> 01:02:43.160
and AI services that are in Microsoft Azure,

01:02:43.160 --> 01:02:45.160
it's very, very likely NVIDIA is in the back

01:02:45.160 --> 01:02:48.160
doing the inference and the token generation.

01:02:48.160 --> 01:02:52.160
They built the largest NVIDIA InfiniBand supercomputer,

01:02:52.160 --> 01:02:54.160
basically a digital twin of ours,

01:02:54.160 --> 01:02:56.160
or a physical twin of ours.

01:02:56.160 --> 01:02:59.160
We're bringing the NVIDIA ecosystem to Azure.

01:02:59.160 --> 01:03:01.160
NVIDIA DGX Cloud to Azure.

01:03:01.160 --> 01:03:04.160
NVIDIA Omniverse is now hosted in Azure.

01:03:04.160 --> 01:03:06.160
NVIDIA Healthcare is in Azure.

01:03:06.160 --> 01:03:08.160
And all of it is deeply integrated

01:03:08.160 --> 01:03:11.160
and deeply connected with Microsoft Fabric.

01:03:11.160 --> 01:03:14.160
The whole industry is gearing up for Blackwell.

01:03:14.160 --> 01:03:16.160
This is what I'm about to show you.

01:03:16.160 --> 01:03:22.160
Most of the scenes that you've seen so far of Blackwell

01:03:22.160 --> 01:03:25.160
are the full fidelity design

01:03:25.160 --> 01:03:27.160
of Blackwell.

01:03:27.160 --> 01:03:30.160
Everything in our company has a digital twin.

01:03:30.160 --> 01:03:35.160
And, in fact, this digital twin idea is really spreading,

01:03:35.160 --> 01:03:39.160
and it helps companies build very complicated things

01:03:39.160 --> 01:03:41.160
perfectly the first time.

01:03:41.160 --> 01:03:46.160
And what could be more exciting than creating a digital twin

01:03:46.160 --> 01:03:50.160
to build a computer that was built in a digital twin?

01:03:50.160 --> 01:03:53.160
And so let me show you what Wishtron is doing.

01:03:55.160 --> 01:03:58.160
To meet the demand for NVIDIA accelerated computing,

01:03:58.160 --> 01:04:01.160
Wishtron, one of our leading manufacturing partners,

01:04:01.160 --> 01:04:05.160
is building digital twins of NVIDIA DGX and HGX factories

01:04:05.160 --> 01:04:11.160
using custom software developed with Omniverse SDKs and APIs.

01:04:11.160 --> 01:04:14.160
For their newest factory, Wishtron started with a digital twin

01:04:14.160 --> 01:04:16.160
to virtually integrate their multi-CAD

01:04:16.160 --> 01:04:20.160
and process simulation data into a unified view.

01:04:20.160 --> 01:04:22.160
Testing and optimizing layouts

01:04:22.160 --> 01:04:24.160
in this physically accurate digital environment

01:04:24.160 --> 01:04:28.160
increased worker efficiency by 51%.

01:04:28.160 --> 01:04:31.160
During construction, the Omniverse digital twin

01:04:31.160 --> 01:04:33.160
was used to verify that the physical build

01:04:33.160 --> 01:04:35.160
matched the digital plans.

01:04:35.160 --> 01:04:37.160
Identifying any discrepancies early

01:04:37.160 --> 01:04:40.160
has helped avoid costly change orders.

01:04:40.160 --> 01:04:42.160
And the results have been impressive.

01:04:42.160 --> 01:04:45.160
Using a digital twin helped bring Wishtron's factory online

01:04:45.160 --> 01:04:49.160
in half the time, just two and a half months instead of five.

01:04:49.160 --> 01:04:51.160
In operation, the Omniverse digital twin

01:04:51.160 --> 01:04:53.160
helps Wishtron rapidly test new layouts

01:04:53.160 --> 01:04:55.160
to accommodate new processes

01:04:55.160 --> 01:04:58.160
or improve operations in the existing space

01:04:58.160 --> 01:05:00.160
and monitor real-time operations

01:05:00.160 --> 01:05:05.160
using live IoT data from every machine on the production line,

01:05:05.160 --> 01:05:07.160
which ultimately enabled Wishtron

01:05:07.160 --> 01:05:10.160
to reduce end-to-end cycle times by 50%

01:05:10.160 --> 01:05:13.160
and defect rates by 40%.

01:05:13.160 --> 01:05:15.160
With NVIDIA AI and Omniverse,

01:05:15.160 --> 01:05:17.160
NVIDIA's global ecosystem of partners

01:05:17.160 --> 01:05:19.160
are building a new era of accelerated,

01:05:19.160 --> 01:05:22.160
AI-enabled digitalization.

01:05:23.160 --> 01:05:25.160
Let's talk about the future.

01:05:34.160 --> 01:05:36.160
That's how we are,

01:05:36.160 --> 01:05:38.160
that's the way it's gonna be in the future.

01:05:38.160 --> 01:05:41.160
We're manufacturing everything digitally first,

01:05:41.160 --> 01:05:43.160
and then we'll manufacture it physically.

01:05:43.160 --> 01:05:45.160
People ask me, how did it start?

01:05:45.160 --> 01:05:48.160
What got you guys so excited?

01:05:48.160 --> 01:05:51.160
What was it that you saw

01:05:51.160 --> 01:05:53.500
on this

01:05:53.500 --> 01:05:54.940
incredible idea

01:05:54.940 --> 01:05:56.900
and it's this.

01:06:01.280 --> 01:06:02.140
Hang on a second.

01:06:07.700 --> 01:06:09.420
Guys, that was going to be such a moment.

01:06:12.860 --> 01:06:14.560
That's what happens when you don't rehearse.

01:06:20.260 --> 01:06:20.820
This

01:06:20.820 --> 01:06:22.260
as you know

01:06:22.260 --> 01:06:24.180
was first contact.

01:06:25.220 --> 01:06:26.500
2012, AlexNet.

01:06:27.360 --> 01:06:28.880
You put a cat

01:06:28.880 --> 01:06:30.440
into this computer

01:06:30.440 --> 01:06:32.440
and it comes out and it says

01:06:32.440 --> 01:06:33.300
cat.

01:06:36.440 --> 01:06:37.820
And we said

01:06:37.820 --> 01:06:40.760
oh my god, this is going to change everything.

01:06:43.300 --> 01:06:44.220
You take

01:06:44.220 --> 01:06:45.600
one million numbers

01:06:45.600 --> 01:06:48.900
across three channels

01:06:48.900 --> 01:06:49.360
RGB

01:06:49.360 --> 01:06:52.520
these numbers make no sense to anybody

01:06:52.520 --> 01:06:54.540
you put it into this

01:06:54.540 --> 01:06:55.260
software

01:06:55.260 --> 01:06:58.580
and it compresses, it dimensionally

01:06:58.580 --> 01:06:59.340
reduces it

01:06:59.340 --> 01:07:02.200
from a million dimensions

01:07:02.200 --> 01:07:03.340
a million dimensions

01:07:03.340 --> 01:07:05.760
it turns it into three letters

01:07:05.760 --> 01:07:08.260
one vector, one number

01:07:08.260 --> 01:07:11.800
and it's generalized

01:07:11.800 --> 01:07:14.060
you could have the cat

01:07:14.060 --> 01:07:15.980
be different cats

01:07:15.980 --> 01:07:19.300
and you could have it be

01:07:19.300 --> 01:07:19.340
different cats

01:07:19.340 --> 01:07:19.360
and you could have it be different cats

01:07:19.360 --> 01:07:20.280
you could have the cat

01:07:20.280 --> 01:07:20.380
and the front of the cat

01:07:20.380 --> 01:07:20.780
the front of the cat

01:07:20.800 --> 01:07:21.340
and the back of the cat

01:07:21.340 --> 01:07:22.420
and the back of the cat

01:07:23.700 --> 01:07:25.120
and you look at this thing, you say

01:07:25.120 --> 01:07:27.040
unbelievable.

01:07:27.040 --> 01:07:27.740
You mean any cats?

01:07:27.740 --> 01:07:31.700
Yeah, any cat

01:07:31.700 --> 01:07:34.440
It was able to recognize all these cats

01:07:34.440 --> 01:07:35.240
and we realized

01:07:35.240 --> 01:07:36.540
how it did it

01:07:36.540 --> 01:07:37.740
systematically

01:07:37.740 --> 01:07:38.720
structurally

01:07:38.720 --> 01:07:41.980
its scaleable

01:07:41.980 --> 01:07:43.560
How big can you make it?

01:07:43.560 --> 01:07:44.700
Well, how big

01:07:44.700 --> 01:07:46.360
do you want to make it?

01:07:46.360 --> 01:07:48.060
And so we imagine

01:07:48.060 --> 01:07:48.480
that this is a completely

01:07:48.480 --> 01:07:49.100
ever-lasting industrial revolution.

01:07:49.100 --> 01:07:55.760
new way of writing software and now today as you know you can have you type

01:07:55.760 --> 01:08:06.060
in the word CAT and what comes out is a cat it went the other way am i right

01:08:06.060 --> 01:08:12.280
unbelievable how is it possible that's right how is it possible you took three

01:08:12.280 --> 01:08:19.440
letters and you generated a million pixels from it and it made sense well

01:08:19.440 --> 01:08:23.720
that's the miracle and here we are just literally 10 years

01:08:23.720 --> 01:08:29.680
later 10 years later where we recognize texts we recognize images we recognize

01:08:29.680 --> 01:08:35.520
videos and sounds and images not only do we recognize them we understand their

01:08:35.520 --> 01:08:39.140
meaning we understand the meaning of the text that's the reason why I can chat

01:08:39.140 --> 01:08:41.800
with you it can summarize for you it

01:08:41.800 --> 01:08:42.260
understands what you read I can read it that's account for lung audio I can get

01:08:42.260 --> 01:08:48.140
understands the text. It understood, not just recognizes the English, it
understood the English.

01:08:48.140 --> 01:08:53.940
It doesn't just recognize the pixels, it understood the pixels. And you can even
condition it

01:08:53.940 --> 01:08:58.980
between two modalities. You can have language condition image and generate all
kinds of

01:08:58.980 --> 01:09:04.140
interesting things. Well, if you can understand these things, what else can you
understand

01:09:04.140 --> 01:09:09.500
that you've digitized? The reason why we started with text and, you know, images
is because

01:09:09.500 --> 01:09:13.320
we digitized those. But what else have we digitized? Well, it turns out we
digitized

01:09:13.320 --> 01:09:21.520
a lot of things. Proteins and genes and brain waves. Anything you can digitize,
so long

01:09:21.520 --> 01:09:25.120
as there's structure, we can probably learn some patterns from it. And if we can
learn

01:09:25.120 --> 01:09:28.780
the patterns from it, we can understand its meaning. If we can understand its
meaning,

01:09:29.460 --> 01:09:34.920
we might be able to generate it as well. And so, therefore, the generative AI
revolution

01:09:34.920 --> 01:09:39.480
is here. Well, what else can we generate? What else can we learn? Well, one of
the things

01:09:39.480 --> 01:09:47.160
that we would love to learn, we would love to learn, is we would love to learn
climate.

01:09:47.160 --> 01:09:55.740
We would love to learn extreme weather. We would love to learn how we can
predict future

01:09:55.740 --> 01:10:02.700
weather at regional scales, at sufficiently high resolution, such that we can
keep people

01:10:02.700 --> 01:10:09.000
out of harm's way before harm comes. Extreme weather cost the world $150
billion, surely

01:10:09.000 --> 01:10:09.460
more than that. But we can do it. We can do it. We can do it. We can do it. We
can do it.

01:10:09.460 --> 01:10:09.620
We can do it. We can do it. We can do it. We can do it. We can do it. We can do
it. We can do it.

01:10:09.620 --> 01:10:14.740
And that, it's not evenly distributed. $150 billion is concentrated in some
parts of the

01:10:14.740 --> 01:10:19.300
world, and of course, to some people of the world. We need to adapt, and we need
to know

01:10:19.300 --> 01:10:25.460
what's coming. And so, we're creating Earth 2, a digital twin of the Earth for
predicting weather.

01:10:25.460 --> 01:10:32.680
And we've made an extraordinary invention called CoreDiv, the ability to use
generative AI

01:10:32.680 --> 01:10:36.760
to predict weather at extremely high resolution. Let's take a look.

01:10:39.460 --> 01:10:44.740
As the Earth's climate changes, AI-powered weather forecasting is allowing us to
more accurately

01:10:44.740 --> 01:10:50.740
predict and track severe storms, like Super Typhoon Chanthu, which caused
widespread damage in Taiwan

01:10:50.740 --> 01:10:56.580
and the surrounding region in 2021. Current AI forecast models can accurately
predict the track

01:10:56.580 --> 01:11:02.180
of storms, but they are limited to 25-kilometer resolution, which can miss
important details.

01:11:02.900 --> 01:11:08.980
NVIDIA's CoreDiv is a revolutionary new generative AI model trained on high-
resolution radar-assimilated

01:11:08.980 --> 01:11:16.580
WARF weather forecasts and ERA5 reanalysis data. Using CoreDiv, extreme events
like Chanthu can

01:11:16.580 --> 01:11:22.820
be super-resolved from 25-kilometer to 2-kilometer resolution, with 1,000 times
the speed and 3,000

01:11:22.820 --> 01:11:27.860
times the energy efficiency of conventional weather models. By combining the
speed and

01:11:27.860 --> 01:11:33.700
accuracy of NVIDIA's weather forecasting model, ForecastNet, and generative AI
models like CoreDiv,

01:11:33.700 --> 01:11:38.740
we can explore hundreds or even thousands of kilometer-scale regional weather
forecasts,

01:11:38.740 --> 01:11:43.460
to provide a clear picture of the best, worst, and most likely impacts of a
storm.

01:11:44.260 --> 01:11:48.260
This wealth of information can help minimize loss of life and property damage.

01:11:48.900 --> 01:11:54.260
Today, CoreDiv is optimized for Taiwan, but soon, generative supersampling will
be available

01:11:54.260 --> 01:11:58.580
as part of the NVIDIA Earth-2 inference service for many regions across the
globe.

01:12:08.740 --> 01:12:15.380
The weather company has to trust the source of global weather prediction. We are
working together

01:12:15.380 --> 01:12:21.620
to accelerate their weather simulation, first principled base of simulation.
However, they're

01:12:21.620 --> 01:12:28.180
also going to integrate Earth-2 CoreDiv so that they can help businesses and
countries do regional

01:12:28.180 --> 01:12:33.220
high-resolution weather prediction. And so, if you have some weather prediction
you'd like to know,

01:12:33.220 --> 01:12:38.580
like to do, reach out to the weather company. Really exciting, really exciting
work. NVIDIA has

01:12:38.580 --> 01:12:43.620
healthcare. Something we started 15 years ago. We're super, super excited about
this. This is

01:12:43.620 --> 01:12:49.460
an area where we're very, very proud. Whether it's medical imaging or gene
sequencing or computational

01:12:49.460 --> 01:12:56.820
chemistry, it is very likely that NVIDIA is the computation behind it. We've
done so much work in

01:12:56.820 --> 01:13:04.020
this area. Today, we're announcing that we're going to do something really,
really cool. Imagine

01:13:04.900 --> 01:13:07.460
all of these AI models that are being used

01:13:08.580 --> 01:13:15.380
to generate images and audio, but instead of images and audio, because it
understood images

01:13:15.380 --> 01:13:21.860
and audio, all the digitization that we've done for genes and proteins and amino
acids,

01:13:21.860 --> 01:13:28.580
that digitization capability is now passed through machine learning so that we
understand

01:13:28.580 --> 01:13:32.980
the language of life. The ability to understand the language of life,

01:13:32.980 --> 01:13:38.580
of course, we saw the first evidence of it with AlphaFold. This is really quite
a

01:13:38.580 --> 01:13:44.100
extraordinary thing. After decades of painstaking work, the world had only
digitized

01:13:45.300 --> 01:13:53.540
and reconstructed using cryo-electron microscopy or x-ray crystallography. These
different

01:13:53.540 --> 01:13:59.700
techniques painstakingly reconstructed the protein, 200,000 of them, in just,
what is it,

01:13:59.700 --> 01:14:08.020
less than a year or so? AlphaFold has reconstructed 200 million proteins.
Basically, every protein,

01:14:08.580 --> 01:14:13.860
every living thing that's ever been sequenced. This is completely revolutionary.
Well,

01:14:13.860 --> 01:14:19.940
those models are incredibly hard to use, incredibly hard for people to build,
and so what we're going

01:14:19.940 --> 01:14:24.980
to do is we're going to build them. We're going to build them for the
researchers around the world,

01:14:24.980 --> 01:14:28.500
and it won't be the only one. There'll be many other models that we create,

01:14:28.500 --> 01:14:30.500
and so let me show you what we're going to do with it.

01:14:35.700 --> 01:14:38.420
Virtual Screening for New Medicines is a computational

01:14:38.580 --> 01:14:43.700
and tractable problem. Existing techniques can only scan billions of compounds

01:14:43.700 --> 01:14:48.820
and require days on thousands of standard compute nodes to identify new drug
candidates.

01:14:49.860 --> 01:14:55.700
NVIDIA BioNemo NIMS enable a new generative screening paradigm. Using NIMS for
protein

01:14:55.700 --> 01:15:01.540
structure prediction with AlphaFold, molecule generation with MolMIM, and
docking with DiffDock,

01:15:02.100 --> 01:15:08.020
we can now generate and screen candidate molecules in a matter of minutes.
MolMIM can connect to

01:15:08.020 --> 01:15:13.140
custom applications to steer the generative process, iteratively optimizing for
desired

01:15:13.140 --> 01:15:19.220
properties. These applications can be defined with BioNemo micro services or
built from scratch.

01:15:20.180 --> 01:15:26.340
Here, a physics-based simulation optimizes for a molecule's ability to bind to a
target protein

01:15:26.340 --> 01:15:30.100
while optimizing for other favorable molecular properties in parallel.

01:15:30.900 --> 01:15:36.260
MolMIM generates high quality, drug-like molecules that bind to the target and
are synthesizable,

01:15:36.260 --> 01:15:41.260
translating to a higher probability of developing successful medicines faster.

01:15:42.260 --> 01:15:46.260
BioNemo is enabling a new paradigm in drug discovery with NIMS,

01:15:46.260 --> 01:15:52.260
providing on-demand microservices that can be combined to build powerful drug
discovery workflows

01:15:52.260 --> 01:15:57.260
like de novo protein design or guided molecule generation for virtual screening.

01:15:58.260 --> 01:16:03.260
BioNemo NIMS are helping researchers and developers reinvent computational drug
design.

01:16:06.260 --> 01:16:10.260
�

01:16:12.220 --> 01:16:16.260
NVIDIA and MulMIM, MulMIM, CORE, DIF, there are a whole bunch of other models.

01:16:17.260 --> 01:16:26.260
Computer vision models, robotics models, and even, of course, some really,
really fantastic open-source language models.

01:16:27.260 --> 01:16:32.260
These models are groundbreaking. However, it is hard for companies to use.

01:16:33.260 --> 01:16:36.260
How would you use it? How would you bring it into your company and integrate it
to your work environment?

01:16:36.260 --> 01:16:41.360
flow how would you package it up and run it remember earlier I just said that

01:16:41.360 --> 01:16:45.860
inference is an extraordinary computation problem how would you do the

01:16:45.860 --> 01:16:50.560
optimization for each and every one of these models and put together the

01:16:50.560 --> 01:16:54.920
computing stack necessary to run that supercomputer so that you can run these

01:16:54.920 --> 01:17:00.560
models in your company and so we have a great idea we're gonna invent a new way

01:17:00.560 --> 01:17:08.800
and invent a new way for you to receive and operate software this software

01:17:08.800 --> 01:17:15.980
comes basically in a digital box we call it a container and we call it the

01:17:15.980 --> 01:17:21.860
NVIDIA inference micro service a NIM and only to explain to you what it is a

01:17:21.860 --> 01:17:28.520
NIM it's a pre-trained model so it's pretty clever and it is packaged and

01:17:28.520 --> 01:17:30.540
optimized to run across any

01:17:30.540 --> 01:17:35.480
NVIDIA's install base which is very very large what's inside it is

01:17:35.480 --> 01:17:39.540
incredible you have all these pre-trained state-of-the-art open source

01:17:39.540 --> 01:17:43.200
models they could be open source they could be from one of our partners it

01:17:43.200 --> 01:17:47.760
could be created by us like NVIDIA moment it is packaged up with all of its

01:17:47.760 --> 01:17:53.940
dependencies so CUDA the right version cu DNN the right version tensor RT LLM

01:17:53.940 --> 01:17:58.860
distributed across the multiple GPUs try an inference server all completely

01:17:58.860 --> 01:18:00.160
packaged together

01:18:00.540 --> 01:18:05.640
it's optimized depending on whether you have a single GPU multi GPU or multi
node

01:18:05.640 --> 01:18:10.620
of GPUs it's optimized for that and it's connected up with API's that are simple

01:18:10.620 --> 01:18:18.000
to use now this think about what an AI API is an AI API is an interface that

01:18:18.000 --> 01:18:23.040
you just talk to and so this is a piece of software in the future that has a

01:18:23.040 --> 01:18:29.340
really simple API and that API is called human and these packages incredible

01:18:29.340 --> 01:18:30.400
bodies of software

01:18:30.400 --> 01:18:36.340
will be optimized and packaged and we'll put it on a website and you can

01:18:36.340 --> 01:18:41.360
download it you could take it with you you can run it in any cloud you can run

01:18:41.360 --> 01:18:44.900
it in your own data center you weren't running workstations of a fit and all

01:18:44.900 --> 01:18:49.960
you have to do is come to AI dot NVIDIA comm we call it NVIDIA inference

01:18:49.960 --> 01:18:56.340
microservice but inside the company we all call it NIMS okay

01:18:56.340 --> 01:18:58.360
you

01:19:00.400 --> 01:19:06.300
just imagine you know one of some someday there there's going to be one of

01:19:06.300 --> 01:19:11.420
these chat bots and these chat bots is gonna just be in the NIM and you know

01:19:11.420 --> 01:19:15.200
you'll like you'll assemble a whole bunch of chat bots and that's the way

01:19:15.200 --> 01:19:18.800
software is going to give you be built someday how do we build software in the

01:19:18.800 --> 01:19:23.340
future it is unlikely that you'll write it from scratch or write a whole bunch

01:19:23.340 --> 01:19:28.080
of Python code or anything like that it is very likely that you assemble a team

01:19:28.080 --> 01:19:29.900
of AI's

01:19:30.400 --> 01:19:36.220
going to be a super AI that you use that takes the mission that you give it and
breaks it

01:19:36.220 --> 01:19:38.980
down into an execution plan.

01:19:38.980 --> 01:19:42.900
Some of that execution plan could be handed off to another NIM.

01:19:42.900 --> 01:19:47.380
That NIM would maybe understand SAP.

01:19:47.380 --> 01:19:49.540
The language of SAP is ABAP.

01:19:49.540 --> 01:19:55.260
It might understand ServiceNow and go retrieve some information from their
platforms.

01:19:55.260 --> 01:19:59.960
It might then hand that result to another NIM who goes off and does some
calculation

01:19:59.960 --> 01:20:00.960
on it.

01:20:00.960 --> 01:20:06.920
Maybe it's an optimization software, a combinatorial optimization algorithm.

01:20:06.920 --> 01:20:11.060
Maybe it's just some basic calculator.

01:20:11.060 --> 01:20:15.240
Maybe it's Pandas to do some numerical analysis on it.

01:20:15.240 --> 01:20:20.800
And then it comes back with its answer, and it gets combined with everybody
else's.

01:20:20.800 --> 01:20:25.240
And because it's been presented with, this is what the right answer should look
like,

01:20:25.240 --> 01:20:30.660
it knows what right answers to produce, and it presents it to you.

01:20:30.660 --> 01:20:34.700
We can get a report every single day at, you know, top of the hour that has
something to

01:20:34.700 --> 01:20:40.080
do with a build plan or some forecast or some customer alert or some bugs
database or whatever

01:20:40.080 --> 01:20:44.460
it happens to be, and we could assemble it using all these NIMs.

01:20:44.460 --> 01:20:49.760
And because these NIMs have been packaged up and ready to work on your systems,
so long

01:20:49.760 --> 01:20:54.980
as you have NVIDIA GPUs in your data center or in the cloud, these NIMs will
work together.

01:20:54.980 --> 01:20:57.660
Together as a team and do amazing things.

01:20:57.660 --> 01:21:02.800
And so we decided, this is such a great idea, we're going to go do that.

01:21:02.800 --> 01:21:05.500
And so NVIDIA has NIMs running all over the company.

01:21:05.500 --> 01:21:08.500
We have chatbots being created all over the place.

01:21:08.500 --> 01:21:13.720
And one of the most important chatbots, of course, is a chip designer chatbot.

01:21:13.720 --> 01:21:14.800
You might not be surprised.

01:21:14.800 --> 01:21:17.100
We care a lot about building chips.

01:21:17.100 --> 01:21:24.980
And so we want to build chatbots, AI co-pilots that are co-designers with our
engineers.

01:21:24.980 --> 01:21:27.080
And so this is the way we did it.

01:21:27.080 --> 01:21:30.400
So we got ourselves a Lama 2.

01:21:30.400 --> 01:21:34.060
This is a 70B, and it's packaged up in a NIM.

01:21:34.060 --> 01:21:39.160
And we asked it, what is a CTL?

01:21:39.160 --> 01:21:45.720
It turns out CTL is an internal program, and it has an internal proprietary
language.

01:21:45.720 --> 01:21:48.680
But it thought the CTL was a combinatorial timing logic.

01:21:48.680 --> 01:21:51.760
And so it describes conventional knowledge of CTL.

01:21:51.760 --> 01:21:54.560
But that's not very useful to us.

01:21:54.560 --> 01:21:58.440
And so we gave it a whole bunch of new examples.

01:21:58.440 --> 01:22:02.320
This is no different than onboarding an employee.

01:22:02.320 --> 01:22:04.380
We say, thanks for that answer.

01:22:04.380 --> 01:22:07.120
It's completely wrong.

01:22:07.120 --> 01:22:10.980
And then we present to them, this is what a CTL is.

01:22:10.980 --> 01:22:13.940
And so this is what a CTL is at NVIDIA.

01:22:13.940 --> 01:22:20.580
And the CTL, as you can see, CTL stands for Compute Trace Library, which makes
sense.

01:22:20.580 --> 01:22:22.980
We're tracing compute cycles all the time.

01:22:22.980 --> 01:22:23.740
And it wrote the program.

01:22:23.740 --> 01:22:24.460
And so we gave it a whole bunch of new examples.

01:22:24.460 --> 01:22:24.540
And so this is what a CTL is at NVIDIA.

01:22:24.540 --> 01:22:25.040
And so this is what a CTL is at NVIDIA.

01:22:25.040 --> 01:22:25.660
Isn't that amazing?

01:22:25.660 --> 01:22:35.280
And so the productivity of our chip designers can go up.

01:22:35.520 --> 01:22:36.740
This is what you can do with a NIM.

01:22:36.880 --> 01:22:38.740
First thing you can do with it is customize it.

01:22:39.000 --> 01:22:46.940
We have a service called Nemo Microservice that helps you curate the data,
preparing the data so that you can teach this onboard this AI.

01:22:47.460 --> 01:22:50.560
You fine-tune them, and then you guardrail it.

01:22:50.560 --> 01:22:54.440
You can even evaluate the answer, evaluate its performance against others.

01:22:54.440 --> 01:22:55.420
This is what we do.

01:22:55.420 --> 01:22:58.320
And this could help you with other examples.

01:22:58.320 --> 01:23:01.520
And so that's called the Nemo Microservice.

01:23:01.520 --> 01:23:06.060
Now, the thing that's emerging here is this – there are three elements, three
pillars of what we're doing.

01:23:06.060 --> 01:23:14.600
The first pillar is, of course, inventing the technology for AI models and
running AI models and packaging it up for you.

01:23:14.600 --> 01:23:18.620
The second is to create tools to help you modify it.

01:23:18.620 --> 01:23:20.300
First is having the AI technology.

01:23:20.300 --> 01:23:21.740
Second is to help you modify it.

01:23:21.740 --> 01:23:24.340
And third is infrastructure for you to fine-tune it.

01:23:24.340 --> 01:23:29.460
you could deploy it on our infrastructure called dgx cloud or you can employ
deploy it on-prem you

01:23:29.460 --> 01:23:35.060
can deploy it anywhere you like once you develop it it's yours to take anywhere
and so we are

01:23:35.060 --> 01:23:44.100
effectively an ai foundry we will do for you and the industry on ai what tsmc
does for us building

01:23:44.100 --> 01:23:50.340
chips and so we go to it with our go to tsmc with our big ideas they manufacture
and we take it with

01:23:50.340 --> 01:23:57.860
us and so exactly the same thing here ai foundry and the three pillars are the
nims nemo microservice

01:23:57.860 --> 01:24:03.620
and dgx cloud the other thing that you could teach the nim to do is to
understand your proprietary

01:24:03.620 --> 01:24:08.980
information remember inside our company the vast majority of our data is not in
the cloud it's

01:24:08.980 --> 01:24:15.300
inside our company it's been sitting there you know being used all the time and
and gosh it's

01:24:15.300 --> 01:24:19.700
it's basically nvidia's intelligence we would like to take that data

01:24:20.660 --> 01:24:24.900
learn its meaning like we learned the meaning of almost anything else that we
just talked about

01:24:24.900 --> 01:24:31.460
learn its meaning and then re-index that knowledge into a new type of database
called a vector

01:24:31.460 --> 01:24:37.460
database and so you essentially take structured data or unstructured data you
learn its meaning

01:24:37.940 --> 01:24:45.060
you encode its meaning so now this becomes an ai database and that ai database
in the future once

01:24:45.060 --> 01:24:49.220
you create it you can talk to it and so let me give you an example of what you
could do so suppose

01:24:49.220 --> 01:24:50.260
you create an ai database

01:24:50.340 --> 01:24:55.540
You've got a whole bunch of multi-modality data, and one good example of that is
PDF.

01:24:56.080 --> 01:25:00.880
So you take the PDF, you take all of your PDFs, all your favorite, you know,

01:25:00.980 --> 01:25:04.580
the stuff that is proprietary to you, critical to your company.

01:25:04.900 --> 01:25:10.540
You can encode it just as we encode the pixels of a cat, and it becomes the word
cat.

01:25:10.920 --> 01:25:17.700
We can encode all of your PDF, and it turns into vectors that are now stored
inside your vector database.

01:25:17.700 --> 01:25:20.100
It becomes the proprietary information of your company.

01:25:20.600 --> 01:25:23.500
And once you have that proprietary information, you can chat to it.

01:25:24.440 --> 01:25:27.940
It's a smart database, so you just chat with data.

01:25:28.500 --> 01:25:30.600
And how much more enjoyable is that?

01:25:31.120 --> 01:25:37.840
You know, for our software team, you know, they just chat with the bugs
database, you know.

01:25:38.200 --> 01:25:39.960
How many bugs was there last night?

01:25:40.260 --> 01:25:41.560
Are we making any progress?

01:25:41.840 --> 01:25:47.040
And then after you're done talking to this bugs database, you need therapy.

01:25:47.980 --> 01:25:50.320
And so we have another chat.

01:25:50.600 --> 01:25:52.600
I'll give you a chat bot for you.

01:25:55.600 --> 01:25:57.600
You can do it.

01:26:06.600 --> 01:26:13.600
Okay, so we call this Nemo Retriever, and the reason for that is because
ultimately its job is to go retrieve information as quickly as possible.

01:26:13.600 --> 01:26:14.600
And you just talk to it.

01:26:14.600 --> 01:26:16.600
Hey, retrieve me this information.

01:26:16.600 --> 01:26:18.600
It brings it back to you.

01:26:18.600 --> 01:26:19.600
Do you mean this?

01:26:19.600 --> 01:26:21.600
And you go, yeah, perfect, okay.

01:26:21.600 --> 01:26:23.600
And so we call it the Nemo Retriever.

01:26:23.600 --> 01:26:27.600
Well, the Nemo service helps you create all these things, and we have all these
different NIMs.

01:26:27.600 --> 01:26:29.600
We even have NIMs of digital humans.

01:26:29.600 --> 01:26:33.600
I'm Rachel, your AI care manager.

01:26:33.600 --> 01:26:41.600
Okay, so it's a really short clip, but there were so many videos to show you, I
guess, so many other demos to show you.

01:26:41.600 --> 01:26:43.600
And so I had to cut this one short.

01:26:43.600 --> 01:26:45.600
But this is Diana.

01:26:45.600 --> 01:26:47.600
She is a digital human NIM.

01:26:47.860 --> 01:26:48.860
And you just saw her.

01:26:48.860 --> 01:26:50.860
You just talked to her.

01:26:50.860 --> 01:26:55.860
And she's connected, in this case, to Hippocratic AI's large language model for
healthcare.

01:26:55.860 --> 01:26:57.860
And it's truly amazing.

01:26:59.860 --> 01:27:02.860
She is just super smart about healthcare things.

01:27:02.860 --> 01:27:03.860
You know?

01:27:03.860 --> 01:27:12.860
And so after Dwight, my VP of software engineering, talks to the chat bot for
Bugs database, then you come over here and talk to Diane.

01:27:12.860 --> 01:27:17.860
And so Diane is completely animated with AI.

01:27:17.860 --> 01:27:19.860
And she's a digital human.

01:27:19.860 --> 01:27:22.860
There's so many companies that would like to build.

01:27:22.860 --> 01:27:24.860
They're sitting on gold mines.

01:27:24.860 --> 01:27:28.860
The enterprise IT industry is sitting on a gold mine.

01:27:28.860 --> 01:27:34.860
It's a gold mine because they have so much understanding of the way work is
done.

01:27:34.860 --> 01:27:37.860
They have all these amazing tools that have been created over the years.

01:27:37.860 --> 01:27:39.860
And they're sitting on a lot of data.

01:27:39.860 --> 01:27:46.860
If they could take that gold mine and turn them into co-pilots, these co-pilots
could help us do things.

01:27:46.860 --> 01:27:55.860
And so just about every IT franchise, IT platform in the world that has valuable
tools that people use is sitting on a gold mine for co-pilots.

01:27:55.860 --> 01:27:58.860
And they would like to build their own co-pilots and their own chat bots.

01:27:58.860 --> 01:28:03.860
And so we're announcing that NVIDIA AI Foundry is working with some of the
world's great companies.

01:28:03.860 --> 01:28:07.860
SAP generates 87% of the world's global commerce.

01:28:07.860 --> 01:28:09.860
Basically, the world runs on SAP.

01:28:09.860 --> 01:28:10.860
We run on SAP.

01:28:10.860 --> 01:28:15.860
NVIDIA and SAP are building SAP Jewel co-pilots using NVIDIA NEMO.

01:28:15.860 --> 01:28:18.860
We're using NVIDIA NEMO and DGX Cloud.

01:28:18.860 --> 01:28:27.860
ServiceNow, they run 85% of the world's Fortune 500 companies run their people
and customer service operations on ServiceNow.

01:28:27.860 --> 01:28:35.860
And they're using NVIDIA AI Foundry to build ServiceNow Assist virtual
assistants.

01:28:35.860 --> 01:28:37.860
Cohesity backs up the world's data.

01:28:37.860 --> 01:28:39.860
They're sitting on a gold mine of data.

01:28:39.860 --> 01:28:43.860
Hundreds of exabytes of data, over 10,000 companies.

01:28:43.860 --> 01:28:45.860
NVIDIA AI Foundry is working with them.

01:28:45.860 --> 01:28:49.860
They build their Gaia generative AI agent.

01:28:49.860 --> 01:29:02.860
Snowflake is a company that stores the world's digital warehouse in the cloud
and serves over 3 billion queries a day for 10,000 enterprise customers.

01:29:02.860 --> 01:29:08.860
Snowflake is working with NVIDIA AI Foundry to build co-pilots with NVIDIA NEMO
and NIMS.

01:29:08.860 --> 01:29:14.860
NetApp, nearly half of the files in the world are stored on-prem on NetApp.

01:29:14.860 --> 01:29:24.860
NVIDIA AI Foundry is helping them build chatbots and co-pilots like those vector
databases and retrievers with NVIDIA NEMO and NIMS.

01:29:24.860 --> 01:29:27.860
And we have a great partnership with Dell.

01:29:27.860 --> 01:29:37.860
Everybody who is building these chatbots and generative AI, when you're ready to
run it, you're going to need an AI factory.

01:29:37.860 --> 01:29:43.860
And nobody is better at building end-to-end systems of very large scale for the
enterprise.

01:29:43.860 --> 01:29:49.860
And so anybody, any company, every company will need to build AI factories.

01:29:49.860 --> 01:29:51.860
And it turns out that Michael is here.

01:29:51.860 --> 01:29:53.860
He's happy to take your order.

01:29:57.860 --> 01:29:59.860
Ladies and gentlemen, Michael Dell.

01:30:04.860 --> 01:30:11.860
Okay, let's talk about the next wave of robotics, the next wave of AI, robotics,
physical AI.

01:30:11.860 --> 01:30:16.860
So far, all of the AI that we've talked about is one computer.

01:30:16.860 --> 01:30:23.860
Data comes into one computer, lots of the world's, if you will, experience in
digital text form.

01:30:23.860 --> 01:30:30.860
The AI imitates us by reading a lot of the language to predict the next words.

01:30:30.860 --> 01:30:34.860
It's imitating you by studying all of the patterns and all the other previous
examples.

01:30:34.860 --> 01:30:37.860
Of course, it has to understand context and so on and so forth.

01:30:37.860 --> 01:30:40.860
But once it understands the context, it's essentially imitating you.

01:30:40.860 --> 01:30:42.860
We take all of the data.

01:30:42.860 --> 01:30:44.860
We put it into a system like DGX.

01:30:44.860 --> 01:30:47.860
We compress it into a large language model.

01:30:47.860 --> 01:30:50.860
Trillions and trillions of parameters become billions and billions.

01:30:50.860 --> 01:30:52.860
Trillions of tokens becomes billions of parameters.

01:30:52.860 --> 01:30:55.860
These billions of parameters becomes your AI.

01:30:55.860 --> 01:31:04.860
Well, in order for us to go to the next wave of AI where the AI understands the
physical world, we're going to need three computers.

01:31:04.860 --> 01:31:06.860
The first computer is still the same computer.

01:31:06.860 --> 01:31:09.860
It's that AI computer that now is going to be watching video.

01:31:09.860 --> 01:31:12.860
And maybe it's doing synthetic data generation.

01:31:12.860 --> 01:31:15.860
And maybe there's a lot of human examples.

01:31:15.860 --> 01:31:21.860
Just as we have human examples in text form, we're going to have human examples
in articulation form.

01:31:21.860 --> 01:31:31.860
And the AIs will watch us, understand what is happening, and try to adapt it for
themselves into the context.

01:31:31.860 --> 01:31:39.860
And because it can generalize with these foundation models, maybe these robots
can also perform in the physical world a fairly general way.

01:31:39.860 --> 01:31:50.860
So I just described in very simple terms essentially what just happened in large
language models, except the chat GPT moment for robotics may be right around the
corner.

01:31:50.860 --> 01:31:54.860
And so we've been building the end-to-end systems for robotics for some time.

01:31:54.860 --> 01:31:56.860
I'm super, super proud of the work.

01:31:56.860 --> 01:31:59.860
We have the AI system, DGX.

01:31:59.860 --> 01:32:03.860
We have the lower system, which is called AGX, for autonomous systems.

01:32:03.860 --> 01:32:05.860
The world's first robotics processor.

01:32:05.860 --> 01:32:08.860
When we first built this thing, people were, what are you guys building?

01:32:08.860 --> 01:32:09.860
It's an SOC.

01:32:09.860 --> 01:32:10.860
It's one chip.

01:32:10.860 --> 01:32:12.860
It's designed to be very low power.

01:32:12.860 --> 01:32:16.860
But it's designed for high-speed sensor processing and AI.

01:32:16.860 --> 01:32:27.860
And so if you want to run transformers in a car or you want to run transformers
in anything that moves, we have the perfect computer for you.

01:32:27.860 --> 01:32:28.860
It's called the Jetson.

01:32:28.860 --> 01:32:31.860
And so the DGX on top for training the AI.

01:32:31.860 --> 01:32:33.860
The Jetson is the autonomous processor.

01:32:33.860 --> 01:32:37.860
And in the middle, we need another computer.

01:32:37.860 --> 01:32:51.860
Because large language models have the benefit of you providing your examples
and then doing reinforcement learning human feedback, what is the reinforcement
learning human feedback of a robot?

01:32:51.860 --> 01:32:55.860
Well, it's reinforcement learning physical feedback.

01:32:55.860 --> 01:32:57.860
That's how you align the robot.

01:32:57.860 --> 01:33:06.860
That's how the robot knows that as it's learning these articulation capabilities
and manipulation capabilities, it's going to adapt properly into the laws of
physics.

01:33:07.860 --> 01:33:18.860
And so we need a simulation engine that represents the world digitally for the
robot so that the robot has a gym to go learn how to be a robot.

01:33:18.860 --> 01:33:23.860
We call that virtual world Omniverse.

01:33:23.860 --> 01:33:26.860
And the computer that runs Omniverse is called OVX.

01:33:26.860 --> 01:33:31.860
And OVX, the computer itself, is hosted in the Azure cloud.

01:33:31.860 --> 01:33:32.860
Okay?

01:33:32.860 --> 01:33:35.860
And so basically we built these three things, these three systems.

01:33:35.860 --> 01:33:37.860
On top of it, we have algorithms for every single robot.

01:33:37.860 --> 01:33:39.860
That's all one.

01:33:39.860 --> 01:33:45.860
Now, I'm going to show you one super example of how AI and Omniverse are going
to work together.

01:33:45.860 --> 01:33:48.860
The example I'm going to show you is kind of insane.

01:33:48.860 --> 01:33:51.860
But it's going to be very, very close to tomorrow.

01:33:51.860 --> 01:33:53.860
It's a robotics building.

01:33:53.860 --> 01:33:55.860
This robotics building is called a warehouse.

01:33:55.860 --> 01:33:59.860
Inside the robotics building are going to be some autonomous systems.

01:33:59.860 --> 01:34:02.860
Some of the autonomous systems are going to be called humans.

01:34:02.860 --> 01:34:06.860
And some of the autonomous systems are going to be called forklifts.

01:34:06.860 --> 01:34:11.860
These autonomous systems are going to interact with each other, of course,
autonomously.

01:34:11.860 --> 01:34:16.860
And it's going to be overlooked upon by this warehouse to keep everybody out of
harm's way.

01:34:16.860 --> 01:34:19.860
The warehouse is essentially an air traffic controller.

01:34:19.860 --> 01:34:25.860
And whenever it sees something happening, it will redirect traffic and give new
waypoints,

01:34:25.860 --> 01:34:28.860
just new waypoints to the robots and the people.

01:34:28.860 --> 01:34:30.860
And they'll know exactly what to do.

01:34:30.860 --> 01:34:34.860
This warehouse, this building, you can also talk to.

01:34:34.860 --> 01:34:35.860
Of course you can talk to it.

01:34:35.860 --> 01:34:38.860
Hey, you know, SAP Center, how are you feeling today?

01:34:38.860 --> 01:34:40.860
For example.

01:34:40.860 --> 01:34:44.860
And so you could ask the warehouse the same questions.

01:34:44.860 --> 01:34:51.860
Basically, the system I just described will have omniverse cloud that's hosting
the virtual simulation

01:34:51.860 --> 01:34:54.860
and AI running on DGX cloud.

01:34:54.860 --> 01:34:56.860
And all of this is running in real time.

01:34:56.860 --> 01:34:57.860
Let's take a look.

01:34:59.860 --> 01:35:03.860
The future of heavy industries starts as a digital twin.

01:35:03.860 --> 01:35:05.860
The AI agents helping robots.

01:35:05.860 --> 01:35:11.860
Robots, workers, and infrastructure navigate unpredictable events in complex
industrial spaces

01:35:11.860 --> 01:35:16.860
will be built and evaluated first in sophisticated digital twins.

01:35:16.860 --> 01:35:20.860
This omniverse digital twin of a 100,000 square foot warehouse

01:35:20.860 --> 01:35:24.860
is operating as a simulation environment that integrates digital workers,

01:35:24.860 --> 01:35:28.860
AMRs running the NVIDIA ISAAC receptor stack,

01:35:28.860 --> 01:35:33.860
centralized activity maps of the entire warehouse from 100 simulated ceiling
mount cameras

01:35:33.860 --> 01:35:35.860
using NVIDIA Metropolis,

01:35:35.860 --> 01:35:38.860
and AMR route planning with NVIDIA Coopt.

01:35:38.860 --> 01:35:44.860
Software-in-loop testing of AI agents in this physically accurate simulated
environment

01:35:44.860 --> 01:35:50.860
enables us to evaluate and refine how the system adapts to real-world
unpredictability.

01:35:50.860 --> 01:35:57.860
Here, an incident occurs along this AMR's planned route, blocking its path as it
moves to pick up a pallet.

01:35:57.860 --> 01:36:04.860
NVIDIA Metropolis updates and sends a real-time occupancy map to Coopt where a
new optimal route is calculated.

01:36:05.860 --> 01:36:10.860
The AMR is enabled to see around corners and improve its mission efficiency.

01:36:10.860 --> 01:36:14.860
With generative AI-powered Metropolis vision foundation models,

01:36:14.860 --> 01:36:18.860
operators can even ask questions using natural language.

01:36:18.860 --> 01:36:24.860
The visual model understands nuanced activity and can offer immediate insights
to improve operations.

01:36:24.860 --> 01:36:29.860
All of the sensor data is created in simulation and passed to the real-time AI

01:36:29.860 --> 01:36:33.860
running as NVIDIA Inference Microservices, or NIMS.

01:36:33.860 --> 01:36:37.860
And when the AI is ready to be deployed in the physical twin, the real
warehouse,

01:36:37.860 --> 01:36:41.860
we connect Metropolis and Isaac NIMS to real sensors

01:36:41.860 --> 01:36:47.860
with the ability for continuous improvement of both the digital twin and the AI
models.

01:36:50.860 --> 01:36:52.860
Isn't that incredible?

01:36:52.860 --> 01:36:57.860
And so, remember, remember,

01:36:57.860 --> 01:37:02.860
a future facility, warehouse, factory, building,

01:37:02.860 --> 01:37:04.860
will be software-defined.

01:37:04.860 --> 01:37:06.860
And so the software is running.

01:37:06.860 --> 01:37:08.860
How else would you test the software?

01:37:08.860 --> 01:37:11.860
So you test the software to building the warehouse,

01:37:11.860 --> 01:37:14.860
the optimization system in the digital twin.

01:37:14.860 --> 01:37:15.860
What about all the robots?

01:37:15.860 --> 01:37:17.860
All of those robots you were seeing just now,

01:37:17.860 --> 01:37:20.860
they're all running their own autonomous robotic stack.

01:37:20.860 --> 01:37:24.860
And so the way you integrate software in the future, CICD in the future,

01:37:24.860 --> 01:37:27.860
for robotic systems is with digital twins.

01:37:27.860 --> 01:37:31.860
We've made Omniverse a lot easier to access.

01:37:31.860 --> 01:37:34.860
Basically Omniverse Cloud APIs,

01:37:34.860 --> 01:37:36.860
four simple API and a channel,

01:37:36.860 --> 01:37:38.860
and you can connect your application to it.

01:37:38.860 --> 01:37:42.860
So this is going to be as wonderfully, beautifully simple

01:37:42.860 --> 01:37:45.860
in the future that Omniverse is going to be.

01:37:45.860 --> 01:37:46.860
And with these APIs,

01:37:46.860 --> 01:37:50.860
you're going to have these magical digital twin capability.

01:37:50.860 --> 01:37:55.860
We also have turned Omniverse into an AI

01:37:55.860 --> 01:37:58.860
and integrated it with the ability to chat USD,

01:37:58.860 --> 01:38:00.860
the language of,

01:38:00.860 --> 01:38:03.860
our language is human and Omniverse's language,

01:38:03.860 --> 01:38:06.860
as it turns out, is universal scene description.

01:38:06.860 --> 01:38:09.860
And so that language is rather complex.

01:38:09.860 --> 01:38:12.860
And so we've taught our Omniverse that language.

01:38:12.860 --> 01:38:16.860
And so you can speak to it in English and it would directly generate USD.

01:38:16.860 --> 01:38:20.860
And it would talk back in USD but converse back to you in English.

01:38:20.860 --> 01:38:24.860
You could also look for information in this world semantically.

01:38:24.860 --> 01:38:27.860
Instead of the world being encoded semantically in language,

01:38:27.860 --> 01:38:29.860
now it's encoded semantically in scenes.

01:38:29.860 --> 01:38:33.860
And so you could ask it of certain objects

01:38:33.860 --> 01:38:35.860
or certain conditions or certain scenarios,

01:38:35.860 --> 01:38:37.860
and it can go and find that scenario for you.

01:38:37.860 --> 01:38:40.860
It also can collaborate with you in generation.

01:38:40.860 --> 01:38:42.860
You could design some things in 3D,

01:38:42.860 --> 01:38:44.860
it could simulate some things in 3D,

01:38:44.860 --> 01:38:46.860
or you could use AI to generate something in 3D.

01:38:46.860 --> 01:38:49.860
Let's take a look at how this is all going to work.

01:38:49.860 --> 01:38:51.860
We have a great partnership with Siemens.

01:38:51.860 --> 01:38:57.860
Siemens is the world's largest industrial engineering and operations platform.

01:38:57.860 --> 01:38:59.860
You've seen now so many different companies

01:38:59.860 --> 01:39:01.860
in the industrial space.

01:39:01.860 --> 01:39:05.860
Heavy Industries is one of the greatest final frontiers of IT.

01:39:05.860 --> 01:39:08.860
And we finally now have the necessary technology

01:39:08.860 --> 01:39:10.860
to go and make a real impact.

01:39:10.860 --> 01:39:13.860
Siemens is building the industrial metaverse.

01:39:13.860 --> 01:39:16.860
And today we're announcing that Siemens is connecting

01:39:16.860 --> 01:39:19.860
their crown jewel accelerator to NVIDIA Omniverse.

01:39:19.860 --> 01:39:21.860
Let's take a look.

01:39:23.860 --> 01:39:26.860
Siemens technology is transformed every day for everyone.

01:39:26.860 --> 01:39:29.860
Teamcenter X, our leading product lifecycle management team,

01:39:29.860 --> 01:39:32.860
from the Siemens accelerator platform,

01:39:32.860 --> 01:39:34.860
is used every day by our customers

01:39:34.860 --> 01:39:37.860
to develop and deliver products at scale.

01:39:37.860 --> 01:39:40.860
Now we are bringing the real and the digital worlds

01:39:40.860 --> 01:39:43.860
even closer by integrating NVIDIA AI

01:39:43.860 --> 01:39:47.860
and Omniverse technologies into Teamcenter X.

01:39:47.860 --> 01:39:50.860
Omniverse APIs enable data interoperability

01:39:50.860 --> 01:39:52.860
and physics-based rendering

01:39:52.860 --> 01:39:56.860
to industrial-scale design and manufacturing projects.

01:39:56.860 --> 01:39:58.860
Our customers, HD Hyundai,

01:39:58.860 --> 01:40:01.860
market leader in sustainable ship manufacturing,

01:40:01.860 --> 01:40:03.860
builds ammonia and hydrogen-powered ships,

01:40:03.860 --> 01:40:07.860
often comprising over 7 million discrete parts.

01:40:08.860 --> 01:40:09.860
With Omniverse APIs,

01:40:09.860 --> 01:40:12.860
Teamcenter X lets companies like HD Hyundai

01:40:12.860 --> 01:40:17.860
unify and visualize these massive engineering datasets interactively

01:40:17.860 --> 01:40:21.860
and integrate generative AI to generate 3D objects

01:40:21.860 --> 01:40:26.860
or HDRI backgrounds to see their projects in context.

01:40:26.860 --> 01:40:27.860
The result?

01:40:27.860 --> 01:40:31.860
An ultra-intuitive photoreal, physics-based digital twin

01:40:31.860 --> 01:40:33.860
that eliminates waste and errors,

01:40:33.860 --> 01:40:36.860
delivering huge savings in cost and time.

01:40:37.860 --> 01:40:39.860
And we are building this for collaboration,

01:40:39.860 --> 01:40:42.860
whether across more Siemens accelerator tools

01:40:42.860 --> 01:40:46.860
like Siemens Annex or Star CCM Plus,

01:40:46.860 --> 01:40:49.860
or across teams working on their favorite devices

01:40:49.860 --> 01:40:51.860
in the same scene together.

01:40:52.860 --> 01:40:54.860
And this is just the beginning.

01:40:54.860 --> 01:40:55.860
Working with NVIDIA,

01:40:55.860 --> 01:40:57.860
we will bring accelerated computing,

01:40:57.860 --> 01:41:00.860
generative AI and Omniverse integration

01:41:00.860 --> 01:41:04.860
across the Siemens accelerator portfolio.

01:41:11.860 --> 01:41:16.860
The professional voice actor

01:41:16.860 --> 01:41:18.860
happens to be a good friend of mine,

01:41:18.860 --> 01:41:19.860
Roland Busch,

01:41:19.860 --> 01:41:21.860
who happens to be the CEO of Siemens.

01:41:25.860 --> 01:41:34.860
Once you get Omniverse connected into your workflow,

01:41:34.860 --> 01:41:36.860
your ecosystem,

01:41:36.860 --> 01:41:38.860
from the beginning of your design

01:41:38.860 --> 01:41:40.860
to engineering,

01:41:40.860 --> 01:41:42.860
to manufacturing planning,

01:41:42.860 --> 01:41:45.860
all the way to digital twin operations,

01:41:45.860 --> 01:41:47.860
once you connect everything together,

01:41:47.860 --> 01:41:51.860
it's insane how much productivity you can get.

01:41:51.860 --> 01:41:53.860
And it's just really, really wonderful.

01:41:53.860 --> 01:41:55.860
All of a sudden, everybody's operating on the same ground table.

01:41:55.860 --> 01:42:02.740
truth. You don't have to exchange data and convert data, make mistakes.
Everybody is working on the

01:42:02.740 --> 01:42:07.600
same ground truth. From the design department to the art department, the
architecture department,

01:42:07.600 --> 01:42:12.360
all the way to the engineering and even the marketing department. Let's take a
look at how

01:42:12.360 --> 01:42:18.780
Nissan has integrated Omniverse into their workflow. And it's all because it's
connected

01:42:18.780 --> 01:42:22.440
by all these wonderful tools and these developers that we're working with. Take
a look.

01:42:25.860 --> 01:42:54.860
.

01:42:54.860 --> 01:42:55.860
.

01:42:55.860 --> 01:43:01.860
.

01:43:01.860 --> 01:43:16.300
.

01:43:16.300 --> 01:43:24.860
.

01:43:24.860 --> 01:43:25.700
.

01:43:25.700 --> 01:43:25.740
.

01:43:25.740 --> 01:43:25.780
.

01:43:25.780 --> 01:43:25.800
.

01:43:25.800 --> 01:43:25.820
.

01:43:25.820 --> 01:43:25.840
.

01:43:25.840 --> 01:43:25.860
.

01:43:25.860 --> 01:43:55.840
Thank you for watching.

01:43:55.860 --> 01:44:25.840
Thank you for watching.

01:44:25.860 --> 01:44:55.840
Thank you for watching.

01:44:56.640 --> 01:44:59.020
And one of the largest industries is going to be automotive.

01:44:59.920 --> 01:45:03.140
We build the robotic stack from top to bottom, as I was mentioning,

01:45:03.720 --> 01:45:06.360
from the computer system, but in the case of self-driving cars,

01:45:06.700 --> 01:45:08.920
including the self-driving car application.

01:45:09.640 --> 01:45:12.200
At the end of this year, or I guess beginning of next year,

01:45:12.380 --> 01:45:16.260
we will be shipping in Mercedes, and then shortly after that, JLR.

01:45:17.000 --> 01:45:20.580
And so these autonomous robotic systems are software-defined.

01:45:21.000 --> 01:45:22.300
They take a lot of work to do.

01:45:22.300 --> 01:45:23.560
It has computer vision.

01:45:23.700 --> 01:45:25.540
It has, obviously, artificial intelligence.

01:45:25.860 --> 01:45:26.840
Control and planning.

01:45:27.260 --> 01:45:30.860
All kinds of very complicated technology, and it takes years to refine.

01:45:31.260 --> 01:45:32.620
We're building the entire stack.

01:45:33.500 --> 01:45:37.220
However, we open up our entire stack for all of the automotive industry.

01:45:37.400 --> 01:45:38.500
This is just the way we work.

01:45:38.900 --> 01:45:40.380
The way we work in every single industry,

01:45:40.500 --> 01:45:43.060
we try to build as much of it as we can so that we understand it,

01:45:43.220 --> 01:45:45.680
but then we open it up so that everybody can access it.

01:45:45.940 --> 01:45:48.280
Whether you would like to buy just our computer,

01:45:48.280 --> 01:45:54.280
which is the world's only full, functional, safe, ASIL-D system,

01:45:54.700 --> 01:45:55.640
that can run...

01:45:55.640 --> 01:46:06.020
This functional, safe, ASIL-D-quality computer or the operating system on top or
of course our data centers,

01:46:06.020 --> 01:46:08.980
which is in basically every EV company in the world.

01:46:10.240 --> 01:46:12.720
However you would like to enjoy it, we're delighted by it.

01:46:13.440 --> 01:46:19.780
Today, we're announcing that BYD, the world's largest EV company, is adopting
our next generation.

01:46:20.080 --> 01:46:21.020
It's called Thor.

01:46:21.420 --> 01:46:23.420
Thor is designed for transformer engines.

01:46:23.920 --> 01:46:25.440
Thor, our next generation EV.

01:46:25.640 --> 01:46:29.640
Thor, our next generation EV computer, will be used by BYD.

01:46:37.640 --> 01:46:41.640
You probably don't know this fact that we have over a million robotics
developers.

01:46:42.640 --> 01:46:45.640
We created Jetson, this robotics computer.

01:46:45.640 --> 01:46:46.640
We're so proud of it.

01:46:46.640 --> 01:46:48.640
The amount of software that goes on top of it is insane.

01:46:49.240 --> 01:46:52.640
But the reason why we can do it at all is because it's 100% CUDA compatible.

01:46:53.040 --> 01:46:54.240
Everything that we do,

01:46:54.240 --> 01:46:57.240
everything that we do in our company is in service of our developers.

01:46:57.240 --> 01:47:05.240
And by us being able to maintain this rich ecosystem and make it compatible with
everything that you access from us,

01:47:05.240 --> 01:47:12.240
we can bring all of that incredible capability to this little tiny computer we
call Jetson, a robotics computer.

01:47:12.240 --> 01:47:17.240
We also today are announcing this incredibly advanced new SDK.

01:47:17.240 --> 01:47:20.240
We call it Isaac Perceptor.

01:47:20.240 --> 01:47:23.240
Isaac Perceptor, most of the robots today,

01:47:23.240 --> 01:47:25.240
are pre-programmed.

01:47:25.240 --> 01:47:30.240
They're either following rails on the ground, digital rails, or they'd be
following April tags.

01:47:30.240 --> 01:47:32.240
But in the future, they're going to have perception.

01:47:32.240 --> 01:47:36.240
And the reason why you want that is so that you could easily program it.

01:47:36.240 --> 01:47:39.240
You say, I would like to go from point A to point B,

01:47:39.240 --> 01:47:42.240
and it will figure out a way to navigate its way there.

01:47:42.240 --> 01:47:45.240
So by only programming waypoints,

01:47:45.240 --> 01:47:48.240
the entire route could be adaptive.

01:47:48.240 --> 01:47:52.240
The entire environment could be reprogrammed, just as I showed you at the very
beginning with the warehouse.

01:47:52.240 --> 01:47:57.240
You can't do that with pre-programmed AGVs.

01:47:57.240 --> 01:48:02.240
If those boxes fall down, they just all gum up and they just wait there for
somebody to come clear it.

01:48:02.240 --> 01:48:06.240
And so now, with the Isaac Perceptor,

01:48:06.240 --> 01:48:10.240
we have incredible state-of-the-art vision odometry,

01:48:10.240 --> 01:48:12.240
3D reconstruction,

01:48:12.240 --> 01:48:15.240
and in addition to 3D reconstruction, depth perception.

01:48:15.240 --> 01:48:20.240
The reason for that is so that you can have two modalities to keep an eye on
what's happening in the world.

01:48:20.240 --> 01:48:21.240
Isaac Perceptor.

01:48:21.240 --> 01:48:28.240
The most used robot today is the manipulator, manufacturing arms,

01:48:28.240 --> 01:48:30.240
and they are also pre-programmed.

01:48:30.240 --> 01:48:33.240
The computer vision algorithms, the AI algorithms,

01:48:33.240 --> 01:48:37.240
the control and path planning algorithms that are geometry-aware,

01:48:37.240 --> 01:48:40.240
incredibly computationally intensive.

01:48:40.240 --> 01:48:43.240
We have made these CUDA accelerated.

01:48:43.240 --> 01:48:49.240
So we have the world's first CUDA accelerated motion planner that is geometry-
aware.

01:48:49.240 --> 01:48:51.240
You put something in front of it,

01:48:51.240 --> 01:48:54.240
it comes up with a new plan and articulates around it.

01:48:54.240 --> 01:48:59.240
It has excellent perception for pose estimation of a 3D object.

01:48:59.240 --> 01:49:03.240
Not just its pose in 2D, but its pose in 3D.

01:49:03.240 --> 01:49:08.240
So it has to imagine what's around and how best to graph it.

01:49:08.240 --> 01:49:12.240
So the foundation pose, the grip foundation,

01:49:12.240 --> 01:49:16.240
and the articulation algorithms are now available.

01:49:16.240 --> 01:49:18.240
We call it Isaac Manipulator.

01:49:18.240 --> 01:49:21.240
And they also just run on NVIDIA's computers.

01:49:21.240 --> 01:49:28.240
We are starting to do some really great work in the next generation of robotics.

01:49:28.240 --> 01:49:33.240
The next generation of robotics will likely be a humanoid robotics.

01:49:33.240 --> 01:49:36.240
We now have the necessary technology,

01:49:36.240 --> 01:49:39.240
and as I was describing earlier,

01:49:39.240 --> 01:49:44.240
the necessary technology to imagine generalized human robotics.

01:49:44.240 --> 01:49:47.240
In a way, human robotics is likely easier,

01:49:47.240 --> 01:49:51.240
and the reason for that is because we have a lot more imitation training

01:49:51.240 --> 01:49:53.240
data that we can provide the robots,

01:49:53.240 --> 01:49:56.240
because we are constructed in a very similar way.

01:49:56.240 --> 01:50:00.240
It is very likely that the humanoid robotics will be much more useful in our
world,

01:50:00.240 --> 01:50:05.240
because we created the world to be something that we can interoperate in and
work well in.

01:50:05.240 --> 01:50:09.240
And the way that we set up our workstations and manufacturing and logistics,

01:50:09.240 --> 01:50:12.240
they were designed for humans, they were designed for people.

01:50:12.240 --> 01:50:18.240
And so these humanoid robotics will likely be much more productive to deploy.

01:50:18.240 --> 01:50:21.240
While we're creating, just like we're doing with the others,

01:50:21.240 --> 01:50:24.240
the entire stack, starting from the top,

01:50:24.240 --> 01:50:31.240
a foundation model that learns from watching video, human examples.

01:50:31.240 --> 01:50:35.240
It could be in video form, it could be in virtual reality form.

01:50:35.240 --> 01:50:40.240
We then created a gym for it called Isaac Reinforcement Learning Gym,

01:50:40.240 --> 01:50:46.240
which allows the humanoid robot to learn how to adapt to the physical world.

01:50:46.240 --> 01:50:48.240
And then an incredible computer,

01:50:48.240 --> 01:50:51.240
the same computer that's going to go into a robotic car,

01:50:51.240 --> 01:50:55.240
this computer will run inside a humanoid robot called Thor.

01:50:55.240 --> 01:50:58.240
It's designed for transformer engines.

01:50:58.240 --> 01:51:01.240
We've combined several of these into one video.

01:51:01.240 --> 01:51:03.240
This is something that you're going to really love.

01:51:03.240 --> 01:51:04.240
Take a look.

01:51:08.240 --> 01:51:11.240
It's not enough for humans to imagine.

01:51:16.240 --> 01:51:18.240
We have to invent.

01:51:18.240 --> 01:51:22.240
And explore.

01:51:22.240 --> 01:51:25.240
And push beyond what's been done.

01:51:32.240 --> 01:51:34.240
We create smarter.

01:51:34.240 --> 01:51:36.240
And faster.

01:51:38.240 --> 01:51:40.240
We push it to fail.

01:51:40.240 --> 01:51:42.240
So it can learn.

01:51:45.240 --> 01:51:47.240
We teach it.

01:51:47.240 --> 01:51:49.240
Then help it teach itself.

01:51:49.240 --> 01:51:52.240
We broaden its understanding.

01:51:55.240 --> 01:51:57.240
To take on new challenges.

01:51:59.240 --> 01:52:02.240
With absolute precision.

01:52:04.240 --> 01:52:06.240
And succeed.

01:52:07.240 --> 01:52:09.240
We make it perceive.

01:52:10.240 --> 01:52:12.240
And move.

01:52:14.240 --> 01:52:16.240
And even reason.

01:52:17.240 --> 01:52:20.240
So it can share our world.

01:52:20.240 --> 01:52:21.240
With us.

01:52:41.240 --> 01:52:44.240
This is where inspiration leads us.

01:52:44.240 --> 01:52:45.240
The next frontier.

01:52:46.240 --> 01:52:49.240
This is NVIDIA Project GROOT.

01:52:53.240 --> 01:52:57.240
A general purpose foundation model for humanoid robot learning.

01:52:59.240 --> 01:53:04.240
The GROOT model takes multimodal instructions and past interactions as input.

01:53:04.240 --> 01:53:07.240
And produces the next action for the robot to execute.

01:53:09.240 --> 01:53:11.240
We developed Isaac Lab.

01:53:11.240 --> 01:53:13.240
A robot learning application to train GROOT.

01:53:13.240 --> 01:53:16.240
On Omniverse Isaac Sim.

01:53:17.240 --> 01:53:19.240
And we scale out with Osmo.

01:53:19.240 --> 01:53:21.240
A new compute orchestration service.

01:53:21.240 --> 01:53:24.240
That coordinates workflows across DGX systems for training.

01:53:24.240 --> 01:53:27.240
And OVX systems for simulation.

01:53:28.240 --> 01:53:29.240
With these tools.

01:53:29.240 --> 01:53:32.240
We can train GROOT in physically based simulation.

01:53:32.240 --> 01:53:35.240
And transfer zero-shot to the real world.

01:53:37.240 --> 01:53:41.240
The GROOT model will enable a robot to learn from a handful of human
demonstrations.

01:53:41.240 --> 01:53:45.240
So it can help with everyday tasks.

01:53:47.240 --> 01:53:49.240
And emulate human movement.

01:53:49.240 --> 01:53:51.240
Just by observing us.

01:53:52.240 --> 01:53:55.240
This is made possible with NVIDIA's technologies.

01:53:55.240 --> 01:53:57.240
That can understand humans from videos.

01:53:57.240 --> 01:53:59.240
Train models and simulation.

01:53:59.240 --> 01:54:02.240
And ultimately deploy them directly to physical robots.

01:54:03.240 --> 01:54:05.240
Connecting GROOT to a large language model.

01:54:05.240 --> 01:54:07.240
Even allows it to generate motions.

01:54:07.240 --> 01:54:10.240
By following natural language instructions.

01:54:10.240 --> 01:54:11.240
Hi GL1.

01:54:11.240 --> 01:54:13.240
Can you give me a high five?

01:54:13.240 --> 01:54:14.240
Sure thing.

01:54:14.240 --> 01:54:15.240
Let's high five.

01:54:16.240 --> 01:54:18.240
Can you give us some cool moves?

01:54:19.240 --> 01:54:20.240
Sure.

01:54:20.240 --> 01:54:21.240
Check this out.

01:54:24.240 --> 01:54:29.240
All this incredible intelligence is powered by the new Jetson Thor Robotics
chips.

01:54:29.240 --> 01:54:30.240
Designed for GROOT.

01:54:30.240 --> 01:54:32.240
Built for the future.

01:54:32.240 --> 01:54:35.240
With Isaac Lab, Osmo and GROOT.

01:54:35.240 --> 01:54:39.240
We're providing the building blocks for the next generation of AI powered
robotics.

01:54:40.240 --> 01:54:53.240
No 보건энерго

01:54:53.240 --> 01:54:57.240
About the same size.

01:55:05.240 --> 01:55:06.240
The soul of NVIDIA.

01:55:06.240 --> 01:55:08.240
The intersection of computer graphics.

01:55:08.240 --> 01:55:09.240
Physics.

01:55:09.240 --> 01:55:15.340
physics, artificial intelligence. It all came to bear at this moment. The name
of

01:55:15.340 --> 01:55:29.380
that project, General Robotics 003. I know, super good. Super good. Well, I
think we

01:55:29.380 --> 01:55:35.240
have some special guests. Do we?

01:55:39.240 --> 01:55:53.680
Hey, guys. So I understand you guys are powered by Jetson. They're powered by
Jetsons. Little

01:55:53.680 --> 01:56:04.360
Jetson robotics computers inside. They learned to walk in Isaac Sim. Ladies and
gentlemen,

01:56:04.360 --> 01:56:08.440
this is orange, and this is the famous green.

01:56:08.440 --> 01:56:19.860
They are the BDX robots of Disney. Amazing Disney research. Come on, you guys.
Let's

01:56:19.860 --> 01:56:35.220
wrap up. Let's go. Five things. Where are you going? I sit right here. Don't be
afraid.

01:56:35.220 --> 01:56:36.660
Come here, green. Hurry up.

01:56:38.440 --> 01:56:52.240
Five things. What are you saying? No, it's not time to eat. I'll give you a
snack in

01:56:52.240 --> 01:57:01.700
a moment. Let me finish up real quick. Come on, green. Hurry up. Stop wasting
time. Five

01:57:01.700 --> 01:57:07.040
things. Five things. First, a new industrial revolution. Every day, every state
and every

01:57:07.040 --> 01:57:08.440
country is going to be on the run. Every day, every city is going to be on the
run. Every

01:57:08.440 --> 01:57:12.280
data centers should be accelerated a trillion dollars worth of installed data

01:57:12.280 --> 01:57:16.600
centers will become modernized over the next several years second because of the

01:57:16.600 --> 01:57:20.180
computational capability we brought to bear a new way of doing software has

01:57:20.180 --> 01:57:24.920
emerged generative AI which is going to create new and new infrastructure

01:57:24.920 --> 01:57:30.040
dedicated to doing one thing and one thing only not for multi-user data

01:57:30.040 --> 01:57:36.220
centers but AI generators these AI generation will create incredibly

01:57:36.220 --> 01:57:41.800
valuable software a new Industrial Revolution second the computer of this

01:57:41.800 --> 01:57:46.900
revolution the computer of this generation generative AI trillion

01:57:46.900 --> 01:57:54.400
parameters Blackwell insane amounts of computers and computing third I'm trying

01:57:54.400 --> 01:58:04.480
to concentrate good job third new computer new computer creates new types

01:58:04.480 --> 01:58:06.220
of software new type of software should

01:58:06.220 --> 01:58:10.780
be distributed in a new way so that it can on the one hand be an endpoint in the

01:58:10.780 --> 01:58:15.160
cloud and easy to use but still allow you to take it with you because it is

01:58:15.160 --> 01:58:19.240
your intelligence your intelligence should be packed packaged up in a way

01:58:19.240 --> 01:58:23.860
that allows you to take it with you we call them NIMS and third these NIMS are

01:58:23.860 --> 01:58:28.720
going to help you create a new type of application for the future not one that

01:58:28.720 --> 01:58:32.260
you wrote completely from scratch but you're going to integrate them like

01:58:32.260 --> 01:58:35.860
teams create these applications we have a

01:58:35.860 --> 01:58:36.160
few of them we have a few of them we have a few of them we have a few of them

01:58:36.160 --> 01:58:36.200
we have a few of them we have a few of them we have a few of them we have a

01:58:36.200 --> 01:58:42.740
fantastic capability between NIMS the AI technology the tools Nemo and the

01:58:42.740 --> 01:58:47.480
infrastructure DGX cloud in our AI foundry to help you create proprietary

01:58:47.480 --> 01:58:51.320
applications proprietary chatbots and then lastly everything that moves in the

01:58:51.320 --> 01:58:56.180
future will be robotic you're not going to be the only one and these robotic

01:58:56.180 --> 01:59:03.320
systems whether they are humanoid AMRs self-driving cars forklifts manipulating

01:59:03.320 --> 01:59:06.140
arms they will all need one thing

01:59:06.140 --> 01:59:11.180
giant stadiums warehouses factories there can be factories that are robotic

01:59:11.180 --> 01:59:15.620
orchestrating factories manufacturing lines that are robotics building cars

01:59:15.620 --> 01:59:22.640
that are robotics these systems all need one thing they need a platform a
digital

01:59:22.640 --> 01:59:27.500
platform a digital twin platform and we call that omniverse the operating system

01:59:27.500 --> 01:59:32.900
of the robotics world these are the five things that we talked about today what

01:59:32.900 --> 01:59:36.080
does NVIDIA look like what does NVIDIA look like when we talk about

01:59:36.080 --> 01:59:40.700
GPUs there's a very different image that I have when I when people ask me

01:59:40.700 --> 01:59:44.780
about GPUs first I see a bunch of software Stags and things like that and

01:59:44.780 --> 01:59:50.360
second I see this this is what we announced to you today this is Blackwell

01:59:50.360 --> 01:59:53.960
this is the platform

01:59:57.320 --> 02:00:05.540
amazing amazing processors MV link switches networking systems and the system
design is a

02:00:05.540 --> 02:00:06.020
miracle

02:00:06.020 --> 02:00:11.000
this is Blackwell and this to me is what a GPU looks like in my mind

02:00:19.220 --> 02:00:24.620
listen orange green I think we have one more treat for everybody what do you
think should we

02:00:26.540 --> 02:00:30.260
okay we have one more thing to show you roll it

02:00:36.020 --> 02:01:01.320
hope you enjoy soon

02:01:01.320 --> 02:01:03.260
hope you enjoy soon

02:01:03.260 --> 02:01:03.320
hope you enjoy soon

02:01:03.320 --> 02:01:04.540
hope you enjoy soon

02:01:04.540 --> 02:01:04.620
hope you enjoy soon

02:01:04.620 --> 02:01:04.820
hope you enjoy soon

02:01:04.820 --> 02:01:05.160
hope you enjoy soon

02:01:05.160 --> 02:01:05.180
hope you enjoy soon

02:01:05.180 --> 02:01:05.660
hope you enjoy soon

02:01:05.660 --> 02:01:05.680
hope you enjoy soon

02:01:05.680 --> 02:01:05.780
hope you enjoy soon

02:01:05.780 --> 02:01:05.860
hope you enjoy soon

02:01:05.860 --> 02:01:06.020
hope you enjoy soon

02:01:06.020 --> 02:01:36.000
To be continued...

02:01:36.020 --> 02:02:06.000
To be continued...

02:02:06.020 --> 02:02:36.000
To be continued...

02:02:36.020 --> 02:03:06.000
To be continued...

