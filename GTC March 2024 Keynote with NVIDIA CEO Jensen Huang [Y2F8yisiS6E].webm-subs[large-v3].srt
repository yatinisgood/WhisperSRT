1
00:00:00,000 --> 00:00:29,980
Thank you for watching.

2
00:00:29,980 --> 00:00:31,320
I am a visionary.

3
00:00:36,980 --> 00:00:41,040
Illuminating galaxies to witness the birth of stars.

4
00:00:47,300 --> 00:00:51,660
And sharpening our understanding of extreme weather events.

5
00:00:56,800 --> 00:00:58,960
I am a helper.

6
00:00:59,980 --> 00:01:03,620
Guiding the blind through a crowded world.

7
00:01:08,160 --> 00:01:10,700
I was thinking about running to the store.

8
00:01:11,060 --> 00:01:13,740
And giving voice to those who cannot speak.

9
00:01:15,300 --> 00:01:17,100
Do not make me laugh.

10
00:01:17,600 --> 00:01:21,160
I am a transformer.

11
00:01:23,240 --> 00:01:26,560
Harnessing gravity to store renewable power.

12
00:01:29,980 --> 00:01:38,580
And paving the way towards unlimited clean energy for us all.

13
00:01:42,200 --> 00:01:44,440
I am a trainer.

14
00:01:45,980 --> 00:01:48,420
Teaching robots to assist.

15
00:01:51,760 --> 00:01:54,180
To watch out for danger.

16
00:01:58,160 --> 00:01:59,160
And help.

17
00:01:59,160 --> 00:01:59,960
And help.

18
00:01:59,960 --> 00:02:01,740
Help save lives.

19
00:02:04,800 --> 00:02:07,120
I am a healer.

20
00:02:09,140 --> 00:02:11,960
Providing a new generation of cures.

21
00:02:13,340 --> 00:02:16,100
And new levels of patient care.

22
00:02:16,240 --> 00:02:18,240
I am a doctor that I am allergic to penicillin.

23
00:02:18,640 --> 00:02:20,180
Is it still okay to take the medications?

24
00:02:20,760 --> 00:02:21,260
Definitely.

25
00:02:21,600 --> 00:02:23,840
These antibiotics don't contain penicillin.

26
00:02:23,960 --> 00:02:25,760
So it's perfectly safe for you to take them.

27
00:02:27,220 --> 00:02:29,380
I am a navigator.

28
00:02:30,640 --> 00:02:31,600
Define.

29
00:02:32,100 --> 00:02:33,260
Be yourself.

30
00:02:33,360 --> 00:02:34,020
Be yourself.

31
00:02:34,220 --> 00:02:35,700
It's your counselors crown.

32
00:02:36,920 --> 00:02:37,400
Be good.

33
00:02:37,520 --> 00:02:38,080
Be open.

34
00:02:38,140 --> 00:02:38,660
Be open.

35
00:02:38,660 --> 00:02:39,080
Be yourself.

36
00:02:39,080 --> 00:02:39,660
Be yourself.

37
00:02:39,720 --> 00:02:40,160
Be yourself.

38
00:02:40,160 --> 00:02:40,660
Be yourself.

39
00:02:40,700 --> 00:02:41,120
Be yourself.

40
00:02:41,140 --> 00:02:41,620
Be yourself.

41
00:02:41,620 --> 00:02:42,120
Be yourself.

42
00:02:42,200 --> 00:02:42,760
Be yourself.

43
00:02:42,980 --> 00:02:43,480
Be yourself.

44
00:02:43,680 --> 00:02:44,020
Be yourself.

45
00:02:44,040 --> 00:02:44,480
Be yourself.

46
00:02:45,020 --> 00:02:46,460
And understand every decision.

47
00:02:46,460 --> 00:02:47,980
To take on challenges.

48
00:02:48,840 --> 00:02:49,500
To help.

49
00:02:49,660 --> 00:02:51,300
To help you get me out of this pit.

50
00:02:51,300 --> 00:02:53,740
I am a data shore not only I do not know theese stuck-up.

51
00:02:53,940 --> 00:02:54,960
But get a more efficient bite.

52
00:02:55,400 --> 00:02:56,160
I am filled.

53
00:02:56,180 --> 00:02:57,060
With seituluks.

54
00:02:57,860 --> 00:02:58,840
With sauna.

55
00:02:58,840 --> 00:02:59,580
With ho iets.

56
00:02:59,960 --> 00:03:19,960
en muchos idiomas y escribí la música i am ai brought to life by nvidia deep
learning

57
00:03:19,960 --> 00:03:31,960
and brilliant minds everywhere

58
00:03:33,960 --> 00:03:45,960
please welcome to the stage nvidia founder and ceo jensen wong

59
00:03:49,960 --> 00:03:55,240
welcome to gtc

60
00:04:00,440 --> 00:04:03,800
i hope you realize this is not a concert

61
00:04:06,520 --> 00:04:10,360
you have arrived at a developers conference

62
00:04:12,280 --> 00:04:19,800
there will be a lot of science described algorithms computer architecture
mathematics

63
00:04:19,960 --> 00:04:35,960
i sensed a very heavy weight in the room all of a sudden almost like you were in
the wrong place

64
00:04:35,960 --> 00:04:44,440
no no conference in the world is there a greater assembly of researchers from
such diverse fields

65
00:04:44,440 --> 00:04:49,480
of science from climate tech to research and innovation

66
00:04:49,960 --> 00:04:55,400
radio science is trying to figure out how to use ai to robotically control mimos

67
00:04:55,400 --> 00:05:03,560
for next generation 6g radios robotic self-driving cars even artificial
intelligence

68
00:05:05,960 --> 00:05:09,160
even artificial intelligence everybody's

69
00:05:11,400 --> 00:05:19,480
first i noticed a sense of relief there all of all of a sudden also this
conference is represented

70
00:05:20,120 --> 00:05:25,640
by some amazing companies this list this is not the attendees

71
00:05:27,640 --> 00:05:32,040
these are the presenters and what's amazing is this

72
00:05:33,560 --> 00:05:41,720
if you take away all of my friends close friends michael dell is sitting right
there in the i.t

73
00:05:41,720 --> 00:05:42,200
industry

74
00:05:47,880 --> 00:05:49,880
all of the friends i grew up with

75
00:05:50,200 --> 00:05:57,960
in the industry if you take away that list this is what's amazing these are the
presenters

76
00:05:58,680 --> 00:06:04,600
of the non-it industries using accelerated computing to solve problems that
normal computers

77
00:06:04,600 --> 00:06:16,040
can't it's represented in life sciences health care genomics transportation of
course retail

78
00:06:16,600 --> 00:06:19,480
logistics manufacturing industrial

79
00:06:20,120 --> 00:06:24,760
tomahawk or pharmaceutical technology technology modern data negotiation chapter

80
00:06:24,760 --> 00:06:27,560
please turn your焼 to a妳

81
00:06:27,560 --> 00:06:33,400
you're here to present to talk about your research 100 trillion dollars of the
world's industries

82
00:06:33,400 --> 00:06:37,080
represented in this room today this is absolutely amazing

83
00:06:43,960 --> 00:06:49,160
there is absolutely something happening there is something going on

84
00:06:49,160 --> 00:06:49,560
university

85
00:06:49,960 --> 00:06:57,200
the industry is being transformed not just ours because the computer industry

86
00:06:57,200 --> 00:07:02,960
the computer is the single most important instrument of society today

87
00:07:02,960 --> 00:07:08,580
fundamental transformations and computing affects every industry but how

88
00:07:08,580 --> 00:07:13,180
did we start how did we get here I made a little cartoon for you literally I

89
00:07:13,180 --> 00:07:22,160
drew this in one page this is NVIDIA's journey started in 1993 this might be

90
00:07:22,160 --> 00:07:28,600
the rest of the talk 1993 this is our journey we were founded in 1993 there are

91
00:07:28,600 --> 00:07:32,860
several important events that happen along the way I'll just highlight a few

92
00:07:32,860 --> 00:07:38,860
in 2006 CUDA which has turned out to have been a revolutionary computing

93
00:07:38,860 --> 00:07:42,760
model we thought it was revolutionary then it was going to be an overnight

94
00:07:42,760 --> 00:07:43,180
success

95
00:07:43,180 --> 00:07:45,420
in almost 20 years later it happenedione

96
00:07:45,420 --> 00:08:05,080
we saw it coming to decades later in 2012 alex net AI and CUDA made first

97
00:08:05,080 --> 00:08:11,920
contact in 2016 recognizing the importance of this computing model we

98
00:08:11,920 --> 00:08:12,760
invented a brand-new

99
00:08:12,760 --> 00:08:21,760
brand new type of computer. We called it DGX-1. 170 teraflops in this
supercomputer. Eight

100
00:08:21,760 --> 00:08:28,040
GPUs connected together for the very first time. I hand delivered the very first
DGX-1

101
00:08:28,040 --> 00:08:44,900
to a startup located in San Francisco called OpenAI. DGX-1 was the world's first
AI supercomputer.

102
00:08:44,900 --> 00:08:55,880
Remember 170 teraflops. 2017, the transformer arrived. 2022, ChatGPT captured
the world's

103
00:08:55,880 --> 00:08:57,980
imaginations.

104
00:08:57,980 --> 00:09:05,700
People realized the importance and the capabilities of artificial intelligence.
In 2023, generative

105
00:09:05,700 --> 00:09:15,520
AI emerged and a new industry begins. Why? Why is it a new industry? Because the
software

106
00:09:15,520 --> 00:09:22,680
never existed before. We are now producing software. Using computers to write
software.

107
00:09:22,680 --> 00:09:27,460
Producing software that never existed before. It is a brand new category. It
took share

108
00:09:27,460 --> 00:09:27,960
from nothing.

109
00:09:27,960 --> 00:09:36,260
It's a brand new category. And the way you produce the software is unlike
anything we've

110
00:09:36,260 --> 00:09:48,000
ever done before. In data centers, generating tokens, producing floating point
numbers at

111
00:09:48,000 --> 00:09:55,500
very large scale. As if in the beginning of this last industrial revolution,
when people

112
00:09:55,500 --> 00:09:57,440
realized that you would set up factories, you would build factories, you would
build

113
00:09:57,440 --> 00:10:00,540
most simple software to produce data and get a large number of monoliths
toлоNH粒s.

114
00:10:00,540 --> 00:10:04,980
Applied energy to it. And this invisible, valuable thing of electricity

115
00:10:04,980 --> 00:10:10,920
came out, . A CGegitatior . And a hundred years later, 200 years later

116
00:10:10,920 --> 00:10:19,500
, we are now creating new types of electrons. Tokens, using infrastructu , we
call! Factories,

117
00:10:19,500 --> 00:10:25,240
AI! Factories, to generate this new incredibly valuable thing. Called Artificial
intelligence.

118
00:10:25,240 --> 00:10:26,240
A

119
00:10:26,240 --> 00:10:27,300
nir Finish ends. Shaftesbury.com. . A new industry has posterized real life이랑
Mars. Makingct Yes.

120
00:10:27,300 --> 00:10:33,260
emerged well we're going to talk about many things about this new industry

121
00:10:33,260 --> 00:10:37,620
we're going to talk about how we're going to do computing next we're going to
talk

122
00:10:37,620 --> 00:10:42,420
about the type of software that you build because of this new industry the

123
00:10:42,420 --> 00:10:47,100
new software how you would think about this new software what about

124
00:10:47,100 --> 00:10:53,220
applications in this new industry and then maybe what's next and how can we

125
00:10:53,220 --> 00:10:59,480
start preparing today for what is about to come next well but before I start I

126
00:10:59,480 --> 00:11:06,720
want to show you the soul of Nvidia the soul of our company at the intersection

127
00:11:06,720 --> 00:11:17,400
of computer graphics physics and artificial intelligence all intersecting

128
00:11:17,400 --> 00:11:23,200
inside a computer in omniverse in a virtual

129
00:11:23,200 --> 00:11:28,240
world simulation. Everything we're going to show you today, literally everything

130
00:11:28,240 --> 00:11:34,420
we're going to show you today, is a simulation, not animation. It's only

131
00:11:34,420 --> 00:11:39,940
beautiful because it's physics. The world is beautiful. It's only amazing
because

132
00:11:39,940 --> 00:11:44,040
it's being animated with robotics. It's being animated with artificial

133
00:11:44,040 --> 00:11:49,120
intelligence. What you're about to see all day is completely generated,

134
00:11:49,120 --> 00:11:54,640
completely simulated, and omniverse. And all of it, what you're about to enjoy,
is

135
00:11:54,640 --> 00:11:59,760
the world's first concert where everything is homemade.

136
00:12:05,040 --> 00:12:11,740
Everything is homemade. You're about to watch some home videos. So sit back and

137
00:12:11,740 --> 00:12:14,600
enjoy yourself.

138
00:12:19,120 --> 00:12:35,060
Mmm...

139
00:12:49,120 --> 00:13:19,100
Let's do this.

140
00:13:19,120 --> 00:13:49,100
Let's do this.

141
00:13:49,120 --> 00:14:19,100
Let's do this.

142
00:14:19,120 --> 00:14:49,100
Let's do this.

143
00:14:49,120 --> 00:14:59,480
God, I love NVIDIA.

144
00:15:04,780 --> 00:15:08,460
Accelerated computing has reached the tipping point.

145
00:15:09,580 --> 00:15:11,840
General purpose computing has run out of steam.

146
00:15:12,740 --> 00:15:17,040
We need another way of doing computing so that we can continue to scale,

147
00:15:17,040 --> 00:15:19,040
so that we can continue to drive down the cost,

148
00:15:19,120 --> 00:15:25,820
so that we can continue to consume more and more computing while being
sustainable.

149
00:15:26,660 --> 00:15:31,800
Accelerated computing is a dramatic speed-up over general purpose computing.

150
00:15:32,520 --> 00:15:39,620
And in every single industry we engage, and I'll show you many, the impact is
dramatic.

151
00:15:40,140 --> 00:15:43,080
But in no industry is it more important than our own.

152
00:15:43,080 --> 00:15:48,840
The industry of using simulation tools to create products.

153
00:15:49,120 --> 00:15:54,980
In this industry, it is not about driving down the cost of computing.

154
00:15:54,980 --> 00:15:57,800
It's about driving up the scale of computing.

155
00:15:57,800 --> 00:16:04,560
We would like to be able to simulate the entire product that we do completely in
full fidelity,

156
00:16:04,560 --> 00:16:10,140
completely digitally, and essentially what we call digital twins.

157
00:16:10,140 --> 00:16:16,740
We would like to design it, build it, simulate it, operate it completely
digitally.

158
00:16:16,740 --> 00:16:18,660
In order to do that, we need to be able to simulate it.

159
00:16:18,660 --> 00:16:23,300
In order to do that, we need to accelerate an entire industry.

160
00:16:23,300 --> 00:16:28,760
And today, I would like to announce that we have some partners who are joining
us in this journey

161
00:16:28,760 --> 00:16:36,920
to accelerate their entire ecosystem so that we can bring the world into
accelerated computing.

162
00:16:36,920 --> 00:16:39,020
But there's a bonus.

163
00:16:39,020 --> 00:16:45,100
When you become accelerated, your infrastructure is CUDA GPUs.

164
00:16:45,100 --> 00:16:48,200
And when that happens, it's exactly the same infrastructure.

165
00:16:48,660 --> 00:16:51,360
For generative AI.

166
00:16:51,360 --> 00:16:56,340
And so I'm just delighted to announce several very important partnerships.

167
00:16:56,340 --> 00:16:58,780
They're some of the most important companies in the world.

168
00:16:58,780 --> 00:17:02,900
ANSYS does engineering simulation for what the world makes.

169
00:17:02,900 --> 00:17:07,080
We're partnering with them to CUDA accelerate the ANSYS ecosystem,

170
00:17:07,080 --> 00:17:10,880
to connect ANSYS to the Omniverse digital twin.

171
00:17:10,880 --> 00:17:12,300
Incredible.

172
00:17:12,300 --> 00:17:17,240
The thing that's really great is that the install base of NVIDIA GPU accelerated
systems are all over the world.

173
00:17:17,240 --> 00:17:18,400
In every cloud.

174
00:17:18,660 --> 00:17:19,940
In every system.

175
00:17:19,940 --> 00:17:21,480
All over enterprises.

176
00:17:21,480 --> 00:17:26,860
And so the applications they accelerate will have a giant install base to go
serve.

177
00:17:26,860 --> 00:17:29,200
End users will have amazing applications.

178
00:17:29,200 --> 00:17:34,720
And of course, system makers and CSPs will have great customer demand.

179
00:17:34,720 --> 00:17:36,740
Synopsys.

180
00:17:36,740 --> 00:17:42,040
Synopsys is NVIDIA's literally first software partner.

181
00:17:42,040 --> 00:17:44,320
They were there in the very first day of our company.

182
00:17:44,320 --> 00:17:48,040
Synopsys revolutionized the chip industry with high level design.

183
00:17:48,660 --> 00:17:52,080
We are going to CUDA accelerate Synopsys.

184
00:17:52,080 --> 00:17:54,660
We're accelerating computational lithography.

185
00:17:54,660 --> 00:17:59,660
One of the most important applications that nobody has ever known about.

186
00:17:59,660 --> 00:18:03,540
In order to make chips, we have to push lithography to a limit.

187
00:18:03,540 --> 00:18:05,780
NVIDIA has created a library.

188
00:18:05,780 --> 00:18:10,380
A domain-specific library that accelerates computational lithography.

189
00:18:10,380 --> 00:18:11,940
Incredibly.

190
00:18:11,940 --> 00:18:17,940
Once we can accelerate it and in software define all of TSMC, who is announcing
today that they're going to go into,

191
00:18:17,940 --> 00:18:18,520
Click.

192
00:18:18,660 --> 00:18:24,320
to production with NVIDIA CooLitho. Once the software defined and accelerated,
the next step

193
00:18:24,320 --> 00:18:29,500
is to apply generative AI to the future of semiconductor manufacturing, pushing
geometry

194
00:18:29,500 --> 00:18:39,380
even further. Cadence builds the world's essential EDA and SDA tools. We also
use Cadence. Between

195
00:18:39,380 --> 00:18:44,900
these three companies, Ansys, Synopsys, and Cadence, we basically build NVIDIA.
Together,

196
00:18:44,900 --> 00:18:51,140
we are CooLitho accelerating Cadence. They're also building a supercomputer out
of NVIDIA GPUs

197
00:18:51,140 --> 00:18:58,240
so that their customers could do fluid dynamic simulation at a hundred, a
thousand times scale.

198
00:18:59,040 --> 00:19:06,140
Basically, a wind tunnel in real time. Cadence Millennium, a supercomputer with
NVIDIA GPUs

199
00:19:06,140 --> 00:19:12,820
inside. A software company building supercomputers. I love seeing that. Building
Cadence co-pilots

200
00:19:12,820 --> 00:19:14,540
together. Imagine a day.

201
00:19:14,900 --> 00:19:23,780
When Cadence could, Synopsys, Ansys, tool providers would offer you AI co-pilots
so that we have

202
00:19:23,780 --> 00:19:30,300
thousands and thousands of co-pilot assistants helping us design chips, design
systems. And we're

203
00:19:30,300 --> 00:19:35,500
also going to connect Cadence Digital Twin Platform to Omniverse. As you can see
the trend here,

204
00:19:35,980 --> 00:19:43,380
we're accelerating the world's CAE, EDA, and SDA so that we could create our
future in digital

205
00:19:43,380 --> 00:19:44,880
twins. And we're going to connect the two together. We're going to create a
future in digital twins.

206
00:19:44,880 --> 00:19:49,680
We're going to connect them all to Omniverse, the fundamental operating system
for future digital twins.

207
00:19:51,560 --> 00:19:57,720
One of the industries that benefited tremendously from scale, and you all know
this one very well,

208
00:19:57,720 --> 00:20:05,320
large language models. Basically, after the transformer was invented, we were
able to scale

209
00:20:05,320 --> 00:20:11,000
large language models at incredible rates, effectively doubling every six
months. Now,

210
00:20:11,000 --> 00:20:13,960
how is it possible that by doubling every six months,

211
00:20:14,880 --> 00:20:20,320
that we have grown the industry, we have grown the computational requirements so
far?

212
00:20:20,320 --> 00:20:23,960
And the reason for that is quite simply this. If you double the size of the
model,

213
00:20:23,960 --> 00:20:28,240
you double the size of your brain, you need twice as much information to go fill
it.

214
00:20:28,240 --> 00:20:36,320
And so every time you double your parameter count, you also have to
appropriately increase

215
00:20:36,320 --> 00:20:44,200
your training token count. The combination of those two numbers becomes the
computation scale you have to support.

216
00:20:44,880 --> 00:20:51,680
The latest, the state of the art openAI model is approximately 1.8 trillion
parameters.

217
00:20:51,680 --> 00:20:58,560
1.8 trillion parameters required several trillion tokens to go train.

218
00:20:58,560 --> 00:21:04,260
So a few trillion parameters on the order of a few trillion tokens on the order
of,

219
00:21:04,260 --> 00:21:14,800
when you multiply the two of them together, approximately 30, 40, 50 billion
quadrillion floating point operations.

220
00:21:14,880 --> 00:21:18,640
per second now we just have to do some co math right now just hang hang with me

221
00:21:19,200 --> 00:21:27,520
so you have 30 billion quadrillion a quadrillion is like a peta and so if you
had a petaflop

222
00:21:27,520 --> 00:21:35,200
gpu you would need 30 billion seconds to go compute to go train that model 30
billion

223
00:21:35,200 --> 00:21:41,840
seconds is approximately 1 000 years well 1 000 years it's worth it

224
00:21:44,880 --> 00:21:49,760
you like to do it sooner but it's worth it

225
00:21:52,800 --> 00:21:56,160
which is usually my answer when most people tell me hey how long how long is it
going to

226
00:21:56,160 --> 00:22:03,440
take to do something so we got 20 years it's worth it but can we do it next week

227
00:22:06,720 --> 00:22:14,400
and so 1 000 years 1 000 years so what we need what we need are bigger gpus

228
00:22:15,520 --> 00:22:22,160
we need much much bigger gpus we recognized this early on and we realized that
the answer

229
00:22:22,160 --> 00:22:26,960
is to put a whole bunch of gpus together and of course innovate a whole bunch of
things along

230
00:22:26,960 --> 00:22:32,640
the way like inventing tensor cores advancing mv links so that we could create
essentially

231
00:22:32,640 --> 00:22:39,040
virtually giant gpus and connecting them all together with amazing networks from
a company

232
00:22:39,040 --> 00:22:44,800
called melanox infiniband so that we could create these giant systems and so
dgx1 was our

233
00:22:44,800 --> 00:22:50,160
first version but it wasn't the last we built we built supercomputers all the
way all along the way

234
00:22:51,360 --> 00:23:01,040
in 2021 we had celine 4 500 gpus or so and then in 2023 we built one of the
largest ai

235
00:23:01,040 --> 00:23:08,960
supercomputers in the world it's just come online eos and as we're building
these things we're

236
00:23:08,960 --> 00:23:13,280
trying to help the world build these things and in order to help the world build
these things we got

237
00:23:13,280 --> 00:23:14,720
to build them first we're trying to help the world build these things we got to
build them first we're

238
00:23:14,720 --> 00:23:16,000
building these things we're trying to help the world build these things we got
to build these things we got to build the chips the systems

239
00:23:16,640 --> 00:23:23,440
the networking all of the software necessary to do this you should see these
systems imagine writing

240
00:23:23,440 --> 00:23:28,240
a piece of software that runs across the entire system distributing the
computation across

241
00:23:29,040 --> 00:23:33,520
thousands of gpus but inside are thousands of smaller gpus

242
00:23:35,280 --> 00:23:40,160
millions of gpus to distribute work across all of that and to balance the
workload

243
00:23:40,160 --> 00:23:44,560
so that you can get the most energy efficiency the best computation time

244
00:23:44,720 --> 00:23:50,000
keep your costs down and so those those fundamental innovations

245
00:23:51,360 --> 00:24:00,560
is what got us here and here we are as we see the miracle of chat gpt emerge in
front of us

246
00:24:00,560 --> 00:24:08,320
we also realize we have a long ways to go we need even larger models we're going
to train it with

247
00:24:08,320 --> 00:24:13,040
multi-modality data not just text on the internet but we're going to we're going
to train it on

248
00:24:13,040 --> 00:24:20,640
texts and images and graphs and charts and just as we learn watching tv and so
there's going to

249
00:24:20,640 --> 00:24:26,720
be a whole bunch of watching video so that these models can be grounded in
physics understands that

250
00:24:26,720 --> 00:24:33,600
an arm doesn't go through a wall and so these models would have common sense by
watching a lot

251
00:24:33,600 --> 00:24:39,680
of the world's video combined with a lot of the world's languages they'll use
things like synthetic

252
00:24:39,680 --> 00:24:42,800
data generation just as you and i do when we try to learn

253
00:24:43,040 --> 00:24:46,800
we might use our imagination to simulate how it's going to

254
00:24:46,800 --> 00:24:52,080
end up just as i did when i was preparing for this keynote i was simulating it
all along the way

255
00:24:55,120 --> 00:24:58,480
i hope it's going to turn out as well as i had into my head

256
00:25:06,000 --> 00:25:11,520
as i was simulating how this keynote was going to turn out somebody did say that
another performer

257
00:25:13,840 --> 00:25:16,240
did her performance completely on a treadmill

258
00:25:17,600 --> 00:25:23,520
so that she could be in shape to deliver it with full energy i i didn't do that

259
00:25:25,920 --> 00:25:29,280
if i get a little winded about 10 minutes into this you know what happened

260
00:25:31,440 --> 00:25:36,880
and so so where were we we're sitting here using synthetic data generation we're
going

261
00:25:36,880 --> 00:25:40,560
to use reinforcement learning we're going to practice it in our mind we're going
to have

262
00:25:40,560 --> 00:25:42,800
ai working with ai training each other

263
00:25:43,040 --> 00:25:48,560
just like student teacher debaters all of that is going to increase the size of
our model it's

264
00:25:48,560 --> 00:25:53,040
going to increase the amount of con the amount of data that we have and we're
going to have to build

265
00:25:53,040 --> 00:26:03,040
even bigger gpus hopper is fantastic but we need bigger gpus and so ladies and
gentlemen

266
00:26:05,200 --> 00:26:11,440
i would like to introduce you to a very very big gpu

267
00:26:13,040 --> 00:26:16,400
i wouldn't want to overstate that in particular but i know that we are an ego of
a всех

268
00:26:16,400 --> 00:26:18,560
the nets you are the ones to네 Herren

269
00:26:18,560 --> 00:26:26,400
i'll break you down

270
00:26:29,200 --> 00:26:34,880
named after david blackwell mathematician

271
00:26:36,000 --> 00:26:40,720
game theorists probability we thought it was a perfect

272
00:26:41,280 --> 00:26:41,840
perfect name

273
00:26:41,840 --> 00:26:42,880
Blackwell ladies and gentlemen enjoy this

274
00:26:42,880 --> 00:27:12,860
I don't know.

275
00:27:12,880 --> 00:27:42,860
I don't know.

276
00:27:42,880 --> 00:28:12,860
I don't know.

277
00:28:12,880 --> 00:28:42,860
I don't know.

278
00:28:42,880 --> 00:29:12,860
I don't know.

279
00:29:12,880 --> 00:29:19,160
Blackwell is not a chip.

280
00:29:19,160 --> 00:29:20,820
Blackwell is the name of a platform.

281
00:29:21,960 --> 00:29:25,700
People think we make GPUs, and we do,

282
00:29:26,580 --> 00:29:29,400
but GPUs don't look the way they used to.

283
00:29:30,960 --> 00:29:36,260
Here's the, if you will, the heart of the Blackwell system,

284
00:29:37,000 --> 00:29:39,800
and this inside the company is not called Blackwell.

285
00:29:39,880 --> 00:29:40,440
It's just a number.

286
00:29:41,400 --> 00:29:42,860
And Blackwell is a chip.

287
00:29:42,860 --> 00:29:47,680
This, this is Blackwell sitting next to, oh,

288
00:29:47,860 --> 00:29:50,380
this is the most advanced GPU in the world in production today.

289
00:29:54,560 --> 00:29:55,720
This is Hopper.

290
00:29:56,780 --> 00:29:57,900
This is Hopper.

291
00:29:58,180 --> 00:29:59,420
Hopper changed the world.

292
00:30:01,260 --> 00:30:02,640
This is Blackwell.

293
00:30:02,640 --> 00:30:02,680
This is Blackwell.

294
00:30:12,100 --> 00:30:12,840
It's okay.

295
00:30:12,860 --> 00:30:13,240
Hopper.

296
00:30:19,120 --> 00:30:20,380
You're, you're very good.

297
00:30:22,000 --> 00:30:23,280
Good, good boy.

298
00:30:25,020 --> 00:30:25,660
Oh, good girl.

299
00:30:30,220 --> 00:30:35,640
208 billion transistors, and so, so you could see, you, I can see,

300
00:30:36,100 --> 00:30:38,360
there's a small line between two dyes.

301
00:30:38,360 --> 00:30:41,640
This is the first time two dyes have abutted like this together

302
00:30:41,640 --> 00:30:42,480
in such a way.

303
00:30:42,480 --> 00:30:42,500
This is the first time two dyes have abutted like this together in such a way.

304
00:30:42,500 --> 00:30:42,560
This is the first time two dyes have abutted like this together in such a way.

305
00:30:42,560 --> 00:30:46,180
It's such a way that the two chip, the two dyes think it's one chip.

306
00:30:47,180 --> 00:30:51,000
There's 10 terabytes of data between it, 10 terabytes per second,

307
00:30:51,700 --> 00:30:56,580
so that these two, these two sides of the Blackwell chip have no clue which side
they're on.

308
00:30:57,540 --> 00:31:03,000
There's no memory locality issues, no cache issues, it's just one giant chip,

309
00:31:03,820 --> 00:31:09,360
and so, when we were told that Blackwell's ambitions were beyond the limits of
physics,

310
00:31:10,000 --> 00:31:11,600
the engineer said, so what?

311
00:31:11,600 --> 00:31:18,040
and so this is what what happened and so this is the Blackwell chip and it goes

312
00:31:18,040 --> 00:31:25,480
into two types of systems the first one is form-fit function compatible to

313
00:31:25,480 --> 00:31:29,540
hopper and so you slide on hopper and you push in Blackwell that's the reason

314
00:31:29,540 --> 00:31:33,980
why one of the challenges of ramping is going to be so efficient there are

315
00:31:33,980 --> 00:31:38,120
installations of hoppers all over the world and they could be they could be

316
00:31:38,120 --> 00:31:42,860
you know the same infrastructure same design the power the electricity the

317
00:31:42,860 --> 00:31:49,580
thermals the software identical push it right back and so this is a hopper

318
00:31:49,580 --> 00:31:55,940
version for the current HGX configuration and this is what the other

319
00:31:55,940 --> 00:32:01,880
the second hopper looks like this now this is a prototype board and Janine

320
00:32:01,880 --> 00:32:08,120
could I just borrow ladies and gentlemen Janine Paul

321
00:32:08,120 --> 00:32:17,140
and so this this is the this is a fully functioning board and I'll just be

322
00:32:17,140 --> 00:32:29,580
careful here this right here is I don't know 10 billion dollars the second one's

323
00:32:29,580 --> 00:32:37,540
five it gets cheaper after that so any customers in the audience

324
00:32:37,560 --> 00:32:37,940
yeah

325
00:32:37,940 --> 00:32:38,100
yeah

326
00:32:38,100 --> 00:32:38,940
It's okay.

327
00:32:42,920 --> 00:32:44,820
All right, but this one's quite expensive.

328
00:32:44,820 --> 00:32:46,360
This is the bring-up board,

329
00:32:46,360 --> 00:32:49,460
and the way it's gonna go to production

330
00:32:49,460 --> 00:32:51,400
is like this one here, okay?

331
00:32:51,400 --> 00:32:53,520
And so you're gonna take this.

332
00:32:53,520 --> 00:32:58,520
It has two Blackwell chips and four Blackwell dies

333
00:32:59,040 --> 00:33:02,200
connected to a Grace CPU.

334
00:33:02,200 --> 00:33:05,660
The Grace CPU has a super-fast chip-to-chip link.

335
00:33:05,660 --> 00:33:09,640
What's amazing is this computer is the first of its kind

336
00:33:09,640 --> 00:33:12,120
where this much computation, first of all,

337
00:33:12,120 --> 00:33:15,540
fits into this small of a place.

338
00:33:15,540 --> 00:33:18,220
Second, it's memory-coherent.

339
00:33:18,220 --> 00:33:20,920
They feel like they're just one big happy family

340
00:33:20,920 --> 00:33:24,120
working on one application together,

341
00:33:24,120 --> 00:33:26,320
and so everything is coherent within it.

342
00:33:28,000 --> 00:33:30,820
Just the amount of, you know, you saw the numbers.

343
00:33:30,820 --> 00:33:34,140
There's a lot of terabytes this and terabytes that,

344
00:33:34,140 --> 00:33:35,440
but this is a miracle.

345
00:33:35,440 --> 00:33:38,140
This is a, this, let's see.

346
00:33:38,140 --> 00:33:40,000
What are some of the things on here?

347
00:33:40,000 --> 00:33:45,000
There's a NVLink on top, PCI Express on the bottom,

348
00:33:47,560 --> 00:33:52,560
on your, which one is mine, and your left?

349
00:33:53,520 --> 00:33:55,780
One of them, it doesn't matter.

350
00:33:55,780 --> 00:33:59,980
One of them is a CPU chip-to-chip link.

351
00:33:59,980 --> 00:34:02,000
It's my left or your, depending on which side.

352
00:34:02,000 --> 00:34:05,060
I was just, I was trying to sort that out, and I just kind of.

353
00:34:06,160 --> 00:34:07,000
Doesn't matter.

354
00:34:12,340 --> 00:34:14,000
Hopefully it comes plugged in, so.

355
00:34:19,140 --> 00:34:22,560
Okay, so this is the Grace Blackwell system.

356
00:34:32,040 --> 00:34:33,040
But there's more.

357
00:34:35,440 --> 00:34:40,140
So turns out, it turns out, all of the specs is fantastic,

358
00:34:40,140 --> 00:34:41,820
but we need a whole lot of new features.

359
00:34:43,360 --> 00:34:47,440
In order to push the limits beyond, if you will, the limits of physics,

360
00:34:48,500 --> 00:34:52,020
we would like to always get a lot more X-factors.

361
00:34:52,020 --> 00:34:55,720
And so one of the things that we did was we invented another transformer engine.

362
00:34:55,720 --> 00:34:58,600
Another transformer engine, the second generation.

363
00:34:58,600 --> 00:35:04,240
It has the ability to dynamically and automatically rescale and recast,

364
00:35:04,240 --> 00:35:05,420
and recast.

365
00:35:05,420 --> 00:35:13,540
numerical formats to a lower precision whenever it can remember artificial
intelligence is about

366
00:35:13,540 --> 00:35:21,220
probability and so you kind of have you know 1.7 approximately 1.7 times
approximately 1.4 to be

367
00:35:21,220 --> 00:35:27,300
approximately something else does that make sense and so so the ability for the
mathematics to

368
00:35:27,300 --> 00:35:34,200
retain the precision and the range necessary in that particular stage of the
pipeline super

369
00:35:34,200 --> 00:35:39,960
important and so this is it's not just about the fact that we designed a smaller
ALU it's not quite

370
00:35:39,960 --> 00:35:46,920
the world's not quite that simple you've got to figure out when you can use that
across a computation

371
00:35:46,920 --> 00:35:55,260
that is thousands of GPUs it's running for weeks and weeks on weeks and you want
to make sure that

372
00:35:55,260 --> 00:36:01,740
the the the train job is going to converge and so this new transformer engine we
have a fifth

373
00:36:01,740 --> 00:36:02,820
generation MV link

374
00:36:04,200 --> 00:36:11,280
it's now twice as fast as hopper but very importantly it has computation in the
network and the reason

375
00:36:11,280 --> 00:36:16,200
for that is because when you have so many different GPUs working together we
have to share our

376
00:36:16,200 --> 00:36:21,380
information with each other we have to synchronize and update each other and
every so often we have

377
00:36:21,380 --> 00:36:27,240
to reduce the partial products and then rebroadcast out the partial products
that some of the partial

378
00:36:27,240 --> 00:36:32,280
products back to everybody else and so there's a lot of what is called all
reduce and all to all and

379
00:36:32,280 --> 00:36:33,420
all gather

380
00:36:34,200 --> 00:36:39,000
in this area of synchronization and collectives so that we can have GPUs working
with each other

381
00:36:39,000 --> 00:36:46,140
having extraordinarily fast links and being able to do mathematics right in the
network allows us to

382
00:36:46,140 --> 00:36:52,980
essentially amplify even further so even though it's 1.8 terabytes per second
it's effectively higher

383
00:36:52,980 --> 00:37:01,920
than that and so it's many times that of hopper the likelihood of a
supercomputer running for weeks

384
00:37:01,920 --> 00:37:04,140
on end is approximately zero

385
00:37:04,200 --> 00:37:08,880
and the reason for that is because there's so many components working at the
same time

386
00:37:09,780 --> 00:37:15,480
the statistic the probability of them working continuously is very low and so we
need to make

387
00:37:15,480 --> 00:37:22,680
sure that whenever there is a well we checkpoint and restart as often as we can
but if we have the

388
00:37:22,680 --> 00:37:32,220
ability to detect a weak chip or a weak note early we can retire it and maybe
swap in another processor

389
00:37:32,220 --> 00:37:34,020
that ability to keep the

390
00:37:34,200 --> 00:37:38,880
authorization of the super computer high especially when you just spent 2
billion dollars building it is super

391
00:37:38,880 --> 00:37:40,880
important and so we put in a race engine a reliability engine that does a
hundred percent self test in

392
00:37:40,880 --> 00:37:50,680
system test of every single gate every single bit of memory on the Blackwell
chip and all the memory

393
00:37:50,680 --> 00:37:57,240
doesn't connected to it it's almost as if I'm helping a'm

394
00:37:57,240 --> 00:37:59,480
helping a rotor test a real-time information chip and all using a physical
computer so just starting

395
00:37:59,480 --> 00:38:00,580
to understand where this K wheat is in my work language the power chip is great
and the

396
00:38:00,580 --> 00:38:02,580
capabilities it's taking you are amazing and they're really really exciting I'm
really interested in

397
00:38:02,580 --> 00:38:03,780
you do learn but you have to use a computer to work so in all that stuff it's
even using a simple radio

398
00:38:03,780 --> 00:38:11,100
as if we shipped with every single chip its own advanced tester that we test our

399
00:38:11,100 --> 00:38:14,880
chips with. This is the first time we're doing this, super excited about it.

400
00:38:14,880 --> 00:38:17,800
Secure AI.

401
00:38:22,780 --> 00:38:32,340
Only this conference do they clap for RAS. The secure AI, obviously you've

402
00:38:32,340 --> 00:38:36,960
just spent hundreds of millions of dollars creating a very important AI and

403
00:38:36,960 --> 00:38:42,840
the code, the intelligence of that AI is encoded in the parameters. You want to

404
00:38:42,840 --> 00:38:45,600
make sure that on the one hand you don't lose it, on the other hand it doesn't
get

405
00:38:45,600 --> 00:38:53,280
contaminated. And so we now have the ability to encrypt data, of course at

406
00:38:53,280 --> 00:39:01,060
rest, but also in transit and while it's being computed. It's all encrypted and
so

407
00:39:01,060 --> 00:39:02,340
we now have the ability to

408
00:39:02,340 --> 00:39:07,820
encrypt and transmission and when we're computing it is in a trusted

409
00:39:07,820 --> 00:39:14,400
environment, trusted engine environment. The last thing is decompression. Moving

410
00:39:14,400 --> 00:39:19,140
data in and out of these nodes when the compute is so fast becomes really

411
00:39:19,140 --> 00:39:25,020
essential. And so we've put in a high line speed compression engine and

412
00:39:25,020 --> 00:39:29,740
effectively moves data 20 times faster in and out of these computers. These

413
00:39:29,740 --> 00:39:31,920
computers are so powerful and

414
00:39:32,340 --> 00:39:36,800
And there's such a large investment, the last thing we want to do is have them
be idle.

415
00:39:36,800 --> 00:39:47,100
And so all of these capabilities are intended to keep Blackwell fed and as busy
as possible.

416
00:39:47,100 --> 00:39:57,780
Overall, compared to Hopper, it is two and a half times the FP8 performance for
training

417
00:39:57,780 --> 00:40:00,020
per chip.

418
00:40:00,020 --> 00:40:05,200
It also has this new format called FP6, so that even though the computation
speed is

419
00:40:05,200 --> 00:40:11,620
the same, the bandwidth that's amplified because of the memory, the amount of
parameters you

420
00:40:11,620 --> 00:40:14,400
can store in the memory is now amplified.

421
00:40:14,400 --> 00:40:17,320
FP4 effectively doubles the throughput.

422
00:40:17,320 --> 00:40:20,840
This is vitally important for inference.

423
00:40:20,840 --> 00:40:27,040
One of the things that is becoming very clear is that whenever you use a
computer with AI

424
00:40:27,040 --> 00:40:29,080
on the other side.

425
00:40:29,080 --> 00:40:30,000
When you're chatting with...

426
00:40:30,000 --> 00:40:38,880
When you get the chat bot, when you're asking it to review or make an image,
remember in

427
00:40:38,880 --> 00:40:43,120
the back is a GPU generating tokens.

428
00:40:43,120 --> 00:40:49,420
Some people call it inference, but it's more appropriately generation.

429
00:40:49,420 --> 00:40:53,360
The way that computing has done in the past was retrieval.

430
00:40:53,360 --> 00:40:58,160
You would grab your phone, you would touch something, some signals go off,
basically

431
00:40:58,160 --> 00:41:00,000
an email goes off to some storage.

432
00:41:00,000 --> 00:41:02,520
You're talking to a computer somewhere.

433
00:41:02,520 --> 00:41:04,060
There's prerecorded content.

434
00:41:04,060 --> 00:41:07,520
Somebody wrote a story or somebody made an image or somebody recorded a video.

435
00:41:07,520 --> 00:41:13,880
That prerecorded content is then streamed back to the phone and recomposed in a
way

436
00:41:13,880 --> 00:41:18,780
based on a recommender system to present the information to you.

437
00:41:18,780 --> 00:41:25,280
You know that in the future, the vast majority of that content will not be
retrieved.

438
00:41:25,280 --> 00:41:28,920
And the reason for that is because that was prerecorded by somebody who doesn't
understand

439
00:41:28,920 --> 00:41:29,840
the context.

440
00:41:29,840 --> 00:41:35,540
retrieve so much content. If you can be working with an AI that understands the

441
00:41:35,540 --> 00:41:39,400
context, who you are, for what reason you're fetching this information, and

442
00:41:39,400 --> 00:41:45,720
produces the information for you just the way you like it, the amount of energy

443
00:41:45,720 --> 00:41:50,000
we save, the amount of networking bandwidth we save, the amount of waste of

444
00:41:50,000 --> 00:41:56,180
time we save, will be tremendous. The future is generative, which is the reason

445
00:41:56,180 --> 00:42:00,200
why we call it generative AI, which is the reason why this is a brand new

446
00:42:00,200 --> 00:42:06,080
industry. The way we compute is fundamentally different. We created a

447
00:42:06,080 --> 00:42:12,200
processor for the generative AI era, and one of the most important parts of it
is

448
00:42:12,200 --> 00:42:21,400
content token generation. We call it, this format is FP4. Well, that's a lot of

449
00:42:21,400 --> 00:42:26,160
computation. 5x, the token

450
00:42:26,160 --> 00:42:37,200
generation, 5x, the inference capability of Hopper, seems like enough. But why

451
00:42:37,200 --> 00:42:43,680
stop there? The answer is it's not enough, and I'm going to show you why. I'm
going

452
00:42:43,680 --> 00:42:48,000
to show you why. And so we would like to have a bigger GPU, even bigger than
this

453
00:42:48,000 --> 00:42:55,400
one. And so we decided to scale it, and notice, but first let me just tell you

454
00:42:55,400 --> 00:42:56,160
how we've scaled.

455
00:42:56,160 --> 00:43:02,320
Over the course of the last eight years, we've increased computation by 1,000

456
00:43:02,320 --> 00:43:06,800
times. Eight years, 1,000 times. Remember back in the good old days of Moore's
Law,

457
00:43:06,800 --> 00:43:13,200
it was 2x, well, 5x every, what, 10x every five years. That's easiest,

458
00:43:13,200 --> 00:43:18,840
easiest math. 10x every five years, a hundred times every ten years. One hundred

459
00:43:18,840 --> 00:43:25,080
times every ten years, at the, in the middle, in the heydays of the PC

460
00:43:25,080 --> 00:43:25,680
Revolution.

461
00:43:26,160 --> 00:43:33,400
One hundred times every ten years. In the last eight years, we've gone 1,000
times.

462
00:43:33,400 --> 00:43:38,000
We have two more years to go. And so that puts it in perspective.

463
00:43:42,000 --> 00:43:46,600
The rate at which we're advancing computing is insane, and it's still not

464
00:43:46,600 --> 00:43:54,040
fast enough, so we built another chip. This chip is just an incredible chip. We

465
00:43:54,040 --> 00:43:56,040
call it the MV-Link switch.

466
00:43:56,960 --> 00:44:02,040
Its 50 billion transistors, it's almost the size of Hopper all by itself. This

467
00:44:02,040 --> 00:44:08,120
switch ship has four MV-Link's in it, each 1.8 terabytes per second,

468
00:44:11,080 --> 00:44:13,400
and it has computation, and as I mentioned,

469
00:44:14,600 --> 00:44:18,280
what is this chip for? If we were to build such

470
00:44:20,280 --> 00:44:23,540
a chip, we can have every single GPU talk

471
00:44:23,540 --> 00:44:25,960
to every other GPU at full speed.

472
00:44:26,160 --> 00:44:29,160
at the same time. That's insane.

473
00:44:37,160 --> 00:44:40,160
It doesn't even make sense.

474
00:44:40,160 --> 00:44:43,160
But if you could do that, if you can find a way to do that

475
00:44:43,160 --> 00:44:46,160
and build a system to do that,

476
00:44:46,160 --> 00:44:49,160
that's cost-effective. That's cost-effective.

477
00:44:49,160 --> 00:44:52,160
How incredible would it be

478
00:44:52,160 --> 00:44:55,160
that we could have all these GPUs connect over

479
00:44:55,160 --> 00:44:58,160
a coherent link

480
00:44:58,160 --> 00:45:01,160
so that they effectively are one giant GPU?

481
00:45:01,160 --> 00:45:04,160
Well, one of the great inventions

482
00:45:04,160 --> 00:45:07,160
in order to make it cost-effective is that this chip

483
00:45:07,160 --> 00:45:10,160
has to drive copper directly.

484
00:45:10,160 --> 00:45:13,160
The Surtees of this chip is just a phenomenal invention

485
00:45:13,160 --> 00:45:16,160
so that we could do direct drive to copper.

486
00:45:16,160 --> 00:45:19,160
And as a result, you can build a system

487
00:45:19,160 --> 00:45:22,160
that looks like this.

488
00:45:22,160 --> 00:45:25,160
Now this system,

489
00:45:25,160 --> 00:45:28,160
this system,

490
00:45:28,160 --> 00:45:31,160
is kind of insane.

491
00:45:31,160 --> 00:45:34,160
This is one DGX.

492
00:45:34,160 --> 00:45:37,160
This is what a DGX looks like now.

493
00:45:37,160 --> 00:45:40,160
Remember, just six years ago,

494
00:45:40,160 --> 00:45:43,160
it was pretty heavy,

495
00:45:43,160 --> 00:45:46,160
but I was able to lift it.

496
00:45:46,160 --> 00:45:49,160
I delivered

497
00:45:49,160 --> 00:45:52,160
the DGX to a company

498
00:45:52,160 --> 00:45:55,160
that was the first DGX1 to open AI.

499
00:45:55,160 --> 00:45:57,160
And the researchers there,

500
00:45:57,160 --> 00:45:59,160
the pictures are on the internet,

501
00:45:59,160 --> 00:46:02,160
and we all autographed it.

502
00:46:02,160 --> 00:46:05,160
And if you come to my office,

503
00:46:05,160 --> 00:46:08,160
it's autographed there. It's really beautiful.

504
00:46:08,160 --> 00:46:10,160
But you could lift it.

505
00:46:10,160 --> 00:46:11,160
This DGX,

506
00:46:11,160 --> 00:46:12,160
this DGX,

507
00:46:12,160 --> 00:46:14,160
that DGX, by the way,

508
00:46:14,160 --> 00:46:18,160
was 170 teraflops.

509
00:46:18,160 --> 00:46:21,160
If you're not familiar with the numbering system,

510
00:46:21,160 --> 00:46:24,160
it's 0.17 teraflops.

511
00:46:24,160 --> 00:46:26,160
So this is 720.

512
00:46:26,160 --> 00:46:28,160
The first one I delivered to open AI

513
00:46:28,160 --> 00:46:30,160
was 0.17.

514
00:46:30,160 --> 00:46:32,160
You could round it up to 0.2.

515
00:46:32,160 --> 00:46:34,160
It won't make any difference.

516
00:46:34,160 --> 00:46:36,160
But back then, it was like, wow,

517
00:46:36,160 --> 00:46:38,160
you know, 30 more teraflops.

518
00:46:38,160 --> 00:46:42,160
And so this is now 720 teraflops,

519
00:46:42,160 --> 00:46:44,160
almost an exaflop for training,

520
00:46:44,160 --> 00:46:47,160
and the world's first one exaflop machine

521
00:46:47,160 --> 00:46:49,160
in one rack.

522
00:46:51,160 --> 00:46:58,160
Just so you know,

523
00:46:58,160 --> 00:47:01,160
there are only a couple, two, three exaflops machines

524
00:47:01,160 --> 00:47:03,160
on the planet as we speak.

525
00:47:03,160 --> 00:47:07,160
And so this is an exaflop AI system

526
00:47:07,160 --> 00:47:09,160
in one single rack.

527
00:47:09,160 --> 00:47:14,160
Well, let's take a look at the back of it.

528
00:47:14,160 --> 00:47:17,160
So this is what makes it possible.

529
00:47:17,160 --> 00:47:18,160
That's the back,

530
00:47:18,160 --> 00:47:20,160
that's the back,

531
00:47:20,160 --> 00:47:23,160
that's the back of the VXMV link spine.

532
00:47:23,160 --> 00:47:27,160
130 terabytes per second

533
00:47:27,160 --> 00:47:30,160
goes through the back of that chassis.

534
00:47:30,160 --> 00:47:32,160
That is more than the aggregate bandwidth

535
00:47:32,160 --> 00:47:34,160
of the internet.

536
00:47:43,160 --> 00:47:45,160
So we could basically send everything

537
00:47:45,160 --> 00:47:47,160
to everybody within a second.

538
00:47:47,160 --> 00:47:49,160
And so we have 5,000 cables,

539
00:47:49,160 --> 00:47:52,160
5,000 NVLink cables,

540
00:47:52,160 --> 00:47:54,160
in total, two miles.

541
00:47:54,160 --> 00:47:56,160
Now this is the amazing thing.

542
00:47:56,160 --> 00:47:58,160
If we had to use optics,

543
00:47:58,160 --> 00:48:01,160
we would have had to use transceivers and retimers,

544
00:48:01,160 --> 00:48:03,160
and those transceivers and retimers alone

545
00:48:03,160 --> 00:48:08,160
would have cost 20,000 watts,

546
00:48:08,160 --> 00:48:11,160
two kilowatts of just transceivers alone,

547
00:48:11,160 --> 00:48:14,160
just to drive the NVLink spine.

548
00:48:14,160 --> 00:48:15,160
As a result,

549
00:48:15,160 --> 00:48:18,160
we did it completely for free over NVLink switch,

550
00:48:18,160 --> 00:48:21,160
and we were able to save the 20 kilowatts for computation.

551
00:48:21,160 --> 00:48:24,160
This entire rack is 120 kilowatts,

552
00:48:24,160 --> 00:48:27,160
so that 20 kilowatts makes a huge difference.

553
00:48:27,160 --> 00:48:29,160
It's liquid cooled.

554
00:48:29,160 --> 00:48:32,160
What goes in is 25 degrees C about room temperature.

555
00:48:32,160 --> 00:48:37,160
What comes out is 45 degrees C about your jacuzzi.

556
00:48:37,160 --> 00:48:39,160
So room temperature goes in,

557
00:48:39,160 --> 00:48:40,160
jacuzzi comes out,

558
00:48:40,160 --> 00:48:42,160
two liters per second.

559
00:48:48,160 --> 00:48:52,160
We could sell a peripheral.

560
00:48:57,160 --> 00:49:00,160
600,000 parts.

561
00:49:00,160 --> 00:49:01,160
Somebody used to say,

562
00:49:01,160 --> 00:49:03,160
you know, you guys make GPUs,

563
00:49:03,160 --> 00:49:04,160
and we do,

564
00:49:04,160 --> 00:49:07,160
but this is what a GPU looks like to me.

565
00:49:07,160 --> 00:49:10,160
When somebody says GPU, I see this.

566
00:49:10,160 --> 00:49:13,160
Two years ago, when I saw a GPU, it was the HGX.

567
00:49:13,160 --> 00:49:16,160
It was 70 pounds, 35,000 parts.

568
00:49:16,160 --> 00:49:17,160
Our GPUs now

569
00:49:17,160 --> 00:49:20,160
are 600,000 parts

570
00:49:20,160 --> 00:49:23,160
and 3,000 pounds.

571
00:49:23,160 --> 00:49:25,160
3,000 pounds.

572
00:49:25,160 --> 00:49:26,160
3,000 pounds,

573
00:49:26,160 --> 00:49:28,160
that's kind of like the weight of a,

574
00:49:28,160 --> 00:49:33,160
you know, carbon fiber Ferrari.

575
00:49:33,160 --> 00:49:36,160
I don't know if that's a useful metric, but...

576
00:49:38,160 --> 00:49:39,160
Everybody's going,

577
00:49:39,160 --> 00:49:40,160
I feel it.

578
00:49:40,160 --> 00:49:41,160
I feel it. I get it.

579
00:49:41,160 --> 00:49:42,160
I get that.

580
00:49:42,160 --> 00:49:45,160
Now that you mention that, I feel it.

581
00:49:45,160 --> 00:49:47,160
I don't know what's 3,000 pounds.

582
00:49:47,160 --> 00:49:50,160
Okay, so 3,000 pounds,

583
00:49:50,160 --> 00:49:51,160
a ton and a half.

584
00:49:51,160 --> 00:49:54,160
So it's not quite an elephant.

585
00:49:54,160 --> 00:49:56,160
So this is what a DGX looks like.

586
00:49:56,160 --> 00:49:58,160
Now let's see what it looks like in operation.

587
00:49:58,160 --> 00:49:59,160
Okay, let's imagine,

588
00:49:59,160 --> 00:50:02,160
how do we put this to work and what does that mean?

589
00:50:02,160 --> 00:50:05,160
Well, if you were to train a GPT model,

590
00:50:05,160 --> 00:50:08,160
1.8 trillion parameter model,

591
00:50:08,160 --> 00:50:11,160
it took apparently about, you know,

592
00:50:11,160 --> 00:50:15,160
three to five months or so with 25,000 amperes.

593
00:50:15,160 --> 00:50:16,160
If we were to do it with Hopper,

594
00:50:16,160 --> 00:50:19,160
it would probably take something like 8,000 GPUs

595
00:50:19,160 --> 00:50:21,160
and it would consume 15 megawatts.

596
00:50:21,160 --> 00:50:23,160
8,000 GPUs and 15 megawatts.

597
00:50:23,160 --> 00:50:25,160
It would take 90 days, about three months.

598
00:50:25,160 --> 00:50:29,160
And that would allow you to train something that is,

599
00:50:29,160 --> 00:50:32,160
you know, this groundbreaking AI model.

600
00:50:32,160 --> 00:50:37,160
And this is obviously not as expensive as anybody would think,

601
00:50:37,160 --> 00:50:39,160
but it's 8,000 GPUs.

602
00:50:39,160 --> 00:50:40,160
It's still a lot of money.

603
00:50:40,160 --> 00:50:43,160
And so 8,000 GPUs, 15 megawatts.

604
00:50:43,160 --> 00:50:45,160
If you were to use Blackwell to do this,

605
00:50:45,160 --> 00:50:50,160
it would only take 2,000 GPUs.

606
00:50:50,160 --> 00:50:53,160
2,000 GPUs, same 90 days.

607
00:50:53,160 --> 00:50:55,160
But this is the amazing part.

608
00:50:55,160 --> 00:50:57,160
Only four megawatts of power.

609
00:50:57,160 --> 00:51:00,160
So from 15, yeah, that's right.

610
00:51:05,160 --> 00:51:07,160
And that's our goal.

611
00:51:07,160 --> 00:51:11,160
Our goal is to continuously drive down the cost and the energy.

612
00:51:11,160 --> 00:51:12,160
They're directly proportional to each other.

613
00:51:12,160 --> 00:51:14,160
Cost and energy associated with the computer.

614
00:51:14,160 --> 00:51:17,160
Associated with the computing so that we can continue to expand

615
00:51:17,160 --> 00:51:20,160
and scale up the computation that we have to do

616
00:51:20,160 --> 00:51:22,160
to train the next generation models.

617
00:51:22,160 --> 00:51:25,160
Well, this is training.

618
00:51:25,160 --> 00:51:30,160
Inference or generation is vitally important going forward.

619
00:51:30,160 --> 00:51:32,160
You know, probably some half of the time

620
00:51:32,160 --> 00:51:34,160
that NVIDIA GPUs are in the cloud these days,

621
00:51:34,160 --> 00:51:36,160
it's being used for token generation.

622
00:51:36,160 --> 00:51:38,160
You know, they're either doing co-pilot this

623
00:51:38,160 --> 00:51:40,160
or chat, you know, chat GPT that

624
00:51:40,160 --> 00:51:42,160
or all these different models that are being used

625
00:51:42,160 --> 00:51:43,160
when you're interacting with it.

626
00:51:43,160 --> 00:51:46,160
Or generating images or generating videos.

627
00:51:46,160 --> 00:51:49,160
Generating proteins, generating chemicals.

628
00:51:49,160 --> 00:51:52,160
There's a bunch of generation going on.

629
00:51:52,160 --> 00:51:56,160
All of that is in the category of computing we call inference.

630
00:51:56,160 --> 00:52:00,160
But inference is extremely hard for large language models

631
00:52:00,160 --> 00:52:03,160
because these large language models have several properties.

632
00:52:03,160 --> 00:52:05,160
One, they're very large.

633
00:52:05,160 --> 00:52:07,160
And so it doesn't fit on one GPU.

634
00:52:07,160 --> 00:52:11,160
This is, imagine, imagine Excel doesn't fit on one GPU.

635
00:52:11,160 --> 00:52:12,160
You know?

636
00:52:12,160 --> 00:52:13,160
And, you know,

637
00:52:13,160 --> 00:52:15,160
imagine some application you're running on a daily basis

638
00:52:15,160 --> 00:52:17,160
doesn't fit on one computer.

639
00:52:17,160 --> 00:52:20,160
Like a video game doesn't fit on one computer.

640
00:52:20,160 --> 00:52:22,160
And most, in fact, do.

641
00:52:22,160 --> 00:52:25,160
And many times in the past, in hyperscale computing,

642
00:52:25,160 --> 00:52:28,160
many applications for many people fit on the same computer.

643
00:52:28,160 --> 00:52:32,160
And now, all of a sudden, this one inference application

644
00:52:32,160 --> 00:52:34,160
where you're interacting with this chat bot,

645
00:52:34,160 --> 00:52:38,160
that chat bot requires a super computer in the back to run it.

646
00:52:38,160 --> 00:52:40,160
And that's the future.

647
00:52:40,160 --> 00:52:42,160
The future is generative with these chat bots.

648
00:52:42,160 --> 00:52:45,160
And these chat bots are trillions of tokens,

649
00:52:45,160 --> 00:52:47,160
trillions of parameters.

650
00:52:47,160 --> 00:52:51,160
And they have to generate tokens at interactive rates.

651
00:52:51,160 --> 00:52:53,160
Now, what does that mean?

652
00:52:53,160 --> 00:52:58,160
Well, three tokens is about a word.

653
00:52:58,160 --> 00:53:05,160
You know, the, you know, space, the final frontier,

654
00:53:05,160 --> 00:53:07,160
these are the adventures.

655
00:53:07,160 --> 00:53:10,160
That's like 80 tokens.

656
00:53:10,160 --> 00:53:11,160
Okay?

657
00:53:11,160 --> 00:53:13,160
I don't know if that's useful to you.

658
00:53:13,160 --> 00:53:19,160
And so, you know, the art of communications

659
00:53:19,160 --> 00:53:22,160
is selecting good analogies.

660
00:53:22,160 --> 00:53:25,160
Yeah, this is not going well.

661
00:53:28,160 --> 00:53:30,160
I don't know what he's talking about.

662
00:53:30,160 --> 00:53:32,160
Never seen Star Trek.

663
00:53:32,160 --> 00:53:34,160
And so, here we are.

664
00:53:34,160 --> 00:53:36,160
We're trying to generate these tokens.

665
00:53:36,160 --> 00:53:38,160
When you're interacting with it, you're hoping that the tokens

666
00:53:38,160 --> 00:53:40,160
come back to you as quickly as possible

667
00:53:40,160 --> 00:53:42,160
and as quickly as you can read it.

668
00:53:42,160 --> 00:53:45,160
And so, the ability for generation tokens is really important.

669
00:53:45,160 --> 00:53:49,160
You have to paralyze the work of this model across many, many GPUs

670
00:53:49,160 --> 00:53:51,160
so that you could achieve several things.

671
00:53:51,160 --> 00:53:54,160
One, on the one hand, you would like throughput

672
00:53:54,160 --> 00:53:57,160
because that throughput reduces the cost,

673
00:53:57,160 --> 00:54:01,160
the overall cost per token of generating.

674
00:54:01,160 --> 00:54:06,160
So, your throughput dictates the cost of delivering the service.

675
00:54:06,160 --> 00:54:09,160
On the other hand, you have another interactive rate,

676
00:54:09,160 --> 00:54:11,160
which is another tokens per second,

677
00:54:11,160 --> 00:54:13,160
where it's about per user.

678
00:54:13,160 --> 00:54:15,160
And that has everything to do with quality of service.

679
00:54:15,160 --> 00:54:20,160
And so, these two things compete against each other.

680
00:54:20,160 --> 00:54:22,160
And we have to find a way to distribute work

681
00:54:22,160 --> 00:54:24,160
across all of these different GPUs

682
00:54:24,160 --> 00:54:27,160
and paralyze it in a way that allows us to achieve both.

683
00:54:27,160 --> 00:54:32,160
And it turns out the search space is enormous.

684
00:54:32,160 --> 00:54:35,160
You know, I told you there's going to be math involved.

685
00:54:35,160 --> 00:54:38,160
And everybody's going, oh, dear.

686
00:54:39,160 --> 00:54:43,160
I heard some gasps just now when I put up that slide.

687
00:54:43,160 --> 00:54:47,160
So, this right here, the y-axis is tokens per second,

688
00:54:47,160 --> 00:54:49,160
data center throughput.

689
00:54:49,160 --> 00:54:53,160
The x-axis is tokens per second, interactivity of the person.

690
00:54:53,160 --> 00:54:55,160
And notice, the upper right is the best.

691
00:54:55,160 --> 00:54:58,160
You want interactivity to be very high,

692
00:54:58,160 --> 00:55:00,160
number of tokens per second per user.

693
00:55:00,160 --> 00:55:03,160
You want the tokens per second per data center to be very high.

694
00:55:03,160 --> 00:55:05,160
The upper right is terrific.

695
00:55:05,160 --> 00:55:07,160
However, it's very hard to do that.

696
00:55:07,160 --> 00:55:09,160
And in order for us to search

697
00:55:09,160 --> 00:55:12,160
for the best answer across every single one

698
00:55:12,160 --> 00:55:15,160
of those intersections, x, y coordinates,

699
00:55:15,160 --> 00:55:18,160
okay, so just look at every single x, y coordinate,

700
00:55:18,160 --> 00:55:21,160
all those blue dots came from some repartitioning

701
00:55:21,160 --> 00:55:23,160
of the software.

702
00:55:23,160 --> 00:55:26,160
Some optimizing solution has to go and figure out

703
00:55:26,160 --> 00:55:31,160
whether to use tensor parallel, expert parallel,

704
00:55:31,160 --> 00:55:34,160
pipeline parallel, or data parallel,

705
00:55:34,160 --> 00:55:37,160
and distribute this enormous model

706
00:55:37,160 --> 00:55:39,160
across all these different GPUs.

707
00:55:39,160 --> 00:55:42,160
And sustain the performance that you need.

708
00:55:42,160 --> 00:55:44,160
This exploration space would be impossible

709
00:55:44,160 --> 00:55:47,160
if not for the programmability of NVIDIA's GPUs.

710
00:55:47,160 --> 00:55:49,160
And so we could, because of CUDA,

711
00:55:49,160 --> 00:55:51,160
because we have such a rich ecosystem,

712
00:55:51,160 --> 00:55:53,160
we could explore this universe

713
00:55:53,160 --> 00:55:56,160
and find that green roofline.

714
00:55:56,160 --> 00:55:58,160
It turns out that green roofline,

715
00:55:58,160 --> 00:56:02,160
notice, you got TP2EPADP4,

716
00:56:02,160 --> 00:56:05,160
it means two tensor parallel,

717
00:56:05,160 --> 00:56:08,160
tensor parallel across two GPUs,

718
00:56:08,160 --> 00:56:10,160
expert parallel across eight,

719
00:56:10,160 --> 00:56:12,160
data parallel across four.

720
00:56:12,160 --> 00:56:13,160
Notice on the other end,

721
00:56:13,160 --> 00:56:15,160
you got tensor parallel across four,

722
00:56:15,160 --> 00:56:17,160
and expert parallel across 16.

723
00:56:17,160 --> 00:56:20,160
The configuration, the distribution of that software,

724
00:56:20,160 --> 00:56:23,160
it's a different, different runtime

725
00:56:23,160 --> 00:56:26,160
that would produce these different results.

726
00:56:26,160 --> 00:56:28,160
And you have to go discover that roofline.

727
00:56:28,160 --> 00:56:30,160
Well, that's just one model.

728
00:56:30,160 --> 00:56:33,160
And this is just one configuration of a computer.

729
00:56:33,160 --> 00:56:36,160
Imagine all of the models being created around the world

730
00:56:36,160 --> 00:56:38,160
and all the different,

731
00:56:38,160 --> 00:56:41,160
configurations of systems that are going to be available.

732
00:56:44,160 --> 00:56:47,160
So now that you understand the basics,

733
00:56:47,160 --> 00:56:54,160
let's take a look at inference of Blackwell compared to Hopper.

734
00:56:54,160 --> 00:56:57,160
And this is the extraordinary thing.

735
00:56:57,160 --> 00:57:00,160
In one generation, because we created a system

736
00:57:00,160 --> 00:57:05,160
that's designed for trillion parameter generative AI,

737
00:57:05,160 --> 00:57:08,160
the inference capability of Blackwell is off the charts.

738
00:57:08,160 --> 00:57:13,160
And in fact, it is some 30 times Hopper.

739
00:57:13,160 --> 00:57:14,160
Yeah.

740
00:57:19,160 --> 00:57:21,160
For large language models,

741
00:57:21,160 --> 00:57:25,160
for large language models like ChatGPT and others like it,

742
00:57:25,160 --> 00:57:27,160
the blue line is Hopper.

743
00:57:27,160 --> 00:57:31,160
I gave you, imagine we didn't change the architecture of Hopper,

744
00:57:31,160 --> 00:57:33,160
we just made it a bigger chip.

745
00:57:33,160 --> 00:57:36,160
We just used the latest, you know, greatest,

746
00:57:36,160 --> 00:57:41,160
10 terabytes per second.

747
00:57:41,160 --> 00:57:43,160
We connected the two chips together.

748
00:57:43,160 --> 00:57:45,160
We got this giant 208 billion parameter chip.

749
00:57:45,160 --> 00:57:48,160
How would we have performed if nothing else changed?

750
00:57:48,160 --> 00:57:51,160
And it turns out, quite wonderfully,

751
00:57:51,160 --> 00:57:54,160
quite wonderfully, and that's the purple line,

752
00:57:54,160 --> 00:57:56,160
but not as great as it could be.

753
00:57:56,160 --> 00:57:59,160
And that's where the FP4 tensor core,

754
00:57:59,160 --> 00:58:01,160
the new transformer engine,

755
00:58:01,160 --> 00:58:04,160
and very importantly, the NVLink switch.

756
00:58:04,160 --> 00:58:05,160
And the reason for that,

757
00:58:05,160 --> 00:58:08,160
is because all these GPUs have to share the results,

758
00:58:08,160 --> 00:58:09,160
partial products.

759
00:58:09,160 --> 00:58:12,160
Whenever they do all to all, all gather,

760
00:58:12,160 --> 00:58:14,160
whenever they communicate with each other,

761
00:58:14,160 --> 00:58:20,160
that NVLink switch is communicating almost 10 times faster

762
00:58:20,160 --> 00:58:24,160
than what we could do in the past using the fastest networks.

763
00:58:24,160 --> 00:58:29,160
Okay, so Blackwell is going to be just an amazing system

764
00:58:29,160 --> 00:58:31,160
for generative AI.

765
00:58:31,160 --> 00:58:33,160
And in the future,

766
00:58:33,160 --> 00:58:34,160
in the future,

767
00:58:34,160 --> 00:58:35,160
in the future,

768
00:58:35,160 --> 00:58:38,160
data centers are going to be thought of,

769
00:58:38,160 --> 00:58:39,160
as I mentioned earlier,

770
00:58:39,160 --> 00:58:41,160
as an AI factory.

771
00:58:41,160 --> 00:58:46,160
An AI factory's goal in life is to generate revenues,

772
00:58:46,160 --> 00:58:49,160
generate, in this case,

773
00:58:49,160 --> 00:58:53,160
intelligence in this facility,

774
00:58:53,160 --> 00:58:56,160
not generating electricity as in AC generators,

775
00:58:56,160 --> 00:58:59,160
but of the last Industrial Revolution

776
00:58:59,160 --> 00:59:00,160
and this Industrial Revolution,

777
00:59:00,160 --> 00:59:02,160
the generation of intelligence.

778
00:59:02,160 --> 00:59:04,160
And so this ability is super,

779
00:59:04,160 --> 00:59:06,160
super important.

780
00:59:06,160 --> 00:59:09,160
The excitement of Blackwell is really off the charts.

781
00:59:09,160 --> 00:59:10,160
You know, when we first,

782
00:59:10,160 --> 00:59:13,160
when we first,

783
00:59:13,160 --> 00:59:15,160
you know, this is a year and a half ago,

784
00:59:15,160 --> 00:59:16,160
two years ago,

785
00:59:16,160 --> 00:59:17,160
I guess two years ago,

786
00:59:17,160 --> 00:59:20,160
when we first started to go to market with Hopper,

787
00:59:20,160 --> 00:59:24,160
you know, we had the benefit of two CSPs

788
00:59:24,160 --> 00:59:26,160
joined us in a lunch.

789
00:59:26,160 --> 00:59:28,160
And we were, you know, delighted.

790
00:59:28,160 --> 00:59:32,160
And so we had two customers.

791
00:59:32,160 --> 00:59:34,160
We have more now.

792
00:59:47,160 --> 00:59:50,160
Unbelievable excitement for Blackwell.

793
00:59:50,160 --> 00:59:51,160
Unbelievable excitement.

794
00:59:51,160 --> 00:59:53,160
And there's a whole bunch of different configurations.

795
00:59:53,160 --> 00:59:56,160
Of course, I showed you the configurations

796
00:59:56,160 --> 00:59:58,160
that slide into the Hopper form factor,

797
00:59:58,160 --> 01:00:00,160
so that's easy to upgrade.

798
01:00:00,160 --> 01:00:01,160
I showed you examples

799
01:00:01,160 --> 01:00:02,160
that are liquid-cooled,

800
01:00:02,160 --> 01:00:04,160
that are the extreme versions of it.

801
01:00:04,160 --> 01:00:08,160
One entire rack that's connected by MVLink-72.

802
01:00:08,160 --> 01:00:10,160
We're going to,

803
01:00:10,160 --> 01:00:13,160
Blackwell is going to be ramping

804
01:00:13,160 --> 01:00:16,160
to the world's AI companies,

805
01:00:16,160 --> 01:00:18,160
of which there are so many now,

806
01:00:18,160 --> 01:00:20,160
doing amazing work in different modalities.

807
01:00:20,160 --> 01:00:24,160
The CSPs, every CSP is geared up.

808
01:00:24,160 --> 01:00:27,160
All the OEMs and ODMs,

809
01:00:27,160 --> 01:00:29,160
regional clouds,

810
01:00:29,160 --> 01:00:30,160
sovereign AI,

811
01:00:30,160 --> 01:00:33,160
and telcos all over the world

812
01:00:33,160 --> 01:00:35,160
are signing up to launch with Blackwell.

813
01:00:43,160 --> 01:00:47,160
Blackwell would be the most successful product launch

814
01:00:47,160 --> 01:00:49,160
in our history.

815
01:00:49,160 --> 01:00:51,160
And so I can't wait to see that.

816
01:00:51,160 --> 01:00:53,160
I want to thank some partners

817
01:00:53,160 --> 01:00:55,160
that are joining us in this.

818
01:00:55,160 --> 01:00:57,160
AWS is gearing up for Blackwell.

819
01:00:57,160 --> 01:00:59,160
They're going to build the first

820
01:00:59,160 --> 01:01:01,160
GPU with secure AI.

821
01:01:01,160 --> 01:01:05,160
They're building out a 222-exaflops system.

822
01:01:05,160 --> 01:01:07,160
You know, just now when we animated,

823
01:01:07,160 --> 01:01:09,160
just now the digital twin,

824
01:01:09,160 --> 01:01:12,160
if you saw all of those clusters coming down.

825
01:01:12,160 --> 01:01:15,160
By the way, that is not just art.

826
01:01:15,160 --> 01:01:18,160
That is a digital twin of what we're building.

827
01:01:18,160 --> 01:01:20,160
That's how big it's going to be.

828
01:01:20,160 --> 01:01:21,160
Besides infrastructure,

829
01:01:21,160 --> 01:01:23,160
we're doing a lot of things together with AWS.

830
01:01:23,160 --> 01:01:26,160
We're CUDA-accelerating SageMaker AI.

831
01:01:26,160 --> 01:01:28,160
We're CUDA-accelerating Bedrock AI.

832
01:01:28,160 --> 01:01:31,160
Amazon Robotics is working with us

833
01:01:31,160 --> 01:01:34,160
using NVIDIA Omniverse and Isaac Sim.

834
01:01:34,160 --> 01:01:38,160
AWS Health has NVIDIA Health integrated into it.

835
01:01:38,160 --> 01:01:43,160
So AWS has really leaned into accelerated computing.

836
01:01:43,160 --> 01:01:45,160
Google is gearing up for Blackwell.

837
01:01:45,160 --> 01:01:49,160
GCP already has A100s, H100s, T4s, L4s,

838
01:01:49,160 --> 01:01:52,160
a whole fleet of NVIDIA CUDA GPUs.

839
01:01:52,160 --> 01:01:55,160
And they recently announced the Gemma model

840
01:01:55,160 --> 01:01:57,160
that runs across all of it.

841
01:01:57,160 --> 01:02:00,160
We're working to optimize and accelerate

842
01:02:00,160 --> 01:02:02,160
every aspect of GCP.

843
01:02:02,160 --> 01:02:04,160
We're accelerating Dataproc for data processing,

844
01:02:04,160 --> 01:02:06,160
their data processing engine,

845
01:02:06,160 --> 01:02:09,160
JAX, XLA, Vertex AI,

846
01:02:09,160 --> 01:02:11,160
and MuJoCo for robotics.

847
01:02:11,160 --> 01:02:13,160
So we're working with Google and GCP

848
01:02:13,160 --> 01:02:15,160
across a whole bunch of initiatives.

849
01:02:15,160 --> 01:02:17,160
Oracle is gearing up for Blackwell.

850
01:02:17,160 --> 01:02:19,160
Oracle is a great partner of ours

851
01:02:19,160 --> 01:02:21,160
for NVIDIA DGX Cloud.

852
01:02:21,160 --> 01:02:23,160
And we're also working together to accelerate

853
01:02:23,160 --> 01:02:25,160
something that's really important to a lot of companies,

854
01:02:25,160 --> 01:02:27,160
Oracle Database.

855
01:02:27,160 --> 01:02:30,160
Microsoft is accelerating,

856
01:02:30,160 --> 01:02:32,160
and Microsoft is gearing up for Blackwell.

857
01:02:32,160 --> 01:02:35,160
Microsoft NVIDIA has a wide-ranging partnership.

858
01:02:35,160 --> 01:02:36,160
We're accelerating CUDA,

859
01:02:36,160 --> 01:02:38,160
accelerating all kinds of services.

860
01:02:38,160 --> 01:02:40,160
When you chat, obviously,

861
01:02:40,160 --> 01:02:43,160
and AI services that are in Microsoft Azure,

862
01:02:43,160 --> 01:02:45,160
it's very, very likely NVIDIA is in the back

863
01:02:45,160 --> 01:02:48,160
doing the inference and the token generation.

864
01:02:48,160 --> 01:02:52,160
They built the largest NVIDIA InfiniBand supercomputer,

865
01:02:52,160 --> 01:02:54,160
basically a digital twin of ours,

866
01:02:54,160 --> 01:02:56,160
or a physical twin of ours.

867
01:02:56,160 --> 01:02:59,160
We're bringing the NVIDIA ecosystem to Azure.

868
01:02:59,160 --> 01:03:01,160
NVIDIA DGX Cloud to Azure.

869
01:03:01,160 --> 01:03:04,160
NVIDIA Omniverse is now hosted in Azure.

870
01:03:04,160 --> 01:03:06,160
NVIDIA Healthcare is in Azure.

871
01:03:06,160 --> 01:03:08,160
And all of it is deeply integrated

872
01:03:08,160 --> 01:03:11,160
and deeply connected with Microsoft Fabric.

873
01:03:11,160 --> 01:03:14,160
The whole industry is gearing up for Blackwell.

874
01:03:14,160 --> 01:03:16,160
This is what I'm about to show you.

875
01:03:16,160 --> 01:03:22,160
Most of the scenes that you've seen so far of Blackwell

876
01:03:22,160 --> 01:03:25,160
are the full fidelity design

877
01:03:25,160 --> 01:03:27,160
of Blackwell.

878
01:03:27,160 --> 01:03:30,160
Everything in our company has a digital twin.

879
01:03:30,160 --> 01:03:35,160
And, in fact, this digital twin idea is really spreading,

880
01:03:35,160 --> 01:03:39,160
and it helps companies build very complicated things

881
01:03:39,160 --> 01:03:41,160
perfectly the first time.

882
01:03:41,160 --> 01:03:46,160
And what could be more exciting than creating a digital twin

883
01:03:46,160 --> 01:03:50,160
to build a computer that was built in a digital twin?

884
01:03:50,160 --> 01:03:53,160
And so let me show you what Wishtron is doing.

885
01:03:55,160 --> 01:03:58,160
To meet the demand for NVIDIA accelerated computing,

886
01:03:58,160 --> 01:04:01,160
Wishtron, one of our leading manufacturing partners,

887
01:04:01,160 --> 01:04:05,160
is building digital twins of NVIDIA DGX and HGX factories

888
01:04:05,160 --> 01:04:11,160
using custom software developed with Omniverse SDKs and APIs.

889
01:04:11,160 --> 01:04:14,160
For their newest factory, Wishtron started with a digital twin

890
01:04:14,160 --> 01:04:16,160
to virtually integrate their multi-CAD

891
01:04:16,160 --> 01:04:20,160
and process simulation data into a unified view.

892
01:04:20,160 --> 01:04:22,160
Testing and optimizing layouts

893
01:04:22,160 --> 01:04:24,160
in this physically accurate digital environment

894
01:04:24,160 --> 01:04:28,160
increased worker efficiency by 51%.

895
01:04:28,160 --> 01:04:31,160
During construction, the Omniverse digital twin

896
01:04:31,160 --> 01:04:33,160
was used to verify that the physical build

897
01:04:33,160 --> 01:04:35,160
matched the digital plans.

898
01:04:35,160 --> 01:04:37,160
Identifying any discrepancies early

899
01:04:37,160 --> 01:04:40,160
has helped avoid costly change orders.

900
01:04:40,160 --> 01:04:42,160
And the results have been impressive.

901
01:04:42,160 --> 01:04:45,160
Using a digital twin helped bring Wishtron's factory online

902
01:04:45,160 --> 01:04:49,160
in half the time, just two and a half months instead of five.

903
01:04:49,160 --> 01:04:51,160
In operation, the Omniverse digital twin

904
01:04:51,160 --> 01:04:53,160
helps Wishtron rapidly test new layouts

905
01:04:53,160 --> 01:04:55,160
to accommodate new processes

906
01:04:55,160 --> 01:04:58,160
or improve operations in the existing space

907
01:04:58,160 --> 01:05:00,160
and monitor real-time operations

908
01:05:00,160 --> 01:05:05,160
using live IoT data from every machine on the production line,

909
01:05:05,160 --> 01:05:07,160
which ultimately enabled Wishtron

910
01:05:07,160 --> 01:05:10,160
to reduce end-to-end cycle times by 50%

911
01:05:10,160 --> 01:05:13,160
and defect rates by 40%.

912
01:05:13,160 --> 01:05:15,160
With NVIDIA AI and Omniverse,

913
01:05:15,160 --> 01:05:17,160
NVIDIA's global ecosystem of partners

914
01:05:17,160 --> 01:05:19,160
are building a new era of accelerated,

915
01:05:19,160 --> 01:05:22,160
AI-enabled digitalization.

916
01:05:23,160 --> 01:05:25,160
Let's talk about the future.

917
01:05:34,160 --> 01:05:36,160
That's how we are,

918
01:05:36,160 --> 01:05:38,160
that's the way it's gonna be in the future.

919
01:05:38,160 --> 01:05:41,160
We're manufacturing everything digitally first,

920
01:05:41,160 --> 01:05:43,160
and then we'll manufacture it physically.

921
01:05:43,160 --> 01:05:45,160
People ask me, how did it start?

922
01:05:45,160 --> 01:05:48,160
What got you guys so excited?

923
01:05:48,160 --> 01:05:51,160
What was it that you saw

924
01:05:51,160 --> 01:05:53,500
on this

925
01:05:53,500 --> 01:05:54,940
incredible idea

926
01:05:54,940 --> 01:05:56,900
and it's this.

927
01:06:01,280 --> 01:06:02,140
Hang on a second.

928
01:06:07,700 --> 01:06:09,420
Guys, that was going to be such a moment.

929
01:06:12,860 --> 01:06:14,560
That's what happens when you don't rehearse.

930
01:06:20,260 --> 01:06:20,820
This

931
01:06:20,820 --> 01:06:22,260
as you know

932
01:06:22,260 --> 01:06:24,180
was first contact.

933
01:06:25,220 --> 01:06:26,500
2012, AlexNet.

934
01:06:27,360 --> 01:06:28,880
You put a cat

935
01:06:28,880 --> 01:06:30,440
into this computer

936
01:06:30,440 --> 01:06:32,440
and it comes out and it says

937
01:06:32,440 --> 01:06:33,300
cat.

938
01:06:36,440 --> 01:06:37,820
And we said

939
01:06:37,820 --> 01:06:40,760
oh my god, this is going to change everything.

940
01:06:43,300 --> 01:06:44,220
You take

941
01:06:44,220 --> 01:06:45,600
one million numbers

942
01:06:45,600 --> 01:06:48,900
across three channels

943
01:06:48,900 --> 01:06:49,360
RGB

944
01:06:49,360 --> 01:06:52,520
these numbers make no sense to anybody

945
01:06:52,520 --> 01:06:54,540
you put it into this

946
01:06:54,540 --> 01:06:55,260
software

947
01:06:55,260 --> 01:06:58,580
and it compresses, it dimensionally

948
01:06:58,580 --> 01:06:59,340
reduces it

949
01:06:59,340 --> 01:07:02,200
from a million dimensions

950
01:07:02,200 --> 01:07:03,340
a million dimensions

951
01:07:03,340 --> 01:07:05,760
it turns it into three letters

952
01:07:05,760 --> 01:07:08,260
one vector, one number

953
01:07:08,260 --> 01:07:11,800
and it's generalized

954
01:07:11,800 --> 01:07:14,060
you could have the cat

955
01:07:14,060 --> 01:07:15,980
be different cats

956
01:07:15,980 --> 01:07:19,300
and you could have it be

957
01:07:19,300 --> 01:07:19,340
different cats

958
01:07:19,340 --> 01:07:19,360
and you could have it be different cats

959
01:07:19,360 --> 01:07:20,280
you could have the cat

960
01:07:20,280 --> 01:07:20,380
and the front of the cat

961
01:07:20,380 --> 01:07:20,780
the front of the cat

962
01:07:20,800 --> 01:07:21,340
and the back of the cat

963
01:07:21,340 --> 01:07:22,420
and the back of the cat

964
01:07:23,700 --> 01:07:25,120
and you look at this thing, you say

965
01:07:25,120 --> 01:07:27,040
unbelievable.

966
01:07:27,040 --> 01:07:27,740
You mean any cats?

967
01:07:27,740 --> 01:07:31,700
Yeah, any cat

968
01:07:31,700 --> 01:07:34,440
It was able to recognize all these cats

969
01:07:34,440 --> 01:07:35,240
and we realized

970
01:07:35,240 --> 01:07:36,540
how it did it

971
01:07:36,540 --> 01:07:37,740
systematically

972
01:07:37,740 --> 01:07:38,720
structurally

973
01:07:38,720 --> 01:07:41,980
its scaleable

974
01:07:41,980 --> 01:07:43,560
How big can you make it?

975
01:07:43,560 --> 01:07:44,700
Well, how big

976
01:07:44,700 --> 01:07:46,360
do you want to make it?

977
01:07:46,360 --> 01:07:48,060
And so we imagine

978
01:07:48,060 --> 01:07:48,480
that this is a completely

979
01:07:48,480 --> 01:07:49,100
ever-lasting industrial revolution.

980
01:07:49,100 --> 01:07:55,760
new way of writing software and now today as you know you can have you type

981
01:07:55,760 --> 01:08:06,060
in the word CAT and what comes out is a cat it went the other way am i right

982
01:08:06,060 --> 01:08:12,280
unbelievable how is it possible that's right how is it possible you took three

983
01:08:12,280 --> 01:08:19,440
letters and you generated a million pixels from it and it made sense well

984
01:08:19,440 --> 01:08:23,720
that's the miracle and here we are just literally 10 years

985
01:08:23,720 --> 01:08:29,680
later 10 years later where we recognize texts we recognize images we recognize

986
01:08:29,680 --> 01:08:35,520
videos and sounds and images not only do we recognize them we understand their

987
01:08:35,520 --> 01:08:39,140
meaning we understand the meaning of the text that's the reason why I can chat

988
01:08:39,140 --> 01:08:41,800
with you it can summarize for you it

989
01:08:41,800 --> 01:08:42,260
understands what you read I can read it that's account for lung audio I can get

990
01:08:42,260 --> 01:08:48,140
understands the text. It understood, not just recognizes the English, it
understood the English.

991
01:08:48,140 --> 01:08:53,940
It doesn't just recognize the pixels, it understood the pixels. And you can even
condition it

992
01:08:53,940 --> 01:08:58,980
between two modalities. You can have language condition image and generate all
kinds of

993
01:08:58,980 --> 01:09:04,140
interesting things. Well, if you can understand these things, what else can you
understand

994
01:09:04,140 --> 01:09:09,500
that you've digitized? The reason why we started with text and, you know, images
is because

995
01:09:09,500 --> 01:09:13,320
we digitized those. But what else have we digitized? Well, it turns out we
digitized

996
01:09:13,320 --> 01:09:21,520
a lot of things. Proteins and genes and brain waves. Anything you can digitize,
so long

997
01:09:21,520 --> 01:09:25,120
as there's structure, we can probably learn some patterns from it. And if we can
learn

998
01:09:25,120 --> 01:09:28,780
the patterns from it, we can understand its meaning. If we can understand its
meaning,

999
01:09:29,460 --> 01:09:34,920
we might be able to generate it as well. And so, therefore, the generative AI
revolution

1000
01:09:34,920 --> 01:09:39,480
is here. Well, what else can we generate? What else can we learn? Well, one of
the things

1001
01:09:39,480 --> 01:09:47,160
that we would love to learn, we would love to learn, is we would love to learn
climate.

1002
01:09:47,160 --> 01:09:55,740
We would love to learn extreme weather. We would love to learn how we can
predict future

1003
01:09:55,740 --> 01:10:02,700
weather at regional scales, at sufficiently high resolution, such that we can
keep people

1004
01:10:02,700 --> 01:10:09,000
out of harm's way before harm comes. Extreme weather cost the world $150
billion, surely

1005
01:10:09,000 --> 01:10:09,460
more than that. But we can do it. We can do it. We can do it. We can do it. We
can do it.

1006
01:10:09,460 --> 01:10:09,620
We can do it. We can do it. We can do it. We can do it. We can do it. We can do
it. We can do it.

1007
01:10:09,620 --> 01:10:14,740
And that, it's not evenly distributed. $150 billion is concentrated in some
parts of the

1008
01:10:14,740 --> 01:10:19,300
world, and of course, to some people of the world. We need to adapt, and we need
to know

1009
01:10:19,300 --> 01:10:25,460
what's coming. And so, we're creating Earth 2, a digital twin of the Earth for
predicting weather.

1010
01:10:25,460 --> 01:10:32,680
And we've made an extraordinary invention called CoreDiv, the ability to use
generative AI

1011
01:10:32,680 --> 01:10:36,760
to predict weather at extremely high resolution. Let's take a look.

1012
01:10:39,460 --> 01:10:44,740
As the Earth's climate changes, AI-powered weather forecasting is allowing us to
more accurately

1013
01:10:44,740 --> 01:10:50,740
predict and track severe storms, like Super Typhoon Chanthu, which caused
widespread damage in Taiwan

1014
01:10:50,740 --> 01:10:56,580
and the surrounding region in 2021. Current AI forecast models can accurately
predict the track

1015
01:10:56,580 --> 01:11:02,180
of storms, but they are limited to 25-kilometer resolution, which can miss
important details.

1016
01:11:02,900 --> 01:11:08,980
NVIDIA's CoreDiv is a revolutionary new generative AI model trained on high-
resolution radar-assimilated

1017
01:11:08,980 --> 01:11:16,580
WARF weather forecasts and ERA5 reanalysis data. Using CoreDiv, extreme events
like Chanthu can

1018
01:11:16,580 --> 01:11:22,820
be super-resolved from 25-kilometer to 2-kilometer resolution, with 1,000 times
the speed and 3,000

1019
01:11:22,820 --> 01:11:27,860
times the energy efficiency of conventional weather models. By combining the
speed and

1020
01:11:27,860 --> 01:11:33,700
accuracy of NVIDIA's weather forecasting model, ForecastNet, and generative AI
models like CoreDiv,

1021
01:11:33,700 --> 01:11:38,740
we can explore hundreds or even thousands of kilometer-scale regional weather
forecasts,

1022
01:11:38,740 --> 01:11:43,460
to provide a clear picture of the best, worst, and most likely impacts of a
storm.

1023
01:11:44,260 --> 01:11:48,260
This wealth of information can help minimize loss of life and property damage.

1024
01:11:48,900 --> 01:11:54,260
Today, CoreDiv is optimized for Taiwan, but soon, generative supersampling will
be available

1025
01:11:54,260 --> 01:11:58,580
as part of the NVIDIA Earth-2 inference service for many regions across the
globe.

1026
01:12:08,740 --> 01:12:15,380
The weather company has to trust the source of global weather prediction. We are
working together

1027
01:12:15,380 --> 01:12:21,620
to accelerate their weather simulation, first principled base of simulation.
However, they're

1028
01:12:21,620 --> 01:12:28,180
also going to integrate Earth-2 CoreDiv so that they can help businesses and
countries do regional

1029
01:12:28,180 --> 01:12:33,220
high-resolution weather prediction. And so, if you have some weather prediction
you'd like to know,

1030
01:12:33,220 --> 01:12:38,580
like to do, reach out to the weather company. Really exciting, really exciting
work. NVIDIA has

1031
01:12:38,580 --> 01:12:43,620
healthcare. Something we started 15 years ago. We're super, super excited about
this. This is

1032
01:12:43,620 --> 01:12:49,460
an area where we're very, very proud. Whether it's medical imaging or gene
sequencing or computational

1033
01:12:49,460 --> 01:12:56,820
chemistry, it is very likely that NVIDIA is the computation behind it. We've
done so much work in

1034
01:12:56,820 --> 01:13:04,020
this area. Today, we're announcing that we're going to do something really,
really cool. Imagine

1035
01:13:04,900 --> 01:13:07,460
all of these AI models that are being used

1036
01:13:08,580 --> 01:13:15,380
to generate images and audio, but instead of images and audio, because it
understood images

1037
01:13:15,380 --> 01:13:21,860
and audio, all the digitization that we've done for genes and proteins and amino
acids,

1038
01:13:21,860 --> 01:13:28,580
that digitization capability is now passed through machine learning so that we
understand

1039
01:13:28,580 --> 01:13:32,980
the language of life. The ability to understand the language of life,

1040
01:13:32,980 --> 01:13:38,580
of course, we saw the first evidence of it with AlphaFold. This is really quite
a

1041
01:13:38,580 --> 01:13:44,100
extraordinary thing. After decades of painstaking work, the world had only
digitized

1042
01:13:45,300 --> 01:13:53,540
and reconstructed using cryo-electron microscopy or x-ray crystallography. These
different

1043
01:13:53,540 --> 01:13:59,700
techniques painstakingly reconstructed the protein, 200,000 of them, in just,
what is it,

1044
01:13:59,700 --> 01:14:08,020
less than a year or so? AlphaFold has reconstructed 200 million proteins.
Basically, every protein,

1045
01:14:08,580 --> 01:14:13,860
every living thing that's ever been sequenced. This is completely revolutionary.
Well,

1046
01:14:13,860 --> 01:14:19,940
those models are incredibly hard to use, incredibly hard for people to build,
and so what we're going

1047
01:14:19,940 --> 01:14:24,980
to do is we're going to build them. We're going to build them for the
researchers around the world,

1048
01:14:24,980 --> 01:14:28,500
and it won't be the only one. There'll be many other models that we create,

1049
01:14:28,500 --> 01:14:30,500
and so let me show you what we're going to do with it.

1050
01:14:35,700 --> 01:14:38,420
Virtual Screening for New Medicines is a computational

1051
01:14:38,580 --> 01:14:43,700
and tractable problem. Existing techniques can only scan billions of compounds

1052
01:14:43,700 --> 01:14:48,820
and require days on thousands of standard compute nodes to identify new drug
candidates.

1053
01:14:49,860 --> 01:14:55,700
NVIDIA BioNemo NIMS enable a new generative screening paradigm. Using NIMS for
protein

1054
01:14:55,700 --> 01:15:01,540
structure prediction with AlphaFold, molecule generation with MolMIM, and
docking with DiffDock,

1055
01:15:02,100 --> 01:15:08,020
we can now generate and screen candidate molecules in a matter of minutes.
MolMIM can connect to

1056
01:15:08,020 --> 01:15:13,140
custom applications to steer the generative process, iteratively optimizing for
desired

1057
01:15:13,140 --> 01:15:19,220
properties. These applications can be defined with BioNemo micro services or
built from scratch.

1058
01:15:20,180 --> 01:15:26,340
Here, a physics-based simulation optimizes for a molecule's ability to bind to a
target protein

1059
01:15:26,340 --> 01:15:30,100
while optimizing for other favorable molecular properties in parallel.

1060
01:15:30,900 --> 01:15:36,260
MolMIM generates high quality, drug-like molecules that bind to the target and
are synthesizable,

1061
01:15:36,260 --> 01:15:41,260
translating to a higher probability of developing successful medicines faster.

1062
01:15:42,260 --> 01:15:46,260
BioNemo is enabling a new paradigm in drug discovery with NIMS,

1063
01:15:46,260 --> 01:15:52,260
providing on-demand microservices that can be combined to build powerful drug
discovery workflows

1064
01:15:52,260 --> 01:15:57,260
like de novo protein design or guided molecule generation for virtual screening.

1065
01:15:58,260 --> 01:16:03,260
BioNemo NIMS are helping researchers and developers reinvent computational drug
design.

1066
01:16:06,260 --> 01:16:10,260
�

1067
01:16:12,220 --> 01:16:16,260
NVIDIA and MulMIM, MulMIM, CORE, DIF, there are a whole bunch of other models.

1068
01:16:17,260 --> 01:16:26,260
Computer vision models, robotics models, and even, of course, some really,
really fantastic open-source language models.

1069
01:16:27,260 --> 01:16:32,260
These models are groundbreaking. However, it is hard for companies to use.

1070
01:16:33,260 --> 01:16:36,260
How would you use it? How would you bring it into your company and integrate it
to your work environment?

1071
01:16:36,260 --> 01:16:41,360
flow how would you package it up and run it remember earlier I just said that

1072
01:16:41,360 --> 01:16:45,860
inference is an extraordinary computation problem how would you do the

1073
01:16:45,860 --> 01:16:50,560
optimization for each and every one of these models and put together the

1074
01:16:50,560 --> 01:16:54,920
computing stack necessary to run that supercomputer so that you can run these

1075
01:16:54,920 --> 01:17:00,560
models in your company and so we have a great idea we're gonna invent a new way

1076
01:17:00,560 --> 01:17:08,800
and invent a new way for you to receive and operate software this software

1077
01:17:08,800 --> 01:17:15,980
comes basically in a digital box we call it a container and we call it the

1078
01:17:15,980 --> 01:17:21,860
NVIDIA inference micro service a NIM and only to explain to you what it is a

1079
01:17:21,860 --> 01:17:28,520
NIM it's a pre-trained model so it's pretty clever and it is packaged and

1080
01:17:28,520 --> 01:17:30,540
optimized to run across any

1081
01:17:30,540 --> 01:17:35,480
NVIDIA's install base which is very very large what's inside it is

1082
01:17:35,480 --> 01:17:39,540
incredible you have all these pre-trained state-of-the-art open source

1083
01:17:39,540 --> 01:17:43,200
models they could be open source they could be from one of our partners it

1084
01:17:43,200 --> 01:17:47,760
could be created by us like NVIDIA moment it is packaged up with all of its

1085
01:17:47,760 --> 01:17:53,940
dependencies so CUDA the right version cu DNN the right version tensor RT LLM

1086
01:17:53,940 --> 01:17:58,860
distributed across the multiple GPUs try an inference server all completely

1087
01:17:58,860 --> 01:18:00,160
packaged together

1088
01:18:00,540 --> 01:18:05,640
it's optimized depending on whether you have a single GPU multi GPU or multi
node

1089
01:18:05,640 --> 01:18:10,620
of GPUs it's optimized for that and it's connected up with API's that are simple

1090
01:18:10,620 --> 01:18:18,000
to use now this think about what an AI API is an AI API is an interface that

1091
01:18:18,000 --> 01:18:23,040
you just talk to and so this is a piece of software in the future that has a

1092
01:18:23,040 --> 01:18:29,340
really simple API and that API is called human and these packages incredible

1093
01:18:29,340 --> 01:18:30,400
bodies of software

1094
01:18:30,400 --> 01:18:36,340
will be optimized and packaged and we'll put it on a website and you can

1095
01:18:36,340 --> 01:18:41,360
download it you could take it with you you can run it in any cloud you can run

1096
01:18:41,360 --> 01:18:44,900
it in your own data center you weren't running workstations of a fit and all

1097
01:18:44,900 --> 01:18:49,960
you have to do is come to AI dot NVIDIA comm we call it NVIDIA inference

1098
01:18:49,960 --> 01:18:56,340
microservice but inside the company we all call it NIMS okay

1099
01:18:56,340 --> 01:18:58,360
you

1100
01:19:00,400 --> 01:19:06,300
just imagine you know one of some someday there there's going to be one of

1101
01:19:06,300 --> 01:19:11,420
these chat bots and these chat bots is gonna just be in the NIM and you know

1102
01:19:11,420 --> 01:19:15,200
you'll like you'll assemble a whole bunch of chat bots and that's the way

1103
01:19:15,200 --> 01:19:18,800
software is going to give you be built someday how do we build software in the

1104
01:19:18,800 --> 01:19:23,340
future it is unlikely that you'll write it from scratch or write a whole bunch

1105
01:19:23,340 --> 01:19:28,080
of Python code or anything like that it is very likely that you assemble a team

1106
01:19:28,080 --> 01:19:29,900
of AI's

1107
01:19:30,400 --> 01:19:36,220
going to be a super AI that you use that takes the mission that you give it and
breaks it

1108
01:19:36,220 --> 01:19:38,980
down into an execution plan.

1109
01:19:38,980 --> 01:19:42,900
Some of that execution plan could be handed off to another NIM.

1110
01:19:42,900 --> 01:19:47,380
That NIM would maybe understand SAP.

1111
01:19:47,380 --> 01:19:49,540
The language of SAP is ABAP.

1112
01:19:49,540 --> 01:19:55,260
It might understand ServiceNow and go retrieve some information from their
platforms.

1113
01:19:55,260 --> 01:19:59,960
It might then hand that result to another NIM who goes off and does some
calculation

1114
01:19:59,960 --> 01:20:00,960
on it.

1115
01:20:00,960 --> 01:20:06,920
Maybe it's an optimization software, a combinatorial optimization algorithm.

1116
01:20:06,920 --> 01:20:11,060
Maybe it's just some basic calculator.

1117
01:20:11,060 --> 01:20:15,240
Maybe it's Pandas to do some numerical analysis on it.

1118
01:20:15,240 --> 01:20:20,800
And then it comes back with its answer, and it gets combined with everybody
else's.

1119
01:20:20,800 --> 01:20:25,240
And because it's been presented with, this is what the right answer should look
like,

1120
01:20:25,240 --> 01:20:30,660
it knows what right answers to produce, and it presents it to you.

1121
01:20:30,660 --> 01:20:34,700
We can get a report every single day at, you know, top of the hour that has
something to

1122
01:20:34,700 --> 01:20:40,080
do with a build plan or some forecast or some customer alert or some bugs
database or whatever

1123
01:20:40,080 --> 01:20:44,460
it happens to be, and we could assemble it using all these NIMs.

1124
01:20:44,460 --> 01:20:49,760
And because these NIMs have been packaged up and ready to work on your systems,
so long

1125
01:20:49,760 --> 01:20:54,980
as you have NVIDIA GPUs in your data center or in the cloud, these NIMs will
work together.

1126
01:20:54,980 --> 01:20:57,660
Together as a team and do amazing things.

1127
01:20:57,660 --> 01:21:02,800
And so we decided, this is such a great idea, we're going to go do that.

1128
01:21:02,800 --> 01:21:05,500
And so NVIDIA has NIMs running all over the company.

1129
01:21:05,500 --> 01:21:08,500
We have chatbots being created all over the place.

1130
01:21:08,500 --> 01:21:13,720
And one of the most important chatbots, of course, is a chip designer chatbot.

1131
01:21:13,720 --> 01:21:14,800
You might not be surprised.

1132
01:21:14,800 --> 01:21:17,100
We care a lot about building chips.

1133
01:21:17,100 --> 01:21:24,980
And so we want to build chatbots, AI co-pilots that are co-designers with our
engineers.

1134
01:21:24,980 --> 01:21:27,080
And so this is the way we did it.

1135
01:21:27,080 --> 01:21:30,400
So we got ourselves a Lama 2.

1136
01:21:30,400 --> 01:21:34,060
This is a 70B, and it's packaged up in a NIM.

1137
01:21:34,060 --> 01:21:39,160
And we asked it, what is a CTL?

1138
01:21:39,160 --> 01:21:45,720
It turns out CTL is an internal program, and it has an internal proprietary
language.

1139
01:21:45,720 --> 01:21:48,680
But it thought the CTL was a combinatorial timing logic.

1140
01:21:48,680 --> 01:21:51,760
And so it describes conventional knowledge of CTL.

1141
01:21:51,760 --> 01:21:54,560
But that's not very useful to us.

1142
01:21:54,560 --> 01:21:58,440
And so we gave it a whole bunch of new examples.

1143
01:21:58,440 --> 01:22:02,320
This is no different than onboarding an employee.

1144
01:22:02,320 --> 01:22:04,380
We say, thanks for that answer.

1145
01:22:04,380 --> 01:22:07,120
It's completely wrong.

1146
01:22:07,120 --> 01:22:10,980
And then we present to them, this is what a CTL is.

1147
01:22:10,980 --> 01:22:13,940
And so this is what a CTL is at NVIDIA.

1148
01:22:13,940 --> 01:22:20,580
And the CTL, as you can see, CTL stands for Compute Trace Library, which makes
sense.

1149
01:22:20,580 --> 01:22:22,980
We're tracing compute cycles all the time.

1150
01:22:22,980 --> 01:22:23,740
And it wrote the program.

1151
01:22:23,740 --> 01:22:24,460
And so we gave it a whole bunch of new examples.

1152
01:22:24,460 --> 01:22:24,540
And so this is what a CTL is at NVIDIA.

1153
01:22:24,540 --> 01:22:25,040
And so this is what a CTL is at NVIDIA.

1154
01:22:25,040 --> 01:22:25,660
Isn't that amazing?

1155
01:22:25,660 --> 01:22:35,280
And so the productivity of our chip designers can go up.

1156
01:22:35,520 --> 01:22:36,740
This is what you can do with a NIM.

1157
01:22:36,880 --> 01:22:38,740
First thing you can do with it is customize it.

1158
01:22:39,000 --> 01:22:46,940
We have a service called Nemo Microservice that helps you curate the data,
preparing the data so that you can teach this onboard this AI.

1159
01:22:47,460 --> 01:22:50,560
You fine-tune them, and then you guardrail it.

1160
01:22:50,560 --> 01:22:54,440
You can even evaluate the answer, evaluate its performance against others.

1161
01:22:54,440 --> 01:22:55,420
This is what we do.

1162
01:22:55,420 --> 01:22:58,320
And this could help you with other examples.

1163
01:22:58,320 --> 01:23:01,520
And so that's called the Nemo Microservice.

1164
01:23:01,520 --> 01:23:06,060
Now, the thing that's emerging here is this – there are three elements, three
pillars of what we're doing.

1165
01:23:06,060 --> 01:23:14,600
The first pillar is, of course, inventing the technology for AI models and
running AI models and packaging it up for you.

1166
01:23:14,600 --> 01:23:18,620
The second is to create tools to help you modify it.

1167
01:23:18,620 --> 01:23:20,300
First is having the AI technology.

1168
01:23:20,300 --> 01:23:21,740
Second is to help you modify it.

1169
01:23:21,740 --> 01:23:24,340
And third is infrastructure for you to fine-tune it.

1170
01:23:24,340 --> 01:23:29,460
you could deploy it on our infrastructure called dgx cloud or you can employ
deploy it on-prem you

1171
01:23:29,460 --> 01:23:35,060
can deploy it anywhere you like once you develop it it's yours to take anywhere
and so we are

1172
01:23:35,060 --> 01:23:44,100
effectively an ai foundry we will do for you and the industry on ai what tsmc
does for us building

1173
01:23:44,100 --> 01:23:50,340
chips and so we go to it with our go to tsmc with our big ideas they manufacture
and we take it with

1174
01:23:50,340 --> 01:23:57,860
us and so exactly the same thing here ai foundry and the three pillars are the
nims nemo microservice

1175
01:23:57,860 --> 01:24:03,620
and dgx cloud the other thing that you could teach the nim to do is to
understand your proprietary

1176
01:24:03,620 --> 01:24:08,980
information remember inside our company the vast majority of our data is not in
the cloud it's

1177
01:24:08,980 --> 01:24:15,300
inside our company it's been sitting there you know being used all the time and
and gosh it's

1178
01:24:15,300 --> 01:24:19,700
it's basically nvidia's intelligence we would like to take that data

1179
01:24:20,660 --> 01:24:24,900
learn its meaning like we learned the meaning of almost anything else that we
just talked about

1180
01:24:24,900 --> 01:24:31,460
learn its meaning and then re-index that knowledge into a new type of database
called a vector

1181
01:24:31,460 --> 01:24:37,460
database and so you essentially take structured data or unstructured data you
learn its meaning

1182
01:24:37,940 --> 01:24:45,060
you encode its meaning so now this becomes an ai database and that ai database
in the future once

1183
01:24:45,060 --> 01:24:49,220
you create it you can talk to it and so let me give you an example of what you
could do so suppose

1184
01:24:49,220 --> 01:24:50,260
you create an ai database

1185
01:24:50,340 --> 01:24:55,540
You've got a whole bunch of multi-modality data, and one good example of that is
PDF.

1186
01:24:56,080 --> 01:25:00,880
So you take the PDF, you take all of your PDFs, all your favorite, you know,

1187
01:25:00,980 --> 01:25:04,580
the stuff that is proprietary to you, critical to your company.

1188
01:25:04,900 --> 01:25:10,540
You can encode it just as we encode the pixels of a cat, and it becomes the word
cat.

1189
01:25:10,920 --> 01:25:17,700
We can encode all of your PDF, and it turns into vectors that are now stored
inside your vector database.

1190
01:25:17,700 --> 01:25:20,100
It becomes the proprietary information of your company.

1191
01:25:20,600 --> 01:25:23,500
And once you have that proprietary information, you can chat to it.

1192
01:25:24,440 --> 01:25:27,940
It's a smart database, so you just chat with data.

1193
01:25:28,500 --> 01:25:30,600
And how much more enjoyable is that?

1194
01:25:31,120 --> 01:25:37,840
You know, for our software team, you know, they just chat with the bugs
database, you know.

1195
01:25:38,200 --> 01:25:39,960
How many bugs was there last night?

1196
01:25:40,260 --> 01:25:41,560
Are we making any progress?

1197
01:25:41,840 --> 01:25:47,040
And then after you're done talking to this bugs database, you need therapy.

1198
01:25:47,980 --> 01:25:50,320
And so we have another chat.

1199
01:25:50,600 --> 01:25:52,600
I'll give you a chat bot for you.

1200
01:25:55,600 --> 01:25:57,600
You can do it.

1201
01:26:06,600 --> 01:26:13,600
Okay, so we call this Nemo Retriever, and the reason for that is because
ultimately its job is to go retrieve information as quickly as possible.

1202
01:26:13,600 --> 01:26:14,600
And you just talk to it.

1203
01:26:14,600 --> 01:26:16,600
Hey, retrieve me this information.

1204
01:26:16,600 --> 01:26:18,600
It brings it back to you.

1205
01:26:18,600 --> 01:26:19,600
Do you mean this?

1206
01:26:19,600 --> 01:26:21,600
And you go, yeah, perfect, okay.

1207
01:26:21,600 --> 01:26:23,600
And so we call it the Nemo Retriever.

1208
01:26:23,600 --> 01:26:27,600
Well, the Nemo service helps you create all these things, and we have all these
different NIMs.

1209
01:26:27,600 --> 01:26:29,600
We even have NIMs of digital humans.

1210
01:26:29,600 --> 01:26:33,600
I'm Rachel, your AI care manager.

1211
01:26:33,600 --> 01:26:41,600
Okay, so it's a really short clip, but there were so many videos to show you, I
guess, so many other demos to show you.

1212
01:26:41,600 --> 01:26:43,600
And so I had to cut this one short.

1213
01:26:43,600 --> 01:26:45,600
But this is Diana.

1214
01:26:45,600 --> 01:26:47,600
She is a digital human NIM.

1215
01:26:47,860 --> 01:26:48,860
And you just saw her.

1216
01:26:48,860 --> 01:26:50,860
You just talked to her.

1217
01:26:50,860 --> 01:26:55,860
And she's connected, in this case, to Hippocratic AI's large language model for
healthcare.

1218
01:26:55,860 --> 01:26:57,860
And it's truly amazing.

1219
01:26:59,860 --> 01:27:02,860
She is just super smart about healthcare things.

1220
01:27:02,860 --> 01:27:03,860
You know?

1221
01:27:03,860 --> 01:27:12,860
And so after Dwight, my VP of software engineering, talks to the chat bot for
Bugs database, then you come over here and talk to Diane.

1222
01:27:12,860 --> 01:27:17,860
And so Diane is completely animated with AI.

1223
01:27:17,860 --> 01:27:19,860
And she's a digital human.

1224
01:27:19,860 --> 01:27:22,860
There's so many companies that would like to build.

1225
01:27:22,860 --> 01:27:24,860
They're sitting on gold mines.

1226
01:27:24,860 --> 01:27:28,860
The enterprise IT industry is sitting on a gold mine.

1227
01:27:28,860 --> 01:27:34,860
It's a gold mine because they have so much understanding of the way work is
done.

1228
01:27:34,860 --> 01:27:37,860
They have all these amazing tools that have been created over the years.

1229
01:27:37,860 --> 01:27:39,860
And they're sitting on a lot of data.

1230
01:27:39,860 --> 01:27:46,860
If they could take that gold mine and turn them into co-pilots, these co-pilots
could help us do things.

1231
01:27:46,860 --> 01:27:55,860
And so just about every IT franchise, IT platform in the world that has valuable
tools that people use is sitting on a gold mine for co-pilots.

1232
01:27:55,860 --> 01:27:58,860
And they would like to build their own co-pilots and their own chat bots.

1233
01:27:58,860 --> 01:28:03,860
And so we're announcing that NVIDIA AI Foundry is working with some of the
world's great companies.

1234
01:28:03,860 --> 01:28:07,860
SAP generates 87% of the world's global commerce.

1235
01:28:07,860 --> 01:28:09,860
Basically, the world runs on SAP.

1236
01:28:09,860 --> 01:28:10,860
We run on SAP.

1237
01:28:10,860 --> 01:28:15,860
NVIDIA and SAP are building SAP Jewel co-pilots using NVIDIA NEMO.

1238
01:28:15,860 --> 01:28:18,860
We're using NVIDIA NEMO and DGX Cloud.

1239
01:28:18,860 --> 01:28:27,860
ServiceNow, they run 85% of the world's Fortune 500 companies run their people
and customer service operations on ServiceNow.

1240
01:28:27,860 --> 01:28:35,860
And they're using NVIDIA AI Foundry to build ServiceNow Assist virtual
assistants.

1241
01:28:35,860 --> 01:28:37,860
Cohesity backs up the world's data.

1242
01:28:37,860 --> 01:28:39,860
They're sitting on a gold mine of data.

1243
01:28:39,860 --> 01:28:43,860
Hundreds of exabytes of data, over 10,000 companies.

1244
01:28:43,860 --> 01:28:45,860
NVIDIA AI Foundry is working with them.

1245
01:28:45,860 --> 01:28:49,860
They build their Gaia generative AI agent.

1246
01:28:49,860 --> 01:29:02,860
Snowflake is a company that stores the world's digital warehouse in the cloud
and serves over 3 billion queries a day for 10,000 enterprise customers.

1247
01:29:02,860 --> 01:29:08,860
Snowflake is working with NVIDIA AI Foundry to build co-pilots with NVIDIA NEMO
and NIMS.

1248
01:29:08,860 --> 01:29:14,860
NetApp, nearly half of the files in the world are stored on-prem on NetApp.

1249
01:29:14,860 --> 01:29:24,860
NVIDIA AI Foundry is helping them build chatbots and co-pilots like those vector
databases and retrievers with NVIDIA NEMO and NIMS.

1250
01:29:24,860 --> 01:29:27,860
And we have a great partnership with Dell.

1251
01:29:27,860 --> 01:29:37,860
Everybody who is building these chatbots and generative AI, when you're ready to
run it, you're going to need an AI factory.

1252
01:29:37,860 --> 01:29:43,860
And nobody is better at building end-to-end systems of very large scale for the
enterprise.

1253
01:29:43,860 --> 01:29:49,860
And so anybody, any company, every company will need to build AI factories.

1254
01:29:49,860 --> 01:29:51,860
And it turns out that Michael is here.

1255
01:29:51,860 --> 01:29:53,860
He's happy to take your order.

1256
01:29:57,860 --> 01:29:59,860
Ladies and gentlemen, Michael Dell.

1257
01:30:04,860 --> 01:30:11,860
Okay, let's talk about the next wave of robotics, the next wave of AI, robotics,
physical AI.

1258
01:30:11,860 --> 01:30:16,860
So far, all of the AI that we've talked about is one computer.

1259
01:30:16,860 --> 01:30:23,860
Data comes into one computer, lots of the world's, if you will, experience in
digital text form.

1260
01:30:23,860 --> 01:30:30,860
The AI imitates us by reading a lot of the language to predict the next words.

1261
01:30:30,860 --> 01:30:34,860
It's imitating you by studying all of the patterns and all the other previous
examples.

1262
01:30:34,860 --> 01:30:37,860
Of course, it has to understand context and so on and so forth.

1263
01:30:37,860 --> 01:30:40,860
But once it understands the context, it's essentially imitating you.

1264
01:30:40,860 --> 01:30:42,860
We take all of the data.

1265
01:30:42,860 --> 01:30:44,860
We put it into a system like DGX.

1266
01:30:44,860 --> 01:30:47,860
We compress it into a large language model.

1267
01:30:47,860 --> 01:30:50,860
Trillions and trillions of parameters become billions and billions.

1268
01:30:50,860 --> 01:30:52,860
Trillions of tokens becomes billions of parameters.

1269
01:30:52,860 --> 01:30:55,860
These billions of parameters becomes your AI.

1270
01:30:55,860 --> 01:31:04,860
Well, in order for us to go to the next wave of AI where the AI understands the
physical world, we're going to need three computers.

1271
01:31:04,860 --> 01:31:06,860
The first computer is still the same computer.

1272
01:31:06,860 --> 01:31:09,860
It's that AI computer that now is going to be watching video.

1273
01:31:09,860 --> 01:31:12,860
And maybe it's doing synthetic data generation.

1274
01:31:12,860 --> 01:31:15,860
And maybe there's a lot of human examples.

1275
01:31:15,860 --> 01:31:21,860
Just as we have human examples in text form, we're going to have human examples
in articulation form.

1276
01:31:21,860 --> 01:31:31,860
And the AIs will watch us, understand what is happening, and try to adapt it for
themselves into the context.

1277
01:31:31,860 --> 01:31:39,860
And because it can generalize with these foundation models, maybe these robots
can also perform in the physical world a fairly general way.

1278
01:31:39,860 --> 01:31:50,860
So I just described in very simple terms essentially what just happened in large
language models, except the chat GPT moment for robotics may be right around the
corner.

1279
01:31:50,860 --> 01:31:54,860
And so we've been building the end-to-end systems for robotics for some time.

1280
01:31:54,860 --> 01:31:56,860
I'm super, super proud of the work.

1281
01:31:56,860 --> 01:31:59,860
We have the AI system, DGX.

1282
01:31:59,860 --> 01:32:03,860
We have the lower system, which is called AGX, for autonomous systems.

1283
01:32:03,860 --> 01:32:05,860
The world's first robotics processor.

1284
01:32:05,860 --> 01:32:08,860
When we first built this thing, people were, what are you guys building?

1285
01:32:08,860 --> 01:32:09,860
It's an SOC.

1286
01:32:09,860 --> 01:32:10,860
It's one chip.

1287
01:32:10,860 --> 01:32:12,860
It's designed to be very low power.

1288
01:32:12,860 --> 01:32:16,860
But it's designed for high-speed sensor processing and AI.

1289
01:32:16,860 --> 01:32:27,860
And so if you want to run transformers in a car or you want to run transformers
in anything that moves, we have the perfect computer for you.

1290
01:32:27,860 --> 01:32:28,860
It's called the Jetson.

1291
01:32:28,860 --> 01:32:31,860
And so the DGX on top for training the AI.

1292
01:32:31,860 --> 01:32:33,860
The Jetson is the autonomous processor.

1293
01:32:33,860 --> 01:32:37,860
And in the middle, we need another computer.

1294
01:32:37,860 --> 01:32:51,860
Because large language models have the benefit of you providing your examples
and then doing reinforcement learning human feedback, what is the reinforcement
learning human feedback of a robot?

1295
01:32:51,860 --> 01:32:55,860
Well, it's reinforcement learning physical feedback.

1296
01:32:55,860 --> 01:32:57,860
That's how you align the robot.

1297
01:32:57,860 --> 01:33:06,860
That's how the robot knows that as it's learning these articulation capabilities
and manipulation capabilities, it's going to adapt properly into the laws of
physics.

1298
01:33:07,860 --> 01:33:18,860
And so we need a simulation engine that represents the world digitally for the
robot so that the robot has a gym to go learn how to be a robot.

1299
01:33:18,860 --> 01:33:23,860
We call that virtual world Omniverse.

1300
01:33:23,860 --> 01:33:26,860
And the computer that runs Omniverse is called OVX.

1301
01:33:26,860 --> 01:33:31,860
And OVX, the computer itself, is hosted in the Azure cloud.

1302
01:33:31,860 --> 01:33:32,860
Okay?

1303
01:33:32,860 --> 01:33:35,860
And so basically we built these three things, these three systems.

1304
01:33:35,860 --> 01:33:37,860
On top of it, we have algorithms for every single robot.

1305
01:33:37,860 --> 01:33:39,860
That's all one.

1306
01:33:39,860 --> 01:33:45,860
Now, I'm going to show you one super example of how AI and Omniverse are going
to work together.

1307
01:33:45,860 --> 01:33:48,860
The example I'm going to show you is kind of insane.

1308
01:33:48,860 --> 01:33:51,860
But it's going to be very, very close to tomorrow.

1309
01:33:51,860 --> 01:33:53,860
It's a robotics building.

1310
01:33:53,860 --> 01:33:55,860
This robotics building is called a warehouse.

1311
01:33:55,860 --> 01:33:59,860
Inside the robotics building are going to be some autonomous systems.

1312
01:33:59,860 --> 01:34:02,860
Some of the autonomous systems are going to be called humans.

1313
01:34:02,860 --> 01:34:06,860
And some of the autonomous systems are going to be called forklifts.

1314
01:34:06,860 --> 01:34:11,860
These autonomous systems are going to interact with each other, of course,
autonomously.

1315
01:34:11,860 --> 01:34:16,860
And it's going to be overlooked upon by this warehouse to keep everybody out of
harm's way.

1316
01:34:16,860 --> 01:34:19,860
The warehouse is essentially an air traffic controller.

1317
01:34:19,860 --> 01:34:25,860
And whenever it sees something happening, it will redirect traffic and give new
waypoints,

1318
01:34:25,860 --> 01:34:28,860
just new waypoints to the robots and the people.

1319
01:34:28,860 --> 01:34:30,860
And they'll know exactly what to do.

1320
01:34:30,860 --> 01:34:34,860
This warehouse, this building, you can also talk to.

1321
01:34:34,860 --> 01:34:35,860
Of course you can talk to it.

1322
01:34:35,860 --> 01:34:38,860
Hey, you know, SAP Center, how are you feeling today?

1323
01:34:38,860 --> 01:34:40,860
For example.

1324
01:34:40,860 --> 01:34:44,860
And so you could ask the warehouse the same questions.

1325
01:34:44,860 --> 01:34:51,860
Basically, the system I just described will have omniverse cloud that's hosting
the virtual simulation

1326
01:34:51,860 --> 01:34:54,860
and AI running on DGX cloud.

1327
01:34:54,860 --> 01:34:56,860
And all of this is running in real time.

1328
01:34:56,860 --> 01:34:57,860
Let's take a look.

1329
01:34:59,860 --> 01:35:03,860
The future of heavy industries starts as a digital twin.

1330
01:35:03,860 --> 01:35:05,860
The AI agents helping robots.

1331
01:35:05,860 --> 01:35:11,860
Robots, workers, and infrastructure navigate unpredictable events in complex
industrial spaces

1332
01:35:11,860 --> 01:35:16,860
will be built and evaluated first in sophisticated digital twins.

1333
01:35:16,860 --> 01:35:20,860
This omniverse digital twin of a 100,000 square foot warehouse

1334
01:35:20,860 --> 01:35:24,860
is operating as a simulation environment that integrates digital workers,

1335
01:35:24,860 --> 01:35:28,860
AMRs running the NVIDIA ISAAC receptor stack,

1336
01:35:28,860 --> 01:35:33,860
centralized activity maps of the entire warehouse from 100 simulated ceiling
mount cameras

1337
01:35:33,860 --> 01:35:35,860
using NVIDIA Metropolis,

1338
01:35:35,860 --> 01:35:38,860
and AMR route planning with NVIDIA Coopt.

1339
01:35:38,860 --> 01:35:44,860
Software-in-loop testing of AI agents in this physically accurate simulated
environment

1340
01:35:44,860 --> 01:35:50,860
enables us to evaluate and refine how the system adapts to real-world
unpredictability.

1341
01:35:50,860 --> 01:35:57,860
Here, an incident occurs along this AMR's planned route, blocking its path as it
moves to pick up a pallet.

1342
01:35:57,860 --> 01:36:04,860
NVIDIA Metropolis updates and sends a real-time occupancy map to Coopt where a
new optimal route is calculated.

1343
01:36:05,860 --> 01:36:10,860
The AMR is enabled to see around corners and improve its mission efficiency.

1344
01:36:10,860 --> 01:36:14,860
With generative AI-powered Metropolis vision foundation models,

1345
01:36:14,860 --> 01:36:18,860
operators can even ask questions using natural language.

1346
01:36:18,860 --> 01:36:24,860
The visual model understands nuanced activity and can offer immediate insights
to improve operations.

1347
01:36:24,860 --> 01:36:29,860
All of the sensor data is created in simulation and passed to the real-time AI

1348
01:36:29,860 --> 01:36:33,860
running as NVIDIA Inference Microservices, or NIMS.

1349
01:36:33,860 --> 01:36:37,860
And when the AI is ready to be deployed in the physical twin, the real
warehouse,

1350
01:36:37,860 --> 01:36:41,860
we connect Metropolis and Isaac NIMS to real sensors

1351
01:36:41,860 --> 01:36:47,860
with the ability for continuous improvement of both the digital twin and the AI
models.

1352
01:36:50,860 --> 01:36:52,860
Isn't that incredible?

1353
01:36:52,860 --> 01:36:57,860
And so, remember, remember,

1354
01:36:57,860 --> 01:37:02,860
a future facility, warehouse, factory, building,

1355
01:37:02,860 --> 01:37:04,860
will be software-defined.

1356
01:37:04,860 --> 01:37:06,860
And so the software is running.

1357
01:37:06,860 --> 01:37:08,860
How else would you test the software?

1358
01:37:08,860 --> 01:37:11,860
So you test the software to building the warehouse,

1359
01:37:11,860 --> 01:37:14,860
the optimization system in the digital twin.

1360
01:37:14,860 --> 01:37:15,860
What about all the robots?

1361
01:37:15,860 --> 01:37:17,860
All of those robots you were seeing just now,

1362
01:37:17,860 --> 01:37:20,860
they're all running their own autonomous robotic stack.

1363
01:37:20,860 --> 01:37:24,860
And so the way you integrate software in the future, CICD in the future,

1364
01:37:24,860 --> 01:37:27,860
for robotic systems is with digital twins.

1365
01:37:27,860 --> 01:37:31,860
We've made Omniverse a lot easier to access.

1366
01:37:31,860 --> 01:37:34,860
Basically Omniverse Cloud APIs,

1367
01:37:34,860 --> 01:37:36,860
four simple API and a channel,

1368
01:37:36,860 --> 01:37:38,860
and you can connect your application to it.

1369
01:37:38,860 --> 01:37:42,860
So this is going to be as wonderfully, beautifully simple

1370
01:37:42,860 --> 01:37:45,860
in the future that Omniverse is going to be.

1371
01:37:45,860 --> 01:37:46,860
And with these APIs,

1372
01:37:46,860 --> 01:37:50,860
you're going to have these magical digital twin capability.

1373
01:37:50,860 --> 01:37:55,860
We also have turned Omniverse into an AI

1374
01:37:55,860 --> 01:37:58,860
and integrated it with the ability to chat USD,

1375
01:37:58,860 --> 01:38:00,860
the language of,

1376
01:38:00,860 --> 01:38:03,860
our language is human and Omniverse's language,

1377
01:38:03,860 --> 01:38:06,860
as it turns out, is universal scene description.

1378
01:38:06,860 --> 01:38:09,860
And so that language is rather complex.

1379
01:38:09,860 --> 01:38:12,860
And so we've taught our Omniverse that language.

1380
01:38:12,860 --> 01:38:16,860
And so you can speak to it in English and it would directly generate USD.

1381
01:38:16,860 --> 01:38:20,860
And it would talk back in USD but converse back to you in English.

1382
01:38:20,860 --> 01:38:24,860
You could also look for information in this world semantically.

1383
01:38:24,860 --> 01:38:27,860
Instead of the world being encoded semantically in language,

1384
01:38:27,860 --> 01:38:29,860
now it's encoded semantically in scenes.

1385
01:38:29,860 --> 01:38:33,860
And so you could ask it of certain objects

1386
01:38:33,860 --> 01:38:35,860
or certain conditions or certain scenarios,

1387
01:38:35,860 --> 01:38:37,860
and it can go and find that scenario for you.

1388
01:38:37,860 --> 01:38:40,860
It also can collaborate with you in generation.

1389
01:38:40,860 --> 01:38:42,860
You could design some things in 3D,

1390
01:38:42,860 --> 01:38:44,860
it could simulate some things in 3D,

1391
01:38:44,860 --> 01:38:46,860
or you could use AI to generate something in 3D.

1392
01:38:46,860 --> 01:38:49,860
Let's take a look at how this is all going to work.

1393
01:38:49,860 --> 01:38:51,860
We have a great partnership with Siemens.

1394
01:38:51,860 --> 01:38:57,860
Siemens is the world's largest industrial engineering and operations platform.

1395
01:38:57,860 --> 01:38:59,860
You've seen now so many different companies

1396
01:38:59,860 --> 01:39:01,860
in the industrial space.

1397
01:39:01,860 --> 01:39:05,860
Heavy Industries is one of the greatest final frontiers of IT.

1398
01:39:05,860 --> 01:39:08,860
And we finally now have the necessary technology

1399
01:39:08,860 --> 01:39:10,860
to go and make a real impact.

1400
01:39:10,860 --> 01:39:13,860
Siemens is building the industrial metaverse.

1401
01:39:13,860 --> 01:39:16,860
And today we're announcing that Siemens is connecting

1402
01:39:16,860 --> 01:39:19,860
their crown jewel accelerator to NVIDIA Omniverse.

1403
01:39:19,860 --> 01:39:21,860
Let's take a look.

1404
01:39:23,860 --> 01:39:26,860
Siemens technology is transformed every day for everyone.

1405
01:39:26,860 --> 01:39:29,860
Teamcenter X, our leading product lifecycle management team,

1406
01:39:29,860 --> 01:39:32,860
from the Siemens accelerator platform,

1407
01:39:32,860 --> 01:39:34,860
is used every day by our customers

1408
01:39:34,860 --> 01:39:37,860
to develop and deliver products at scale.

1409
01:39:37,860 --> 01:39:40,860
Now we are bringing the real and the digital worlds

1410
01:39:40,860 --> 01:39:43,860
even closer by integrating NVIDIA AI

1411
01:39:43,860 --> 01:39:47,860
and Omniverse technologies into Teamcenter X.

1412
01:39:47,860 --> 01:39:50,860
Omniverse APIs enable data interoperability

1413
01:39:50,860 --> 01:39:52,860
and physics-based rendering

1414
01:39:52,860 --> 01:39:56,860
to industrial-scale design and manufacturing projects.

1415
01:39:56,860 --> 01:39:58,860
Our customers, HD Hyundai,

1416
01:39:58,860 --> 01:40:01,860
market leader in sustainable ship manufacturing,

1417
01:40:01,860 --> 01:40:03,860
builds ammonia and hydrogen-powered ships,

1418
01:40:03,860 --> 01:40:07,860
often comprising over 7 million discrete parts.

1419
01:40:08,860 --> 01:40:09,860
With Omniverse APIs,

1420
01:40:09,860 --> 01:40:12,860
Teamcenter X lets companies like HD Hyundai

1421
01:40:12,860 --> 01:40:17,860
unify and visualize these massive engineering datasets interactively

1422
01:40:17,860 --> 01:40:21,860
and integrate generative AI to generate 3D objects

1423
01:40:21,860 --> 01:40:26,860
or HDRI backgrounds to see their projects in context.

1424
01:40:26,860 --> 01:40:27,860
The result?

1425
01:40:27,860 --> 01:40:31,860
An ultra-intuitive photoreal, physics-based digital twin

1426
01:40:31,860 --> 01:40:33,860
that eliminates waste and errors,

1427
01:40:33,860 --> 01:40:36,860
delivering huge savings in cost and time.

1428
01:40:37,860 --> 01:40:39,860
And we are building this for collaboration,

1429
01:40:39,860 --> 01:40:42,860
whether across more Siemens accelerator tools

1430
01:40:42,860 --> 01:40:46,860
like Siemens Annex or Star CCM Plus,

1431
01:40:46,860 --> 01:40:49,860
or across teams working on their favorite devices

1432
01:40:49,860 --> 01:40:51,860
in the same scene together.

1433
01:40:52,860 --> 01:40:54,860
And this is just the beginning.

1434
01:40:54,860 --> 01:40:55,860
Working with NVIDIA,

1435
01:40:55,860 --> 01:40:57,860
we will bring accelerated computing,

1436
01:40:57,860 --> 01:41:00,860
generative AI and Omniverse integration

1437
01:41:00,860 --> 01:41:04,860
across the Siemens accelerator portfolio.

1438
01:41:11,860 --> 01:41:16,860
The professional voice actor

1439
01:41:16,860 --> 01:41:18,860
happens to be a good friend of mine,

1440
01:41:18,860 --> 01:41:19,860
Roland Busch,

1441
01:41:19,860 --> 01:41:21,860
who happens to be the CEO of Siemens.

1442
01:41:25,860 --> 01:41:34,860
Once you get Omniverse connected into your workflow,

1443
01:41:34,860 --> 01:41:36,860
your ecosystem,

1444
01:41:36,860 --> 01:41:38,860
from the beginning of your design

1445
01:41:38,860 --> 01:41:40,860
to engineering,

1446
01:41:40,860 --> 01:41:42,860
to manufacturing planning,

1447
01:41:42,860 --> 01:41:45,860
all the way to digital twin operations,

1448
01:41:45,860 --> 01:41:47,860
once you connect everything together,

1449
01:41:47,860 --> 01:41:51,860
it's insane how much productivity you can get.

1450
01:41:51,860 --> 01:41:53,860
And it's just really, really wonderful.

1451
01:41:53,860 --> 01:41:55,860
All of a sudden, everybody's operating on the same ground table.

1452
01:41:55,860 --> 01:42:02,740
truth. You don't have to exchange data and convert data, make mistakes.
Everybody is working on the

1453
01:42:02,740 --> 01:42:07,600
same ground truth. From the design department to the art department, the
architecture department,

1454
01:42:07,600 --> 01:42:12,360
all the way to the engineering and even the marketing department. Let's take a
look at how

1455
01:42:12,360 --> 01:42:18,780
Nissan has integrated Omniverse into their workflow. And it's all because it's
connected

1456
01:42:18,780 --> 01:42:22,440
by all these wonderful tools and these developers that we're working with. Take
a look.

1457
01:42:25,860 --> 01:42:54,860
.

1458
01:42:54,860 --> 01:42:55,860
.

1459
01:42:55,860 --> 01:43:01,860
.

1460
01:43:01,860 --> 01:43:16,300
.

1461
01:43:16,300 --> 01:43:24,860
.

1462
01:43:24,860 --> 01:43:25,700
.

1463
01:43:25,700 --> 01:43:25,740
.

1464
01:43:25,740 --> 01:43:25,780
.

1465
01:43:25,780 --> 01:43:25,800
.

1466
01:43:25,800 --> 01:43:25,820
.

1467
01:43:25,820 --> 01:43:25,840
.

1468
01:43:25,840 --> 01:43:25,860
.

1469
01:43:25,860 --> 01:43:55,840
Thank you for watching.

1470
01:43:55,860 --> 01:44:25,840
Thank you for watching.

1471
01:44:25,860 --> 01:44:55,840
Thank you for watching.

1472
01:44:56,640 --> 01:44:59,020
And one of the largest industries is going to be automotive.

1473
01:44:59,920 --> 01:45:03,140
We build the robotic stack from top to bottom, as I was mentioning,

1474
01:45:03,720 --> 01:45:06,360
from the computer system, but in the case of self-driving cars,

1475
01:45:06,700 --> 01:45:08,920
including the self-driving car application.

1476
01:45:09,640 --> 01:45:12,200
At the end of this year, or I guess beginning of next year,

1477
01:45:12,380 --> 01:45:16,260
we will be shipping in Mercedes, and then shortly after that, JLR.

1478
01:45:17,000 --> 01:45:20,580
And so these autonomous robotic systems are software-defined.

1479
01:45:21,000 --> 01:45:22,300
They take a lot of work to do.

1480
01:45:22,300 --> 01:45:23,560
It has computer vision.

1481
01:45:23,700 --> 01:45:25,540
It has, obviously, artificial intelligence.

1482
01:45:25,860 --> 01:45:26,840
Control and planning.

1483
01:45:27,260 --> 01:45:30,860
All kinds of very complicated technology, and it takes years to refine.

1484
01:45:31,260 --> 01:45:32,620
We're building the entire stack.

1485
01:45:33,500 --> 01:45:37,220
However, we open up our entire stack for all of the automotive industry.

1486
01:45:37,400 --> 01:45:38,500
This is just the way we work.

1487
01:45:38,900 --> 01:45:40,380
The way we work in every single industry,

1488
01:45:40,500 --> 01:45:43,060
we try to build as much of it as we can so that we understand it,

1489
01:45:43,220 --> 01:45:45,680
but then we open it up so that everybody can access it.

1490
01:45:45,940 --> 01:45:48,280
Whether you would like to buy just our computer,

1491
01:45:48,280 --> 01:45:54,280
which is the world's only full, functional, safe, ASIL-D system,

1492
01:45:54,700 --> 01:45:55,640
that can run...

1493
01:45:55,640 --> 01:46:06,020
This functional, safe, ASIL-D-quality computer or the operating system on top or
of course our data centers,

1494
01:46:06,020 --> 01:46:08,980
which is in basically every EV company in the world.

1495
01:46:10,240 --> 01:46:12,720
However you would like to enjoy it, we're delighted by it.

1496
01:46:13,440 --> 01:46:19,780
Today, we're announcing that BYD, the world's largest EV company, is adopting
our next generation.

1497
01:46:20,080 --> 01:46:21,020
It's called Thor.

1498
01:46:21,420 --> 01:46:23,420
Thor is designed for transformer engines.

1499
01:46:23,920 --> 01:46:25,440
Thor, our next generation EV.

1500
01:46:25,640 --> 01:46:29,640
Thor, our next generation EV computer, will be used by BYD.

1501
01:46:37,640 --> 01:46:41,640
You probably don't know this fact that we have over a million robotics
developers.

1502
01:46:42,640 --> 01:46:45,640
We created Jetson, this robotics computer.

1503
01:46:45,640 --> 01:46:46,640
We're so proud of it.

1504
01:46:46,640 --> 01:46:48,640
The amount of software that goes on top of it is insane.

1505
01:46:49,240 --> 01:46:52,640
But the reason why we can do it at all is because it's 100% CUDA compatible.

1506
01:46:53,040 --> 01:46:54,240
Everything that we do,

1507
01:46:54,240 --> 01:46:57,240
everything that we do in our company is in service of our developers.

1508
01:46:57,240 --> 01:47:05,240
And by us being able to maintain this rich ecosystem and make it compatible with
everything that you access from us,

1509
01:47:05,240 --> 01:47:12,240
we can bring all of that incredible capability to this little tiny computer we
call Jetson, a robotics computer.

1510
01:47:12,240 --> 01:47:17,240
We also today are announcing this incredibly advanced new SDK.

1511
01:47:17,240 --> 01:47:20,240
We call it Isaac Perceptor.

1512
01:47:20,240 --> 01:47:23,240
Isaac Perceptor, most of the robots today,

1513
01:47:23,240 --> 01:47:25,240
are pre-programmed.

1514
01:47:25,240 --> 01:47:30,240
They're either following rails on the ground, digital rails, or they'd be
following April tags.

1515
01:47:30,240 --> 01:47:32,240
But in the future, they're going to have perception.

1516
01:47:32,240 --> 01:47:36,240
And the reason why you want that is so that you could easily program it.

1517
01:47:36,240 --> 01:47:39,240
You say, I would like to go from point A to point B,

1518
01:47:39,240 --> 01:47:42,240
and it will figure out a way to navigate its way there.

1519
01:47:42,240 --> 01:47:45,240
So by only programming waypoints,

1520
01:47:45,240 --> 01:47:48,240
the entire route could be adaptive.

1521
01:47:48,240 --> 01:47:52,240
The entire environment could be reprogrammed, just as I showed you at the very
beginning with the warehouse.

1522
01:47:52,240 --> 01:47:57,240
You can't do that with pre-programmed AGVs.

1523
01:47:57,240 --> 01:48:02,240
If those boxes fall down, they just all gum up and they just wait there for
somebody to come clear it.

1524
01:48:02,240 --> 01:48:06,240
And so now, with the Isaac Perceptor,

1525
01:48:06,240 --> 01:48:10,240
we have incredible state-of-the-art vision odometry,

1526
01:48:10,240 --> 01:48:12,240
3D reconstruction,

1527
01:48:12,240 --> 01:48:15,240
and in addition to 3D reconstruction, depth perception.

1528
01:48:15,240 --> 01:48:20,240
The reason for that is so that you can have two modalities to keep an eye on
what's happening in the world.

1529
01:48:20,240 --> 01:48:21,240
Isaac Perceptor.

1530
01:48:21,240 --> 01:48:28,240
The most used robot today is the manipulator, manufacturing arms,

1531
01:48:28,240 --> 01:48:30,240
and they are also pre-programmed.

1532
01:48:30,240 --> 01:48:33,240
The computer vision algorithms, the AI algorithms,

1533
01:48:33,240 --> 01:48:37,240
the control and path planning algorithms that are geometry-aware,

1534
01:48:37,240 --> 01:48:40,240
incredibly computationally intensive.

1535
01:48:40,240 --> 01:48:43,240
We have made these CUDA accelerated.

1536
01:48:43,240 --> 01:48:49,240
So we have the world's first CUDA accelerated motion planner that is geometry-
aware.

1537
01:48:49,240 --> 01:48:51,240
You put something in front of it,

1538
01:48:51,240 --> 01:48:54,240
it comes up with a new plan and articulates around it.

1539
01:48:54,240 --> 01:48:59,240
It has excellent perception for pose estimation of a 3D object.

1540
01:48:59,240 --> 01:49:03,240
Not just its pose in 2D, but its pose in 3D.

1541
01:49:03,240 --> 01:49:08,240
So it has to imagine what's around and how best to graph it.

1542
01:49:08,240 --> 01:49:12,240
So the foundation pose, the grip foundation,

1543
01:49:12,240 --> 01:49:16,240
and the articulation algorithms are now available.

1544
01:49:16,240 --> 01:49:18,240
We call it Isaac Manipulator.

1545
01:49:18,240 --> 01:49:21,240
And they also just run on NVIDIA's computers.

1546
01:49:21,240 --> 01:49:28,240
We are starting to do some really great work in the next generation of robotics.

1547
01:49:28,240 --> 01:49:33,240
The next generation of robotics will likely be a humanoid robotics.

1548
01:49:33,240 --> 01:49:36,240
We now have the necessary technology,

1549
01:49:36,240 --> 01:49:39,240
and as I was describing earlier,

1550
01:49:39,240 --> 01:49:44,240
the necessary technology to imagine generalized human robotics.

1551
01:49:44,240 --> 01:49:47,240
In a way, human robotics is likely easier,

1552
01:49:47,240 --> 01:49:51,240
and the reason for that is because we have a lot more imitation training

1553
01:49:51,240 --> 01:49:53,240
data that we can provide the robots,

1554
01:49:53,240 --> 01:49:56,240
because we are constructed in a very similar way.

1555
01:49:56,240 --> 01:50:00,240
It is very likely that the humanoid robotics will be much more useful in our
world,

1556
01:50:00,240 --> 01:50:05,240
because we created the world to be something that we can interoperate in and
work well in.

1557
01:50:05,240 --> 01:50:09,240
And the way that we set up our workstations and manufacturing and logistics,

1558
01:50:09,240 --> 01:50:12,240
they were designed for humans, they were designed for people.

1559
01:50:12,240 --> 01:50:18,240
And so these humanoid robotics will likely be much more productive to deploy.

1560
01:50:18,240 --> 01:50:21,240
While we're creating, just like we're doing with the others,

1561
01:50:21,240 --> 01:50:24,240
the entire stack, starting from the top,

1562
01:50:24,240 --> 01:50:31,240
a foundation model that learns from watching video, human examples.

1563
01:50:31,240 --> 01:50:35,240
It could be in video form, it could be in virtual reality form.

1564
01:50:35,240 --> 01:50:40,240
We then created a gym for it called Isaac Reinforcement Learning Gym,

1565
01:50:40,240 --> 01:50:46,240
which allows the humanoid robot to learn how to adapt to the physical world.

1566
01:50:46,240 --> 01:50:48,240
And then an incredible computer,

1567
01:50:48,240 --> 01:50:51,240
the same computer that's going to go into a robotic car,

1568
01:50:51,240 --> 01:50:55,240
this computer will run inside a humanoid robot called Thor.

1569
01:50:55,240 --> 01:50:58,240
It's designed for transformer engines.

1570
01:50:58,240 --> 01:51:01,240
We've combined several of these into one video.

1571
01:51:01,240 --> 01:51:03,240
This is something that you're going to really love.

1572
01:51:03,240 --> 01:51:04,240
Take a look.

1573
01:51:08,240 --> 01:51:11,240
It's not enough for humans to imagine.

1574
01:51:16,240 --> 01:51:18,240
We have to invent.

1575
01:51:18,240 --> 01:51:22,240
And explore.

1576
01:51:22,240 --> 01:51:25,240
And push beyond what's been done.

1577
01:51:32,240 --> 01:51:34,240
We create smarter.

1578
01:51:34,240 --> 01:51:36,240
And faster.

1579
01:51:38,240 --> 01:51:40,240
We push it to fail.

1580
01:51:40,240 --> 01:51:42,240
So it can learn.

1581
01:51:45,240 --> 01:51:47,240
We teach it.

1582
01:51:47,240 --> 01:51:49,240
Then help it teach itself.

1583
01:51:49,240 --> 01:51:52,240
We broaden its understanding.

1584
01:51:55,240 --> 01:51:57,240
To take on new challenges.

1585
01:51:59,240 --> 01:52:02,240
With absolute precision.

1586
01:52:04,240 --> 01:52:06,240
And succeed.

1587
01:52:07,240 --> 01:52:09,240
We make it perceive.

1588
01:52:10,240 --> 01:52:12,240
And move.

1589
01:52:14,240 --> 01:52:16,240
And even reason.

1590
01:52:17,240 --> 01:52:20,240
So it can share our world.

1591
01:52:20,240 --> 01:52:21,240
With us.

1592
01:52:41,240 --> 01:52:44,240
This is where inspiration leads us.

1593
01:52:44,240 --> 01:52:45,240
The next frontier.

1594
01:52:46,240 --> 01:52:49,240
This is NVIDIA Project GROOT.

1595
01:52:53,240 --> 01:52:57,240
A general purpose foundation model for humanoid robot learning.

1596
01:52:59,240 --> 01:53:04,240
The GROOT model takes multimodal instructions and past interactions as input.

1597
01:53:04,240 --> 01:53:07,240
And produces the next action for the robot to execute.

1598
01:53:09,240 --> 01:53:11,240
We developed Isaac Lab.

1599
01:53:11,240 --> 01:53:13,240
A robot learning application to train GROOT.

1600
01:53:13,240 --> 01:53:16,240
On Omniverse Isaac Sim.

1601
01:53:17,240 --> 01:53:19,240
And we scale out with Osmo.

1602
01:53:19,240 --> 01:53:21,240
A new compute orchestration service.

1603
01:53:21,240 --> 01:53:24,240
That coordinates workflows across DGX systems for training.

1604
01:53:24,240 --> 01:53:27,240
And OVX systems for simulation.

1605
01:53:28,240 --> 01:53:29,240
With these tools.

1606
01:53:29,240 --> 01:53:32,240
We can train GROOT in physically based simulation.

1607
01:53:32,240 --> 01:53:35,240
And transfer zero-shot to the real world.

1608
01:53:37,240 --> 01:53:41,240
The GROOT model will enable a robot to learn from a handful of human
demonstrations.

1609
01:53:41,240 --> 01:53:45,240
So it can help with everyday tasks.

1610
01:53:47,240 --> 01:53:49,240
And emulate human movement.

1611
01:53:49,240 --> 01:53:51,240
Just by observing us.

1612
01:53:52,240 --> 01:53:55,240
This is made possible with NVIDIA's technologies.

1613
01:53:55,240 --> 01:53:57,240
That can understand humans from videos.

1614
01:53:57,240 --> 01:53:59,240
Train models and simulation.

1615
01:53:59,240 --> 01:54:02,240
And ultimately deploy them directly to physical robots.

1616
01:54:03,240 --> 01:54:05,240
Connecting GROOT to a large language model.

1617
01:54:05,240 --> 01:54:07,240
Even allows it to generate motions.

1618
01:54:07,240 --> 01:54:10,240
By following natural language instructions.

1619
01:54:10,240 --> 01:54:11,240
Hi GL1.

1620
01:54:11,240 --> 01:54:13,240
Can you give me a high five?

1621
01:54:13,240 --> 01:54:14,240
Sure thing.

1622
01:54:14,240 --> 01:54:15,240
Let's high five.

1623
01:54:16,240 --> 01:54:18,240
Can you give us some cool moves?

1624
01:54:19,240 --> 01:54:20,240
Sure.

1625
01:54:20,240 --> 01:54:21,240
Check this out.

1626
01:54:24,240 --> 01:54:29,240
All this incredible intelligence is powered by the new Jetson Thor Robotics
chips.

1627
01:54:29,240 --> 01:54:30,240
Designed for GROOT.

1628
01:54:30,240 --> 01:54:32,240
Built for the future.

1629
01:54:32,240 --> 01:54:35,240
With Isaac Lab, Osmo and GROOT.

1630
01:54:35,240 --> 01:54:39,240
We're providing the building blocks for the next generation of AI powered
robotics.

1631
01:54:40,240 --> 01:54:53,240
No 보건энерго

1632
01:54:53,240 --> 01:54:57,240
About the same size.

1633
01:55:05,240 --> 01:55:06,240
The soul of NVIDIA.

1634
01:55:06,240 --> 01:55:08,240
The intersection of computer graphics.

1635
01:55:08,240 --> 01:55:09,240
Physics.

1636
01:55:09,240 --> 01:55:15,340
physics, artificial intelligence. It all came to bear at this moment. The name
of

1637
01:55:15,340 --> 01:55:29,380
that project, General Robotics 003. I know, super good. Super good. Well, I
think we

1638
01:55:29,380 --> 01:55:35,240
have some special guests. Do we?

1639
01:55:39,240 --> 01:55:53,680
Hey, guys. So I understand you guys are powered by Jetson. They're powered by
Jetsons. Little

1640
01:55:53,680 --> 01:56:04,360
Jetson robotics computers inside. They learned to walk in Isaac Sim. Ladies and
gentlemen,

1641
01:56:04,360 --> 01:56:08,440
this is orange, and this is the famous green.

1642
01:56:08,440 --> 01:56:19,860
They are the BDX robots of Disney. Amazing Disney research. Come on, you guys.
Let's

1643
01:56:19,860 --> 01:56:35,220
wrap up. Let's go. Five things. Where are you going? I sit right here. Don't be
afraid.

1644
01:56:35,220 --> 01:56:36,660
Come here, green. Hurry up.

1645
01:56:38,440 --> 01:56:52,240
Five things. What are you saying? No, it's not time to eat. I'll give you a
snack in

1646
01:56:52,240 --> 01:57:01,700
a moment. Let me finish up real quick. Come on, green. Hurry up. Stop wasting
time. Five

1647
01:57:01,700 --> 01:57:07,040
things. Five things. First, a new industrial revolution. Every day, every state
and every

1648
01:57:07,040 --> 01:57:08,440
country is going to be on the run. Every day, every city is going to be on the
run. Every

1649
01:57:08,440 --> 01:57:12,280
data centers should be accelerated a trillion dollars worth of installed data

1650
01:57:12,280 --> 01:57:16,600
centers will become modernized over the next several years second because of the

1651
01:57:16,600 --> 01:57:20,180
computational capability we brought to bear a new way of doing software has

1652
01:57:20,180 --> 01:57:24,920
emerged generative AI which is going to create new and new infrastructure

1653
01:57:24,920 --> 01:57:30,040
dedicated to doing one thing and one thing only not for multi-user data

1654
01:57:30,040 --> 01:57:36,220
centers but AI generators these AI generation will create incredibly

1655
01:57:36,220 --> 01:57:41,800
valuable software a new Industrial Revolution second the computer of this

1656
01:57:41,800 --> 01:57:46,900
revolution the computer of this generation generative AI trillion

1657
01:57:46,900 --> 01:57:54,400
parameters Blackwell insane amounts of computers and computing third I'm trying

1658
01:57:54,400 --> 01:58:04,480
to concentrate good job third new computer new computer creates new types

1659
01:58:04,480 --> 01:58:06,220
of software new type of software should

1660
01:58:06,220 --> 01:58:10,780
be distributed in a new way so that it can on the one hand be an endpoint in the

1661
01:58:10,780 --> 01:58:15,160
cloud and easy to use but still allow you to take it with you because it is

1662
01:58:15,160 --> 01:58:19,240
your intelligence your intelligence should be packed packaged up in a way

1663
01:58:19,240 --> 01:58:23,860
that allows you to take it with you we call them NIMS and third these NIMS are

1664
01:58:23,860 --> 01:58:28,720
going to help you create a new type of application for the future not one that

1665
01:58:28,720 --> 01:58:32,260
you wrote completely from scratch but you're going to integrate them like

1666
01:58:32,260 --> 01:58:35,860
teams create these applications we have a

1667
01:58:35,860 --> 01:58:36,160
few of them we have a few of them we have a few of them we have a few of them

1668
01:58:36,160 --> 01:58:36,200
we have a few of them we have a few of them we have a few of them we have a

1669
01:58:36,200 --> 01:58:42,740
fantastic capability between NIMS the AI technology the tools Nemo and the

1670
01:58:42,740 --> 01:58:47,480
infrastructure DGX cloud in our AI foundry to help you create proprietary

1671
01:58:47,480 --> 01:58:51,320
applications proprietary chatbots and then lastly everything that moves in the

1672
01:58:51,320 --> 01:58:56,180
future will be robotic you're not going to be the only one and these robotic

1673
01:58:56,180 --> 01:59:03,320
systems whether they are humanoid AMRs self-driving cars forklifts manipulating

1674
01:59:03,320 --> 01:59:06,140
arms they will all need one thing

1675
01:59:06,140 --> 01:59:11,180
giant stadiums warehouses factories there can be factories that are robotic

1676
01:59:11,180 --> 01:59:15,620
orchestrating factories manufacturing lines that are robotics building cars

1677
01:59:15,620 --> 01:59:22,640
that are robotics these systems all need one thing they need a platform a
digital

1678
01:59:22,640 --> 01:59:27,500
platform a digital twin platform and we call that omniverse the operating system

1679
01:59:27,500 --> 01:59:32,900
of the robotics world these are the five things that we talked about today what

1680
01:59:32,900 --> 01:59:36,080
does NVIDIA look like what does NVIDIA look like when we talk about

1681
01:59:36,080 --> 01:59:40,700
GPUs there's a very different image that I have when I when people ask me

1682
01:59:40,700 --> 01:59:44,780
about GPUs first I see a bunch of software Stags and things like that and

1683
01:59:44,780 --> 01:59:50,360
second I see this this is what we announced to you today this is Blackwell

1684
01:59:50,360 --> 01:59:53,960
this is the platform

1685
01:59:57,320 --> 02:00:05,540
amazing amazing processors MV link switches networking systems and the system
design is a

1686
02:00:05,540 --> 02:00:06,020
miracle

1687
02:00:06,020 --> 02:00:11,000
this is Blackwell and this to me is what a GPU looks like in my mind

1688
02:00:19,220 --> 02:00:24,620
listen orange green I think we have one more treat for everybody what do you
think should we

1689
02:00:26,540 --> 02:00:30,260
okay we have one more thing to show you roll it

1690
02:00:36,020 --> 02:01:01,320
hope you enjoy soon

1691
02:01:01,320 --> 02:01:03,260
hope you enjoy soon

1692
02:01:03,260 --> 02:01:03,320
hope you enjoy soon

1693
02:01:03,320 --> 02:01:04,540
hope you enjoy soon

1694
02:01:04,540 --> 02:01:04,620
hope you enjoy soon

1695
02:01:04,620 --> 02:01:04,820
hope you enjoy soon

1696
02:01:04,820 --> 02:01:05,160
hope you enjoy soon

1697
02:01:05,160 --> 02:01:05,180
hope you enjoy soon

1698
02:01:05,180 --> 02:01:05,660
hope you enjoy soon

1699
02:01:05,660 --> 02:01:05,680
hope you enjoy soon

1700
02:01:05,680 --> 02:01:05,780
hope you enjoy soon

1701
02:01:05,780 --> 02:01:05,860
hope you enjoy soon

1702
02:01:05,860 --> 02:01:06,020
hope you enjoy soon

1703
02:01:06,020 --> 02:01:36,000
To be continued...

1704
02:01:36,020 --> 02:02:06,000
To be continued...

1705
02:02:06,020 --> 02:02:36,000
To be continued...

1706
02:02:36,020 --> 02:03:06,000
To be continued...

